[["index.html", "Multifunctional peptide engineering Preface", " Multifunctional peptide engineering Renee Ti Chou and Henry T. Hsueh 2023-02-06 Preface The research notebook contains the code of the machine learning algorithms used to generate the results in the paper “Machine learning-driven multifunctional peptide engineering for sustained ocular drug delivery.” The research involves a super learner-based methodology to improve multifunctional peptide engineering. The aim is to impart high melanin binding, high cell-penetration, and low cytotoxicity to ocular drugs through peptide-drug conjugation, with the ultimate goal of enhancing the sustained delivery of the drug to maintain the therapeutic level in the eye for a prolonged period. Citation: Chou RT, et al. Supplementary materials for machine learning-driven multifunctional peptide engineering for sustained ocular drug delivery. https://doi.org/10.13016/0jck-hnnv, (2023). @misc{https://doi.org/10.13016/0jck-hnnv, doi = {10.13016/0JCK-HNNV}, url = {https://drum.lib.umd.edu/handle/1903/29529}, author = {Chou, Renee Ti and Hsueh, Henry T. and Rai, Usha and Liyanage, Wathsala and Kim, Yoo Chun and Appell, Matthew B. and Pejavar, Jahnavi and Leo, Kirby T. and Davison, Charlotte and Kolodziejski, Patricia and Mozzer, Ann and Kwon, HyeYoung and Sista, Maanasa and Anders, Nicole M. and Hemingway, Avelina and Rompicharla, Sri Vishnu Kiran and Edwards, Malia and Pitha, Ian and Hanes, Justin and Cummings, Michael P. and Ensign, Laura M.}, keywords = {machine learning, drug delivery}, language = {en}, title = {Supplementary materials for machine learning-driven multifunctional peptide engineering for sustained ocular drug delivery}, publisher = {Digital Repository at the University of Maryland}, year = {2023} } "],["id_01_pilot_peptide_array.html", "Section 1 Melanin binding pilot peptide array 1.1 Overview 1.2 Generating ML input 1.3 Training initial RF model 1.4 Melanin binding variable importance", " Section 1 Melanin binding pilot peptide array library(seqinr) library(Peptides) library(stringr) library(protr) library(plyr) library(randomForest) library(ggplot2) 1.1 Overview 1.2 Generating ML input # -------------- # Peptides_2.4.4 # -------------- # read in fasta file peptides &lt;- read.fasta(&quot;./other_data/mb_pilot_peptide_array.fasta&quot;, seqtype = &quot;AA&quot;, as.string = TRUE, set.attributes = FALSE) # Molecular weight weights &lt;- as.matrix(sapply(peptides, function(x) mw(x, monoisotopic = FALSE))) colnames(weights) &lt;- &quot;weight&quot; # Amino acid composition aa.comp &lt;- sapply(peptides, function(x) aaComp(x)) aa.comp.matrix &lt;- t(sapply(aa.comp, function(x) x[, &quot;Mole%&quot;])) # Isoelectric point pI.values &lt;- as.matrix(sapply(peptides, function(x) pI(x, pKscale = &quot;EMBOSS&quot;))) colnames(pI.values) &lt;- &quot;pI.value&quot; # Hydrophobicity hydrophobicity.values &lt;- as.matrix(sapply(peptides, function(x) hydrophobicity(x, scale = &quot;KyteDoolittle&quot;))) colnames(hydrophobicity.values) &lt;- &quot;hydrophobicity.value&quot; # Net charge at pH 7 net.charges &lt;- as.matrix(sapply(peptides, function(x) charge(x, pH = 7, pKscale = &quot;EMBOSS&quot;))) colnames(net.charges) &lt;- &quot;net.charge&quot; # Boman index boman.indices &lt;- as.matrix(sapply(peptides, function(x) boman(x))) colnames(boman.indices) &lt;- &quot;boman.index&quot; # combine peptides results peptides_res &lt;- data.frame(cbind(weights, aa.comp.matrix, pI.values, hydrophobicity.values, net.charges, boman.indices)) # ----------- # protr_1.6-2 # ----------- length &lt;- 7 # read in FASTA file sequences &lt;- readFASTA(&quot;./other_data/mb_pilot_peptide_array.fasta&quot;) sequences &lt;- sequences[unlist(lapply(sequences, function(x) nchar(x) &gt; 1))] # calculate amino acid composition descriptors (dim = 20) x1 &lt;- t(sapply(sequences, extractAAC)) colnames(x1)[colnames(x1) == &quot;Y&quot;] &lt;- &quot;Y_tyrosine&quot; # calculate dipeptide composition descriptors (dim = 400) x2 &lt;- t(sapply(sequences, extractDC)) colnames(x2)[colnames(x2) == &quot;NA&quot;] &lt;- &quot;NA_dipeptide&quot; # calculate Moreau-Broto autocorrelation descriptors (dim = 8 * (length - 1)) x3 &lt;- t(sapply(sequences, extractMoreauBroto, nlag = length - 1L)) colnames(x3) &lt;- paste(&quot;moreau_broto&quot;, colnames(x3), sep = &quot;_&quot;) # calculate composition descriptors (dim = 21) x4 &lt;- t(sapply(sequences, extractCTDC)) # calculate transition descriptors (dim = 21) x5 &lt;- t(sapply(sequences, extractCTDT)) # calculate distribution descriptors (dim = 105) x6 &lt;- t(sapply(sequences, extractCTDD)) # calculate conjoint triad descriptors (dim = 343) x7 &lt;- t(sapply(sequences, extractCTriad)) # calculate sequence-order-coupling numbers (dim = 2 * (length - 1)) x8 &lt;- t(sapply(sequences, extractSOCN, nlag = length - 1L)) # calculate quasi-sequence-order descriptors (dim = 40 + 2 * (length - 1)) x9 &lt;- t(sapply(sequences, extractQSO, nlag = length - 1L)) # calculate pseudo-amino acid composition (dim = 20 + (length - 1)) x10 &lt;- t(sapply(sequences, extractPAAC, lambda = length - 1L)) # calculate amphiphilic pseudo-amino acid composition (dim = 20 + 2 * (length - 1)) x11 &lt;- t(sapply(sequences, extractAPAAC, lambda = length - 1L)) # combine all of the result datasets protr_res &lt;- data.frame(cbind(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11)) labels &lt;- read.csv(&quot;./other_data/mb_pilot_peptide_array_labels.csv&quot;, row.names = 1) merge.all &lt;- function(x, ..., by = &quot;row.names&quot;) { L &lt;- list(...) for (i in seq_along(L)) { x &lt;- merge(x, L[[i]], by = by) rownames(x) &lt;- x$Row.names x$Row.names &lt;- NULL } return(x) } data &lt;- merge.all(peptides_res, protr_res, labels) write.csv(data, file = &quot;./data/mb_pilot_peptide_array_ml_input.csv&quot;) 1.3 Training initial RF model data &lt;- read.csv(&quot;./data/mb_pilot_peptide_array_ml_input.csv&quot;, check.names = FALSE, row.names = 1) predictor_variables &lt;- subset(data, select = -category) response_variable &lt;- factor(data$category, levels = c(&quot;non-bind&quot;, &quot;bind&quot;)) # impute missing values response_variable &lt;- na.roughfix(response_variable) # perform balance sampling samp_size &lt;- min(table(response_variable)) # build RF model set.seed(22) ntree &lt;- 100000 rf &lt;- randomForest( x = predictor_variables, y = response_variable, ntree = ntree, sampsize = rep(samp_size, 2), importance = TRUE, proximity = TRUE, do.trace = 0.1 * ntree ) save(rf, file = &quot;./rdata/mb_pilot_rf.RData&quot;) 1.4 Melanin binding variable importance load(file = &quot;./rdata/mb_pilot_rf.RData&quot;) imp_ds &lt;- as.data.frame(importance(rf)[, &quot;MeanDecreaseAccuracy&quot;, drop = FALSE]) imp_ds$`Feature` &lt;- rownames(imp_ds) colnames(imp_ds) &lt;- c(&quot;Mean decrease accuracy&quot;, &quot;Feature&quot;) imp_ds &lt;- imp_ds[order(-imp_ds$`Mean decrease accuracy`), ][1:20, ] p &lt;- ggplot(imp_ds, aes(x = reorder(`Feature`, `Mean decrease accuracy`), y = `Mean decrease accuracy`)) + geom_col(color = &quot;grey10&quot;, fill = &quot;#f5b8b8&quot;, size = 0.2) + geom_text(aes(label = sprintf(&quot;%.2f&quot;, `Mean decrease accuracy`)), nudge_y = 4.2, size = 3) + coord_flip() + ylim(0, max(imp_ds$`Mean decrease accuracy`) + 5) + theme_bw() + theme( panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank(), panel.border = element_blank(), axis.line = element_line(color = &quot;black&quot;), legend.text = element_text(colour = &quot;black&quot;, size = 10), plot.title = element_text(hjust = 0.5, size = 18), plot.margin = ggplot2::margin(10, 10, 10, 0, &quot;pt&quot;), axis.title.x = element_text(colour = &quot;black&quot;, size = 18), axis.title.y = element_text(colour = &quot;black&quot;, size = 18), axis.text.x = element_text(colour = &quot;black&quot;, size = 12), axis.text.y = element_text(colour = &quot;black&quot;, size = 12), legend.title = element_text(size = 14), legend.position = c(0.85, 0.5) ) + xlab(&quot;&quot;) + ggtitle(&quot;&quot;) sessionInfo() ## R version 4.2.2 (2022-10-31) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur ... 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] ggplot2_3.3.6 randomForest_4.7-1.1 plyr_1.8.7 ## [4] protr_1.6-2 stringr_1.4.1 Peptides_2.4.4 ## [7] seqinr_4.2-16 ## ## loaded via a namespace (and not attached): ## [1] styler_1.8.0 tidyselect_1.1.2 xfun_0.32 bslib_0.4.0 ## [5] purrr_0.3.4 colorspace_2.0-3 vctrs_0.4.1 generics_0.1.3 ## [9] htmltools_0.5.3 yaml_2.3.5 utf8_1.2.2 rlang_1.0.4 ## [13] R.oo_1.25.0 jquerylib_0.1.4 pillar_1.8.1 glue_1.6.2 ## [17] withr_2.5.0 DBI_1.1.3 R.utils_2.12.0 R.cache_0.16.0 ## [21] lifecycle_1.0.1 munsell_0.5.0 gtable_0.3.0 R.methodsS3_1.8.2 ## [25] codetools_0.2-18 evaluate_0.16 knitr_1.40 fastmap_1.1.0 ## [29] fansi_1.0.3 highr_0.9 Rcpp_1.0.9 scales_1.2.1 ## [33] cachem_1.0.6 jsonlite_1.8.0 digest_0.6.29 stringi_1.7.8 ## [37] bookdown_0.28 dplyr_1.0.9 grid_4.2.2 ade4_1.7-19 ## [41] cli_3.3.0 tools_4.2.2 magrittr_2.0.3 sass_0.4.2 ## [45] tibble_3.1.8 pkgconfig_2.0.3 MASS_7.3-58.1 assertthat_0.2.1 ## [49] rmarkdown_2.16 rstudioapi_0.14 R6_2.5.1 compiler_4.2.2 "],["id_02_variable_reduction.html", "Section 2 Variable reduction 2.1 Methods 2.2 ML input generating function 2.3 AIC functions 2.4 Melanin binding (regression) 2.5 Cell-penetration (classification) 2.6 Toxicity (classification) 2.7 Plotting", " Section 2 Variable reduction library(Peptides) library(protr) library(rlist) library(ranger) library(ggplot2) library(cowplot) library(scales) 2.1 Methods Formula of AIC \\[\\begin{equation} AIC = -2\\ln(\\hat{L}) + 2k, \\tag{2.1} \\end{equation}\\] \\[\\begin{equation} \\begin{split} \\mathrm{where}\\:\\hat{L}\\:\\mathrm{is\\:the\\:maximum\\:likelihood\\:value,}\\\\ \\mathrm{and}\\:k\\:\\mathrm{is\\:the\\:number\\:of\\:parameters.} \\end{split} \\end{equation}\\] 2.1.1 Regression Likelihood of normal distribution Assume residuals are normally distributed, then \\[\\begin{equation} L(\\theta) = \\prod_{i = 1}^{N} \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{{(\\hat{y}_{i} - y_i)}^2}{2\\sigma^2}} \\tag{2.2} \\end{equation}\\] \\[\\begin{equation} \\begin{split} \\ln{L(\\theta)} &amp; = \\sum_{i = 1}^{N} [\\ln{(\\frac{1}{\\sqrt{2\\pi\\sigma^2}})} - {\\frac{{(\\hat{y}_{i} - y_i)}^2}{2\\sigma^2}}] \\\\ &amp; = -\\frac{N}{2}\\ln{(2\\pi)} -\\frac{N}{2}\\ln{(\\sigma^2)} - \\frac{1}{2\\sigma^2}\\sum_{i = 1}^{N}{{(\\hat{y}_{i} - y_i)}^2.} \\\\ \\end{split} \\tag{2.3} \\end{equation}\\] \\(\\mathrm{Since}\\:{\\hat{\\sigma}^2} = \\frac{\\sum_{i = 1}^{N}{{(\\hat{y}_{i} - y_i)}^2}}{N} = {MSE},\\) \\[\\begin{equation} \\ln{L(\\theta)} = -\\frac{N}{2}\\ln{(MSE)} + C, \\mathrm{where}\\:C\\:\\mathrm{is\\:a\\:constant.} \\tag{2.4} \\end{equation}\\] Derived regression AIC \\[\\begin{equation} AIC_{reg} = N\\ln{(MSE)} + 2k \\tag{2.5} \\end{equation}\\] 2.1.2 Classification Likelihood of Bernoulli distribution \\[\\begin{equation} L(\\theta) = \\prod_{i = 1}^{N} [q_{\\theta}(y_i = 1)^{y_i}q_{\\theta}(y_i = 0)^{1-y_i}] \\tag{2.6} \\end{equation}\\] \\[\\begin{equation} \\begin{split} \\ln{L(\\theta)} = \\sum_{i = 1}^{N} [y_i\\ln{q_{\\theta}(y_i = 1)} + (1 - y_i)\\ln{q_{\\theta}(y_i = 0)}], \\end{split} \\tag{2.7} \\end{equation}\\] \\[\\begin{equation} \\begin{split} \\mathrm{where}\\:q_{\\theta}\\:\\mathrm{is\\:the\\:estimated\\:probability\\:with\\:parameters\\:\\theta}\\:\\mathrm{and\\:}\\:y_i \\in \\{0, 1\\} \\end{split} \\end{equation}\\] Binary cross-entropy \\[\\begin{equation} \\begin{split} H_p(q_{\\theta}) &amp; = -\\frac{1}{N}\\sum_{i = 1}^{N} [y_i\\log_2{q_{\\theta}(y_i = 1)} + (1 - y_i)\\log_2{q_{\\theta}(y_i = 0)}] \\\\ &amp; = -\\frac{1}{N{\\cdot}\\ln{2}}\\sum_{i = 1}^{N} [y_i\\ln{q_{\\theta}(y_i = 1)} + (1 - y_i)\\ln{q_{\\theta}(y_i = 0)}] \\end{split} \\tag{2.8} \\end{equation}\\] Derived classification AIC \\[\\begin{equation} AIC_{clf} = 2{\\cdot}\\ln{2}{\\cdot}N{\\cdot}H_p(q_{\\theta}) + 2k \\tag{2.9} \\end{equation}\\] 2.1.2.1 Extension to multi-class classification Generalized likelihood estimation Assume the classes are one-hot encoded, then we can generalize equation (2.9) to multi-class problems. \\[\\begin{equation} L(\\theta) = \\prod_{i = 1}^{N}\\prod_{j = 1}^{M}\\hat{y}_{ij}^{y_{ij}} \\tag{2.10} \\end{equation}\\] \\[\\begin{equation} \\begin{split} \\ln{L(\\theta)} = \\sum_{i = 1}^{N}\\sum_{j = 1}^{M}y_{ij}\\ln{\\hat{y}_{ij}}, \\end{split} \\tag{2.11} \\end{equation}\\] \\[\\begin{equation} \\begin{split} \\mathrm{where}\\:\\hat{y}_{ij}\\:\\mathrm{is\\:the\\:estimated\\:probability\\:of\\:class}\\:j\\:\\mathrm{of\\:sample\\:}i\\:\\mathrm{and\\:}\\:y_{ij} \\in \\{0, 1\\} \\end{split} \\end{equation}\\] Categorical cross-entropy \\[\\begin{equation} H_p(q_{\\theta}) = -\\frac{1}{N}\\sum_{i = 1}^{N}\\sum_{j = 1}^{M}y_{ij}\\log_2{\\hat{y}_{ij}} \\tag{2.12} \\end{equation}\\] Derived generalized classification AIC \\[\\begin{equation} AIC_{clf} = 2{\\cdot}\\ln{2}{\\cdot}N{\\cdot}H_p(q_{\\theta}) + 2k \\tag{2.13} \\end{equation}\\] 2.2 ML input generating function generate_ml_input &lt;- function(fasta_file, label_file, output_file) { # -------------- # Peptides_2.4.4 # -------------- # read in fasta file peptides &lt;- read.fasta(fasta_file, seqtype = &quot;AA&quot;, as.string = TRUE, set.attributes = FALSE) # Molecular weight weights &lt;- as.matrix(sapply(peptides, function(x) mw(x, monoisotopic = FALSE))) colnames(weights) &lt;- &quot;weight&quot; # Amino acid composition aa.comp &lt;- sapply(peptides, function(x) aaComp(x)) aa.comp.matrix &lt;- t(sapply(aa.comp, function(x) x[, &quot;Mole%&quot;])) # Isoelectric point pI.values &lt;- as.matrix(sapply(peptides, function(x) pI(x, pKscale = &quot;EMBOSS&quot;))) colnames(pI.values) &lt;- &quot;pI.value&quot; # Hydrophobicity hydrophobicity.values &lt;- as.matrix(sapply(peptides, function(x) hydrophobicity(x, scale = &quot;KyteDoolittle&quot;))) colnames(hydrophobicity.values) &lt;- &quot;hydrophobicity.value&quot; # Net charge at pH 7 net.charges &lt;- as.matrix(sapply(peptides, function(x) charge(x, pH = 7, pKscale = &quot;EMBOSS&quot;))) colnames(net.charges) &lt;- &quot;net.charge&quot; # Boman index boman.indices &lt;- as.matrix(sapply(peptides, function(x) boman(x))) colnames(boman.indices) &lt;- &quot;boman.index&quot; # combine peptides results peptides_res &lt;- data.frame(cbind(weights, aa.comp.matrix, pI.values, hydrophobicity.values, net.charges, boman.indices)) # ----------- # protr_1.6-2 # ----------- length &lt;- 7 # read in FASTA file sequences &lt;- readFASTA(fasta_file) sequences &lt;- sequences[unlist(lapply(sequences, function(x) nchar(x) &gt; 1))] # calculate amino acid composition descriptors (dim = 20) x1 &lt;- t(sapply(sequences, extractAAC)) colnames(x1)[colnames(x1) == &quot;Y&quot;] &lt;- &quot;Y_tyrosine&quot; # calculate dipeptide composition descriptors (dim = 400) x2 &lt;- t(sapply(sequences, extractDC)) colnames(x2)[colnames(x2) == &quot;NA&quot;] &lt;- &quot;NA_dipeptide&quot; # calculate Moreau-Broto autocorrelation descriptors (dim = 8 * (length - 1)) x3 &lt;- t(sapply(sequences, extractMoreauBroto, nlag = length - 1L)) colnames(x3) &lt;- paste(&quot;moreau_broto&quot;, colnames(x3), sep = &quot;_&quot;) # calculate composition descriptors (dim = 21) x4 &lt;- t(sapply(sequences, extractCTDC)) # calculate transition descriptors (dim = 21) x5 &lt;- t(sapply(sequences, extractCTDT)) # calculate distribution descriptors (dim = 105) x6 &lt;- t(sapply(sequences, extractCTDD)) # calculate conjoint triad descriptors (dim = 343) x7 &lt;- t(sapply(sequences, extractCTriad)) # calculate sequence-order-coupling numbers (dim = 2 * (length - 1)) x8 &lt;- t(sapply(sequences, extractSOCN, nlag = length - 1L)) # calculate quasi-sequence-order descriptors (dim = 40 + 2 * (length - 1)) x9 &lt;- t(sapply(sequences, extractQSO, nlag = length - 1L)) # calculate pseudo-amino acid composition (dim = 20 + (length - 1)) x10 &lt;- t(sapply(sequences, extractPAAC, lambda = length - 1L)) # calculate amphiphilic pseudo-amino acid composition (dim = 20 + 2 * (length - 1)) x11 &lt;- t(sapply(sequences, extractAPAAC, lambda = length - 1L)) # combine all of the result datasets protr_res &lt;- data.frame(cbind(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11)) labels &lt;- read.csv(label_file, row.names = 1) merge.all &lt;- function(x, ..., by = &quot;row.names&quot;) { # https://stackoverflow.com/a/22618297 L &lt;- list(...) for (i in seq_along(L)) { x &lt;- merge(x, L[[i]], by = by) rownames(x) &lt;- x$Row.names x$Row.names &lt;- NULL } return(x) } data &lt;- merge.all(peptides_res, protr_res, labels) write.csv(data, file = output_file) } 2.3 AIC functions #&#39; Mean squared error function #&#39; #&#39; @param y_true a factor vector representing true values. #&#39; @param y_pred a numeric vector or a matrix of predicted values. #&#39; #&#39; @return MSE #&#39; mse &lt;- function(y_true, y_pred) { return(mean((y_true - y_pred)^2)) } #&#39; Regression AIC #&#39; #&#39; @param y_true a factor vector representing true values. #&#39; @param y_pred a numeric vector or a matrix of predicted values. #&#39; @param k number of parameters/variables. #&#39; @param eps a very small value to avoid negative infinitives generated by log(p=0). #&#39; #&#39; @return AIC #&#39; aic_reg &lt;- function(y_true, y_pred, k, eps = 1e-15) { mserr &lt;- mse(y_true, y_pred) if (mserr == 0) mserr &lt;- eps AIC &lt;- length(y_true) * log(mserr) + 2 * k return(AIC) } #&#39; Cross-entropy function #&#39; #&#39; @param y_true a factor vector representing true labels. #&#39; @param y_pred a numeric vector (probabilities of the second class/factor level) #&#39; or a matrix of predicted probabilities. #&#39; @param eps a very small value to avoid negative infinitives generated by log(p=0). #&#39; If the function returns NaN, then increasing eps may solve the issue. Alternatively, #&#39; As NaN is usually generated in the case of p = 1 - eps = 1, resulting in log(1 - p) #&#39; = log(0) from binary cross-entropy, the user can pass prediction values as a matrix #&#39; to calculate categorical cross-entropy instead. #&#39; #&#39; @return H, cross-entropy (mean loss per sample) #&#39; crossEntropy &lt;- function(y_true, y_pred, eps = 1e-15) { stopifnot(is.factor(y_true)) y_pred &lt;- pmax(pmin(y_pred, 1 - eps), eps) n_levels &lt;- nlevels(y_true) H &lt;- NULL # Binary classification if (n_levels == 2 &amp;&amp; is.vector(y_pred)) { y_true &lt;- as.numeric(y_true == levels(y_true)[-1]) H &lt;- -mean(y_true * log2(y_pred) + (1 - y_true) * log2(1 - y_pred)) } # Multi-class classification else if (n_levels == ncol(y_pred)) { y_true &lt;- as.numeric(y_true) y_true_encoded &lt;- t(sapply(y_true, function(x) { tmp &lt;- rep(0, n_levels) tmp[x] &lt;- 1 return(tmp) })) H &lt;- -mean(sapply(1:nrow(y_true_encoded), function(x) sum(y_true_encoded[x, ] * log2(y_pred[x, ])))) } return(H) } #&#39; Classification AIC #&#39; #&#39; @param y_true a factor vector representing true labels. #&#39; @param y_pred a numeric vector or a matrix of predicted probabilities. #&#39; @param k number of parameters/variables. #&#39; @param ... other parameters to be passed to `crossEntropy()` #&#39; #&#39; @return AIC #&#39; aic_clf &lt;- function(y_true, y_pred, k, ...) { H &lt;- crossEntropy(y_true, y_pred, ...) AIC &lt;- 2 * log(2) * length(y_true) * H + 2 * k return(AIC) } 2.4 Melanin binding (regression) 2.4.1 Generating ML input generate_ml_input( fasta_file = &quot;./other_data/mb_second_peptide_array.fasta&quot;, label_file = &quot;./other_data/mb_second_peptide_array_labels.csv&quot;, output_file = &quot;./data/mb_second_peptide_array_ml_input.csv&quot; ) 2.4.2 Variable importance set.seed(12) mb_data &lt;- read.csv(&quot;./data/mb_second_peptide_array_ml_input.csv&quot;, check.names = FALSE, row.names = 1) # random shuffle mb_data &lt;- mb_data[sample(1:nrow(mb_data), replace = FALSE), ] # train random forest using ranger ntree &lt;- 100000 predictor_variables &lt;- subset(mb_data, select = -log_intensity) response_variable &lt;- mb_data$log_intensity rf &lt;- ranger( x = predictor_variables, y = response_variable, num.trees = ntree, importance = &quot;permutation&quot;, verbose = TRUE, scale.permutation.importance = TRUE, seed = 3, keep.inbag = TRUE ) save(rf, file = &quot;./rdata/var_reduct_mb_rf.RData&quot;) 2.4.3 Variable reduction load(file = &quot;./rdata/var_reduct_mb_rf.RData&quot;) var_imp &lt;- ranger::importance(rf) var_imp &lt;- var_imp[order(-var_imp)] var_imp &lt;- var_imp[var_imp &gt;= 0] set.seed(12) mb_data &lt;- read.csv(&quot;./data/mb_second_peptide_array_ml_input.csv&quot;, check.names = FALSE, row.names = 1) # random shuffle mb_data &lt;- mb_data[sample(1:nrow(mb_data), replace = FALSE), ] ntree &lt;- 1000 var_subset_res &lt;- list() for (i in 1:length(var_imp)) { cat(paste0(i, &quot;\\n&quot;)) predictor_variables &lt;- mb_data[, colnames(mb_data) %in% names(var_imp)[1:i], drop = FALSE] response_variable &lt;- mb_data$log_intensity rf &lt;- ranger(x = predictor_variables, y = response_variable, num.trees = ntree, importance = &quot;none&quot;, verbose = TRUE, seed = 3) var_subset_res &lt;- list.append(var_subset_res, list( rsq = rf$r.squared, aic = aic_reg( y_true = response_variable, y_pred = rf$predictions, k = i ) )) } save(var_subset_res, file = &quot;./rdata/var_reduct_mb_var_subset_res.RData&quot;) load(file = &quot;./rdata/var_reduct_mb_var_subset_res.RData&quot;) rsq_vec &lt;- unlist(lapply(var_subset_res, function(x) x$rsq)) aic_vec &lt;- unlist(lapply(var_subset_res, function(x) x$aic)) rsq_cutoff &lt;- which.max(rsq_vec) aic_cutoff &lt;- which.min(aic_vec) p1_1 &lt;- ggplot(data = data.frame(x = 1:length(rsq_vec), y = rsq_vec), aes(x = x, y = y)) + geom_point(colour = &quot;black&quot;, alpha = 0.3, size = 0.5) + geom_vline(xintercept = rsq_cutoff, colour = &quot;red&quot;, linetype = &quot;dashed&quot;) + scale_x_continuous(breaks = c(1, rsq_cutoff, 200, 400, 600, 735)) + scale_y_continuous( breaks = c(0.40, 0.45, 0.50, 0.55), labels = c(&quot;0.40&quot;, &quot;0.45&quot;, &quot;0.50&quot;, &quot;0.55&quot;) ) + theme_bw() + theme( plot.margin = unit(c(10, 10, -10, 10), &quot;pt&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank(), panel.border = element_blank(), axis.line = element_line(color = &quot;black&quot;), plot.title = element_text(hjust = 0.5), axis.title = element_text(colour = &quot;black&quot;), axis.text = element_text(colour = &quot;black&quot;) ) + xlab(&quot;&quot;) + ylab(expression(italic(R^2))) + ggtitle(&quot;Melanin binding&quot;) p1_2 &lt;- ggplot(data = data.frame(x = 1:length(aic_vec), y = aic_vec), aes(x = x, y = y)) + geom_point(colour = &quot;black&quot;, alpha = 0.3, size = 0.5) + geom_vline(xintercept = aic_cutoff, colour = &quot;red&quot;, linetype = &quot;dashed&quot;) + scale_x_continuous(breaks = c(1, aic_cutoff, 200, 400, 600, 735)) + scale_y_continuous( breaks = c(-500, 0, 500, 1000), labels = c(&quot;-500&quot;, &quot;0&quot;, &quot;500&quot;, &quot;1,000&quot;) ) + theme_bw() + theme( plot.margin = unit(c(-15, 10, 10, 10), &quot;pt&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank(), panel.border = element_blank(), axis.line = element_line(color = &quot;black&quot;), plot.title = element_text(hjust = 0.5), axis.title = element_text(colour = &quot;black&quot;), axis.text = element_text(colour = &quot;black&quot;) ) + xlab(&quot;Number of variables&quot;) + ylab(&quot;AIC&quot;) + ggtitle(&quot;&quot;) cat(paste0(&quot;Maximum R-squared: &quot;, formatC(rsq_vec[rsq_cutoff], format = &quot;f&quot;, digits = 3), &quot;\\n&quot;)) cat(paste0(&quot;R-squared at AIC cutoff: &quot;, formatC(rsq_vec[aic_cutoff], format = &quot;f&quot;, digits = 3), &quot;\\n&quot;)) 2.4.4 Generating train-test splits set.seed(12) mb_data &lt;- read.csv(&quot;./data/mb_second_peptide_array_ml_input.csv&quot;, check.names = FALSE, row.names = 1) # random shuffle mb_data &lt;- mb_data[sample(1:nrow(mb_data), replace = FALSE), ] cutoff &lt;- aic_cutoff load(file = &quot;./rdata/var_reduct_mb_rf.RData&quot;) var_imp &lt;- ranger::importance(rf) var_imp &lt;- var_imp[order(-var_imp)][1:cutoff] mb_data &lt;- mb_data[, colnames(mb_data) %in% c(names(var_imp), &quot;log_intensity&quot;)] outer_splits &lt;- list() n_train &lt;- round(nrow(mb_data) * 0.9) set.seed(22) for (i in 1:10) { # random shuffle mb_data_ &lt;- mb_data[sample(1:nrow(mb_data), replace = FALSE), ] outer_train &lt;- mb_data_[1:n_train, ] outer_test &lt;- mb_data_[(n_train + 1):nrow(mb_data_), ] outer_splits &lt;- list.append(outer_splits, list(train = outer_train, test = outer_test)) } save(mb_data, outer_splits, file = &quot;./rdata/var_reduct_mb_train_test_splits.RData&quot;) 2.5 Cell-penetration (classification) 2.5.1 Generating ML input generate_ml_input( fasta_file = &quot;./other_data/CPP924.fasta&quot;, label_file = &quot;./other_data/cpp_labels.csv&quot;, output_file = &quot;./data/cpp_ml_input.csv&quot; ) 2.5.2 Variable importance set.seed(12) cpp_data &lt;- read.csv(&quot;./data/cpp_ml_input.csv&quot;, check.names = FALSE, row.names = 1) # random shuffle cpp_data &lt;- cpp_data[sample(1:nrow(cpp_data), replace = FALSE), ] # train random forest using ranger ntree &lt;- 100000 predictor_variables &lt;- subset(cpp_data, select = -category) response_variable &lt;- factor(cpp_data$category, levels = c(&quot;non-penetrating&quot;, &quot;penetrating&quot;)) rf &lt;- ranger( x = predictor_variables, y = response_variable, num.trees = ntree, sample.fraction = rep(0.5, 2), importance = &quot;permutation&quot;, verbose = TRUE, scale.permutation.importance = TRUE, seed = 3, keep.inbag = TRUE ) save(rf, file = &quot;./rdata/var_reduct_cpp_rf.RData&quot;) 2.5.3 Variable reduction load(file = &quot;./rdata/var_reduct_cpp_rf.RData&quot;) var_imp &lt;- ranger::importance(rf) var_imp &lt;- var_imp[order(-var_imp)] var_imp &lt;- var_imp[var_imp &gt;= 0] set.seed(12) cpp_data &lt;- read.csv(&quot;./data/cpp_ml_input.csv&quot;, check.names = FALSE, row.names = 1) # random shuffle cpp_data &lt;- cpp_data[sample(1:nrow(cpp_data), replace = FALSE), ] ntree &lt;- 1000 var_subset_res &lt;- list() for (i in 1:length(var_imp)) { cat(paste0(i, &quot;\\n&quot;)) predictor_variables &lt;- cpp_data[, colnames(cpp_data) %in% names(var_imp)[1:i], drop = FALSE] response_variable &lt;- factor(cpp_data$category, levels = c(&quot;non-penetrating&quot;, &quot;penetrating&quot;)) rf &lt;- ranger( x = predictor_variables, y = response_variable, num.trees = ntree, importance = &quot;none&quot;, verbose = TRUE, seed = 3, probability = TRUE ) var_subset_res &lt;- list.append(var_subset_res, list( acc = 1 - rf$prediction.error, aic = aic_clf( y_true = response_variable, y_pred = rf$predictions, k = i ) )) } save(var_subset_res, file = &quot;./rdata/var_reduct_cpp_var_subset_res.RData&quot;) load(file = &quot;./rdata/var_reduct_cpp_var_subset_res.RData&quot;) acc_vec &lt;- unlist(lapply(var_subset_res, function(x) x$acc)) aic_vec &lt;- unlist(lapply(var_subset_res, function(x) x$aic)) acc_cutoff &lt;- which.max(acc_vec) aic_cutoff &lt;- which.min(aic_vec) p2_1 &lt;- ggplot(data = data.frame(x = 1:length(acc_vec), y = acc_vec), aes(x = x, y = y)) + geom_point(colour = &quot;black&quot;, alpha = 0.3, size = 0.5) + geom_vline(xintercept = acc_cutoff, colour = &quot;red&quot;, linetype = &quot;dashed&quot;) + scale_x_continuous(breaks = c(1, acc_cutoff, 250, 500, 750, 955)) + scale_y_continuous( breaks = c(0.88, 0.90, 0.92), labels = c(&quot;0.88&quot;, &quot;0.90&quot;, &quot;0.92&quot;) ) + theme_bw() + theme( plot.margin = unit(c(10, 10, -10, 10), &quot;pt&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank(), panel.border = element_blank(), axis.line = element_line(color = &quot;black&quot;), plot.title = element_text(hjust = 0.5), axis.title = element_text(colour = &quot;black&quot;), axis.text = element_text(colour = &quot;black&quot;) ) + xlab(&quot;&quot;) + ylab(&quot;Accuracy&quot;) + ggtitle(&quot;Cell-penetration&quot;) p2_2 &lt;- ggplot(data = data.frame(x = 1:length(aic_vec), y = aic_vec), aes(x = x, y = y)) + geom_point(colour = &quot;black&quot;, alpha = 0.3, size = 0.5) + geom_vline(xintercept = aic_cutoff, colour = &quot;red&quot;, linetype = &quot;dashed&quot;) + scale_x_continuous(breaks = c(aic_cutoff, 250, 500, 750, 955)) + scale_y_continuous( breaks = c(500, 1000, 1500, 2000, 2500), labels = c(&quot;500&quot;, &quot;1,000&quot;, &quot;1,500&quot;, &quot;2,000&quot;, &quot;2,500&quot;) ) + theme_bw() + theme( plot.margin = unit(c(-15, 10, 10, 10), &quot;pt&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank(), panel.border = element_blank(), axis.line = element_line(color = &quot;black&quot;), plot.title = element_text(hjust = 0.5), axis.title = element_text(colour = &quot;black&quot;), axis.text = element_text(colour = &quot;black&quot;) ) + xlab(&quot;Number of variables&quot;) + ylab(&quot;AIC&quot;) + ggtitle(&quot;&quot;) cat(paste0(&quot;Maximum accuracy: &quot;, formatC(acc_vec[acc_cutoff], format = &quot;f&quot;, digits = 3), &quot;\\n&quot;)) cat(paste0(&quot;Accuracy at AIC cutoff: &quot;, formatC(acc_vec[aic_cutoff], format = &quot;f&quot;, digits = 3), &quot;\\n&quot;)) 2.5.4 Generating train-test splits set.seed(12) cpp_data &lt;- read.csv(&quot;./data/cpp_ml_input.csv&quot;, check.names = FALSE, row.names = 1) # random shuffle cpp_data &lt;- cpp_data[sample(1:nrow(cpp_data), replace = FALSE), ] cutoff &lt;- aic_cutoff load(file = &quot;./rdata/var_reduct_cpp_rf.RData&quot;) var_imp &lt;- ranger::importance(rf) var_imp &lt;- var_imp[order(-var_imp)][1:cutoff] cpp_data &lt;- cpp_data[, colnames(cpp_data) %in% c(names(var_imp), &quot;category&quot;)] outer_splits &lt;- list() n_train &lt;- round(nrow(cpp_data) * 0.9) set.seed(22) for (i in 1:10) { # random shuffle cpp_data_ &lt;- cpp_data[sample(1:nrow(cpp_data), replace = FALSE), ] outer_train &lt;- cpp_data_[1:n_train, ] outer_test &lt;- cpp_data_[(n_train + 1):nrow(cpp_data_), ] outer_splits &lt;- list.append(outer_splits, list(train = outer_train, test = outer_test)) } save(cpp_data, outer_splits, file = &quot;./rdata/var_reduct_cpp_train_test_splits.RData&quot;) 2.6 Toxicity (classification) 2.6.1 Generating ML input generate_ml_input( fasta_file = &quot;./other_data/toxicity.fasta&quot;, label_file = &quot;./other_data/tx_labels.csv&quot;, output_file = &quot;./data/tx_ml_input.csv&quot; ) 2.6.2 Variable importance set.seed(12) tx_data &lt;- read.csv(&quot;./data/tx_ml_input.csv&quot;, check.names = FALSE, row.names = 1) # random shuffle tx_data &lt;- tx_data[sample(1:nrow(tx_data), replace = FALSE), ] # train random forest using ranger ntree &lt;- 100000 predictor_variables &lt;- subset(tx_data, select = -category) response_variable &lt;- factor(tx_data$category, levels = c(&quot;non-toxic&quot;, &quot;toxic&quot;)) rf &lt;- ranger( x = predictor_variables, y = response_variable, num.trees = ntree, sample.fraction = rep(0.5, 2), importance = &quot;permutation&quot;, verbose = TRUE, scale.permutation.importance = TRUE, seed = 3, keep.inbag = TRUE ) save(rf, file = &quot;./rdata/var_reduct_tx_rf.RData&quot;) 2.6.3 Variable reduction load(file = &quot;./rdata/var_reduct_tx_rf.RData&quot;) var_imp &lt;- ranger::importance(rf) var_imp &lt;- var_imp[order(-var_imp)] var_imp &lt;- var_imp[var_imp &gt;= 0] set.seed(12) tx_data &lt;- read.csv(&quot;./data/tx_ml_input.csv&quot;, check.names = FALSE, row.names = 1) # random shuffle tx_data &lt;- tx_data[sample(1:nrow(tx_data), replace = FALSE), ] ntree &lt;- 1000 var_subset_res &lt;- list() for (i in 1:length(var_imp)) { cat(paste0(i, &quot;\\n&quot;)) predictor_variables &lt;- tx_data[, colnames(tx_data) %in% names(var_imp)[1:i], drop = FALSE] response_variable &lt;- factor(tx_data$category, levels = c(&quot;non-toxic&quot;, &quot;toxic&quot;)) rf &lt;- ranger( x = predictor_variables, y = response_variable, num.trees = ntree, importance = &quot;none&quot;, verbose = TRUE, seed = 3, probability = TRUE ) var_subset_res &lt;- list.append(var_subset_res, list( acc = 1 - rf$prediction.error, aic = aic_clf( y_true = response_variable, y_pred = rf$predictions, k = i ) )) } save(var_subset_res, file = &quot;./rdata/var_reduct_tx_var_subset_res.RData&quot;) load(file = &quot;./rdata/var_reduct_tx_var_subset_res.RData&quot;) acc_vec &lt;- unlist(lapply(var_subset_res, function(x) x$acc)) aic_vec &lt;- unlist(lapply(var_subset_res, function(x) x$aic)) acc_cutoff &lt;- which.max(acc_vec) aic_cutoff &lt;- which.min(aic_vec) p3_1 &lt;- ggplot(data = data.frame(x = 1:length(acc_vec), y = acc_vec), aes(x = x, y = y)) + geom_point(colour = &quot;black&quot;, alpha = 0.3, size = 0.5) + geom_vline(xintercept = acc_cutoff, colour = &quot;red&quot;, linetype = &quot;dashed&quot;) + scale_x_continuous( breaks = c(1, acc_cutoff, 300, 600, 900, 1076), labels = c(&quot;1&quot;, &quot;148&quot;, &quot;300&quot;, &quot;600&quot;, &quot;900&quot;, &quot;1,076&quot;) ) + scale_y_continuous( breaks = c(0.91, 0.92, 0.93, 0.94, 0.95, 0.96), labels = c(&quot;0.91&quot;, &quot;0.92&quot;, &quot;0.93&quot;, &quot;0.94&quot;, &quot;0.95&quot;, &quot;0.96&quot;) ) + theme_bw() + theme( plot.margin = unit(c(10, 10, -10, 10), &quot;pt&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank(), panel.border = element_blank(), axis.line = element_line(color = &quot;black&quot;), plot.title = element_text(hjust = 0.5), axis.title = element_text(colour = &quot;black&quot;), axis.text = element_text(colour = &quot;black&quot;) ) + xlab(&quot;&quot;) + ylab(&quot;Accuracy&quot;) + ggtitle(&quot;Toxicity&quot;) p3_2 &lt;- ggplot(data = data.frame(x = 1:length(aic_vec), y = aic_vec), aes(x = x, y = y)) + geom_point(colour = &quot;black&quot;, alpha = 0.3, size = 0.5) + geom_vline(xintercept = aic_cutoff, colour = &quot;red&quot;, linetype = &quot;dashed&quot;) + scale_x_continuous( breaks = c(aic_cutoff, 300, 600, 900, 1076), labels = c(&quot;56&quot;, &quot;300&quot;, &quot;600&quot;, &quot;900&quot;, &quot;1,076&quot;) ) + scale_y_continuous( breaks = c(2000, 3000, 4000, 5000, 6000, 7000), labels = c(&quot;2,000&quot;, &quot;3,000&quot;, &quot;4,000&quot;, &quot;5,000&quot;, &quot;6,000&quot;, &quot;7,000&quot;) ) + theme_bw() + theme( plot.margin = unit(c(-15, 10, 10, 10), &quot;pt&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank(), panel.border = element_blank(), axis.line = element_line(color = &quot;black&quot;), plot.title = element_text(hjust = 0.5), axis.title = element_text(colour = &quot;black&quot;), axis.text = element_text(colour = &quot;black&quot;) ) + xlab(&quot;Number of variables&quot;) + ylab(&quot;AIC&quot;) + ggtitle(&quot;&quot;) cat(paste0(&quot;Maximum accuracy: &quot;, formatC(acc_vec[acc_cutoff], format = &quot;f&quot;, digits = 3), &quot;\\n&quot;)) cat(paste0(&quot;Accuracy at AIC cutoff: &quot;, formatC(acc_vec[aic_cutoff], format = &quot;f&quot;, digits = 3), &quot;\\n&quot;)) 2.6.4 Generating train-test splits set.seed(12) tx_data &lt;- read.csv(&quot;./data/tx_ml_input.csv&quot;, check.names = FALSE, row.names = 1) # random shuffle tx_data &lt;- tx_data[sample(1:nrow(tx_data), replace = FALSE), ] # convert category so that positive class represents non-toxic # for evaluation metrics that focus on true positives more than # true negatives tx_data$category &lt;- as.factor(sapply(tx_data$category, function(x) if (x == &quot;non-toxic&quot;) 1 else 0)) cutoff &lt;- aic_cutoff load(file = &quot;./rdata/var_reduct_tx_rf.RData&quot;) var_imp &lt;- ranger::importance(rf) var_imp &lt;- var_imp[order(-var_imp)][1:cutoff] tx_data &lt;- tx_data[, colnames(tx_data) %in% c(names(var_imp), &quot;category&quot;)] outer_splits &lt;- list() n_train &lt;- round(nrow(tx_data) * 0.9) set.seed(22) for (i in 1:10) { # random shuffle tx_data_ &lt;- tx_data[sample(1:nrow(tx_data), replace = FALSE), ] outer_train &lt;- tx_data_[1:n_train, ] outer_test &lt;- tx_data_[(n_train + 1):nrow(tx_data_), ] outer_splits &lt;- list.append(outer_splits, list(train = outer_train, test = outer_test)) } save(tx_data, outer_splits, file = &quot;./rdata/var_reduct_tx_train_test_splits.RData&quot;) 2.7 Plotting p_combined &lt;- plot_grid(p1_1, p2_1, p3_1, p1_2, p2_2, p3_2, nrow = 2, labels = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;), align = &quot;v&quot; ) p_combined sessionInfo() ## R version 4.2.2 (2022-10-31) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur ... 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] scales_1.2.1 cowplot_1.1.1 ggplot2_3.3.6 ranger_0.14.1 rlist_0.4.6.2 ## [6] protr_1.6-2 Peptides_2.4.4 ## ## loaded via a namespace (and not attached): ## [1] styler_1.8.0 tidyselect_1.1.2 xfun_0.32 bslib_0.4.0 ## [5] purrr_0.3.4 lattice_0.20-45 colorspace_2.0-3 vctrs_0.4.1 ## [9] generics_0.1.3 htmltools_0.5.3 yaml_2.3.5 utf8_1.2.2 ## [13] rlang_1.0.4 R.oo_1.25.0 jquerylib_0.1.4 pillar_1.8.1 ## [17] withr_2.5.0 glue_1.6.2 DBI_1.1.3 R.utils_2.12.0 ## [21] R.cache_0.16.0 lifecycle_1.0.1 stringr_1.4.1 munsell_0.5.0 ## [25] gtable_0.3.0 R.methodsS3_1.8.2 codetools_0.2-18 evaluate_0.16 ## [29] knitr_1.40 fastmap_1.1.0 fansi_1.0.3 highr_0.9 ## [33] Rcpp_1.0.9 cachem_1.0.6 jsonlite_1.8.0 digest_0.6.29 ## [37] stringi_1.7.8 bookdown_0.28 dplyr_1.0.9 grid_4.2.2 ## [41] cli_3.3.0 tools_4.2.2 magrittr_2.0.3 sass_0.4.2 ## [45] tibble_3.1.8 pkgconfig_2.0.3 Matrix_1.5-1 data.table_1.14.2 ## [49] assertthat_0.2.1 rmarkdown_2.16 rstudioapi_0.14 R6_2.5.1 ## [53] compiler_4.2.2 "],["id_03_model_training.html", "Section 3 Model training 3.1 Overview 3.2 General code/functions 3.3 Melanin binding (regression) 3.4 Cell-penetration (classification) 3.5 Toxicity (classification)", " Section 3 Model training library(h2o) library(MLmetrics) library(mltools) library(scales) library(enrichvs) library(rlist) library(stringr) library(digest) library(DT) library(reshape2) 3.1 Overview 3.2 General code/functions # h2o.init(nthreads=-1, max_mem_size=&#39;100G&#39;, port=54321) h2o.init(nthreads = -1) h2o.removeAll() # DeepLearning Grid 1 deeplearning_params_1 &lt;- list( activation = &quot;RectifierWithDropout&quot;, epochs = 10000, # early stopping epsilon = c(1e-6, 1e-7, 1e-8, 1e-9), hidden = list(c(50), c(200), c(500)), hidden_dropout_ratios = list(c(0.1), c(0.2), c(0.3), c(0.4), c(0.5)), input_dropout_ratio = c(0, 0.05, 0.1, 0.15, 0.2), rho = c(0.9, 0.95, 0.99) ) # DeepLearning Grid 2 deeplearning_params_2 &lt;- list( activation = &quot;RectifierWithDropout&quot;, epochs = 10000, # early stopping epsilon = c(1e-6, 1e-7, 1e-8, 1e-9), hidden = list(c(50, 50), c(200, 200), c(500, 500)), hidden_dropout_ratios = list( c(0.1, 0.1), c(0.2, 0.2), c(0.3, 0.3), c(0.4, 0.4), c(0.5, 0.5) ), input_dropout_ratio = c(0, 0.05, 0.1, 0.15, 0.2), rho = c(0.9, 0.95, 0.99) ) # DeepLearning Grid 3 deeplearning_params_3 &lt;- list( activation = &quot;RectifierWithDropout&quot;, epochs = 10000, # early stopping epsilon = c(1e-6, 1e-7, 1e-8, 1e-9), hidden = list(c(50, 50, 50), c(200, 200, 200), c(500, 500, 500)), hidden_dropout_ratios = list( c(0.1, 0.1, 0.1), c(0.2, 0.2, 0.2), c(0.3, 0.3, 0.3), c(0.4, 0.4, 0.4), c(0.5, 0.5, 0.5) ), input_dropout_ratio = c(0, 0.05, 0.1, 0.15, 0.2), rho = c(0.9, 0.95, 0.99) ) # GBM gbm_params &lt;- list( col_sample_rate = c(0.4, 0.7, 1.0), col_sample_rate_per_tree = c(0.4, 0.7, 1.0), learn_rate = 0.1, max_depth = c(3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17), min_rows = c(1, 5, 10, 15, 30, 100), min_split_improvement = c(1e-4, 1e-5), ntrees = 10000, # early stopping sample_rate = c(0.5, 0.6, 0.7, 0.8, 0.9, 1.0) ) # XGBoost xgboost_params &lt;- list( booster = c(&quot;gbtree&quot;, &quot;dart&quot;), col_sample_rate = c(0.6, 0.8, 1.0), col_sample_rate_per_tree = c(0.7, 0.8, 0.9, 1.0), max_depth = c(5, 10, 15, 20), min_rows = c(0.01, 0.1, 1.0, 3.0, 5.0, 10.0, 15.0, 20.0), ntrees = 10000, # early stopping reg_alpha = c(0.001, 0.01, 0.1, 1, 10, 100), reg_lambda = c(0.001, 0.01, 0.1, 0.5, 1), sample_rate = c(0.6, 0.8, 1.0) ) sem &lt;- function(x, na.rm = TRUE) sd(x, na.rm) / sqrt(length(na.omit(x))) statistical_testing &lt;- function(data, metric_vec, decreasing_vec, p_value_threshold = 0.05, num_entries = 10, output_path = NULL) { for (i in 1:length(metric_vec)) { metric &lt;- metric_vec[i] ranked_models &lt;- rownames(data[order(data[, paste(metric, &quot;mean&quot;)], decreasing = decreasing_vec[i]), ]) pval &lt;- c() for (j in 2:length(ranked_models)) { if (sum(is.na(data[ranked_models[j], paste(metric, &quot;CV&quot;, 1:10)]))) { pval &lt;- c(pval, NA) } else { pval &lt;- c(pval, wilcox.test(as.numeric(data[ranked_models[1], paste(metric, &quot;CV&quot;, 1:10)]), as.numeric(data[ranked_models[j], paste(metric, &quot;CV&quot;, 1:10)]), exact = FALSE )$p.value) } } adj_pval &lt;- c(NA, p.adjust(pval, method = &quot;BH&quot;, n = length(pval))) df &lt;- data.frame(adj_pval) rownames(df) &lt;- ranked_models colnames(df) &lt;- paste(metric, &quot;adj. &lt;i&gt;p&lt;/i&gt; value&quot;) data &lt;- merge(data, df, by = &quot;row.names&quot;, all = TRUE) rownames(data) &lt;- data$Row.names data$Row.names &lt;- NULL } for (i in 1:length(metric_vec)) { if (decreasing_vec[i]) { data[paste(metric_vec[i], &quot;rank&quot;)] &lt;- rank(-data[, paste(metric_vec[i], &quot;mean&quot;)], ties.method = &quot;average&quot;) } else { data[paste(metric_vec[i], &quot;rank&quot;)] &lt;- rank(data[, paste(metric_vec[i], &quot;mean&quot;)], ties.method = &quot;average&quot;) } } data[&quot;Rank sum&quot;] &lt;- rowSums(data[(ncol(data) - length(metric_vec) + 1):ncol(data)]) data &lt;- data[order(data$`Rank sum`), ] competitive &lt;- rowSums(data[paste(metric_vec, &quot;adj. &lt;i&gt;p&lt;/i&gt; value&quot;)] &lt; p_value_threshold) == 0 competitive[is.na(competitive)] &lt;- TRUE data &lt;- data[competitive, ] if (!is.null(output_path)) { data[c( paste(metric_vec, &quot;adj. &lt;i&gt;p&lt;/i&gt; value&quot;), paste(metric_vec, &quot;rank&quot;), &quot;Rank sum&quot;, melt(sapply(metric_vec, function(x) paste(x, c(&quot;mean&quot;, &quot;s.e.m.&quot;))))$value )] %&gt;% round(digits = 4) %&gt;% write.csv(., file = output_path) } data[c(paste(metric_vec, &quot;adj. &lt;i&gt;p&lt;/i&gt; value&quot;), paste(metric_vec, &quot;rank&quot;), &quot;Rank sum&quot;)] %&gt;% round(digits = 4) %&gt;% datatable(options = list(pageLength = num_entries), escape = FALSE) } 3.3 Melanin binding (regression) 3.3.1 Model training train_mb_models &lt;- function(train_set, exp_dir, prefix, nfolds = 10, grid_seed = 1) { tmp &lt;- as.h2o(train_set, destination_frame = prefix) y &lt;- &quot;log_intensity&quot; x &lt;- setdiff(names(tmp), y) res &lt;- as.data.frame(tmp$log_intensity) # ------------------- # base model training # ------------------- cat(&quot;Deep learning grid 1\\n&quot;) deeplearning_1 &lt;- h2o.grid( algorithm = &quot;deeplearning&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = deeplearning_params_1, stopping_rounds = 3, keep_cross_validation_models = FALSE, search_criteria = list( strategy = &quot;RandomDiscrete&quot;, max_models = 100, seed = grid_seed ), fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in deeplearning_1@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;Deep learning grid 2\\n&quot;) deeplearning_2 &lt;- h2o.grid( algorithm = &quot;deeplearning&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = deeplearning_params_2, stopping_rounds = 3, keep_cross_validation_models = FALSE, search_criteria = list( strategy = &quot;RandomDiscrete&quot;, max_models = 100, seed = grid_seed ), fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in deeplearning_2@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;Deep learning grid 3\\n&quot;) deeplearning_3 &lt;- h2o.grid( algorithm = &quot;deeplearning&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = deeplearning_params_3, stopping_rounds = 3, keep_cross_validation_models = FALSE, search_criteria = list( strategy = &quot;RandomDiscrete&quot;, max_models = 100, seed = grid_seed ), fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in deeplearning_3@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;GBM grid\\n&quot;) gbm &lt;- h2o.grid( algorithm = &quot;gbm&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = gbm_params, stopping_rounds = 3, search_criteria = list(strategy = &quot;RandomDiscrete&quot;, max_models = 300, seed = grid_seed), keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in gbm@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;GBM 5 default models\\n&quot;) gbm_1 &lt;- h2o.gbm( model_id = &quot;GBM_1&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 6, min_rows = 1, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_2 &lt;- h2o.gbm( model_id = &quot;GBM_2&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 7, min_rows = 10, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_3 &lt;- h2o.gbm( model_id = &quot;GBM_3&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 8, min_rows = 10, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_4 &lt;- h2o.gbm( model_id = &quot;GBM_4&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 10, min_rows = 10, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_5 &lt;- h2o.gbm( model_id = &quot;GBM_5&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 15, min_rows = 100, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) for (model_id in paste0(&quot;GBM_&quot;, 1:5)) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;XGBoost grid\\n&quot;) xgboost &lt;- h2o.grid( algorithm = &quot;xgboost&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = xgboost_params, stopping_rounds = 3, search_criteria = list(strategy = &quot;RandomDiscrete&quot;, max_models = 300, seed = grid_seed), keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in xgboost@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;XGBoost 3 default models\\n&quot;) xgboost_1 &lt;- h2o.xgboost( model_id = &quot;XGBoost_1&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, booster = &quot;gbtree&quot;, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, max_depth = 10, min_rows = 5, ntrees = 10000, reg_alpha = 0, reg_lambda = 1, sample_rate = 0.6, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) xgboost_2 &lt;- h2o.xgboost( model_id = &quot;XGBoost_2&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, booster = &quot;gbtree&quot;, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, max_depth = 20, min_rows = 10, ntrees = 10000, reg_alpha = 0, reg_lambda = 1, sample_rate = 0.6, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) xgboost_3 &lt;- h2o.xgboost( model_id = &quot;XGBoost_3&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, booster = &quot;gbtree&quot;, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, max_depth = 5, min_rows = 3, ntrees = 10000, reg_alpha = 0, reg_lambda = 1, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) for (model_id in paste0(&quot;XGBoost_&quot;, 1:3)) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;GLM\\n&quot;) glm &lt;- h2o.glm( model_id = &quot;GLM&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, alpha = c(0.0, 0.2, 0.4, 0.6, 0.8, 1.0), keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;GLM&quot;), path = exp_dir, force = TRUE) cat(&quot;DRF\\n&quot;) drf &lt;- h2o.randomForest( model_id = &quot;DRF&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, ntrees = 10000, score_tree_interval = 5, stopping_rounds = 3, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;DRF&quot;), path = exp_dir, force = TRUE) cat(&quot;XRT\\n&quot;) xrt &lt;- h2o.randomForest( model_id = &quot;XRT&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, ntrees = 10000, histogram_type = &quot;Random&quot;, score_tree_interval = 5, stopping_rounds = 3, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;XRT&quot;), path = exp_dir, force = TRUE) # ----------------------- # get holdout predictions # ----------------------- base_models &lt;- as.list(c( unlist(deeplearning_1@model_ids), unlist(deeplearning_2@model_ids), unlist(deeplearning_3@model_ids), unlist(gbm@model_ids), paste0(&quot;GBM_&quot;, 1:5), unlist(xgboost@model_ids), paste0(&quot;XGBoost_&quot;, 1:3), &quot;GLM&quot;, &quot;DRF&quot;, &quot;XRT&quot; )) for (model_id in base_models) { res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id)), col.names = model_id )) } # ---------------------- # super learner training # ---------------------- sl_iter &lt;- 0 cat(paste0(&quot;Super learner iteration &quot;, sl_iter, &quot; (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = paste0(&quot;superlearner_iter_&quot;, sl_iter), training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list(standardize = TRUE, keep_cross_validation_predictions = TRUE) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(paste0(&quot;superlearner_iter_&quot;, sl_iter)), path = exp_dir, force = TRUE) model_id &lt;- paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter) tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id)), col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) # ---------------------------------- # super learner base model reduction # ---------------------------------- while (TRUE) { meta &lt;- h2o.getModel(paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter, &quot;_cv_1&quot;)) names &lt;- meta@model$coefficients_table[, &quot;names&quot;] coeffs &lt;- (meta@model$coefficients_table[, &quot;standardized_coefficients&quot;] &gt; 0) for (j in 2:nfolds) { meta &lt;- h2o.getModel(paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter, &quot;_cv_&quot;, j)) names &lt;- meta@model$coefficients_table[, &quot;names&quot;] coeffs &lt;- coeffs + (meta@model$coefficients_table[, &quot;standardized_coefficients&quot;] &gt; 0) } base_models_ &lt;- as.list(names[coeffs &gt;= ceiling(nfolds / 2) &amp; names != &quot;Intercept&quot;]) if (length(base_models_) == 0) { cat(&quot;No base models passing the threshold\\n\\n&quot;) break } if (sum(base_models %in% base_models_) == length(base_models)) { cat(&quot;No further reduction of base models\\n\\n&quot;) break } sl_iter &lt;- sl_iter + 1 base_models &lt;- base_models_ cat(paste0(&quot;Super learner iteration &quot;, sl_iter, &quot; (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = paste0(&quot;superlearner_iter_&quot;, sl_iter), training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list(standardize = TRUE, keep_cross_validation_predictions = TRUE) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(paste0(&quot;superlearner_iter_&quot;, sl_iter)), path = exp_dir, force = TRUE) model_id &lt;- paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter) tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id)), col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) } # ----------------------------------------- # super learner for homogeneous base models # ----------------------------------------- # DeepLearning base_models &lt;- as.list(c( unlist(deeplearning_1@model_ids), unlist(deeplearning_2@model_ids), unlist(deeplearning_3@model_ids) )) cat(paste0(&quot;Super learner deep learning (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = &quot;superlearner_deeplearning&quot;, training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list(standardize = TRUE, keep_cross_validation_predictions = TRUE) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;superlearner_deeplearning&quot;), path = exp_dir, force = TRUE) model_id &lt;- &quot;metalearner_glm_superlearner_deeplearning&quot; tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id)), col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) # GBM base_models &lt;- as.list(c(unlist(gbm@model_ids), paste0(&quot;GBM_&quot;, 1:5))) cat(paste0(&quot;Super learner GBM (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = &quot;superlearner_gbm&quot;, training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list(standardize = TRUE, keep_cross_validation_predictions = TRUE) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;superlearner_gbm&quot;), path = exp_dir, force = TRUE) model_id &lt;- &quot;metalearner_glm_superlearner_gbm&quot; tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id)), col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) # XGBoost base_models &lt;- as.list(c(unlist(xgboost@model_ids), paste0(&quot;XGBoost_&quot;, 1:3))) cat(paste0(&quot;Super learner XGBoost (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = &quot;superlearner_xgboost&quot;, training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list(standardize = TRUE, keep_cross_validation_predictions = TRUE) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;superlearner_xgboost&quot;), path = exp_dir, force = TRUE) model_id &lt;- &quot;metalearner_glm_superlearner_xgboost&quot; tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id)), col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) write.csv(res, file = paste0(exp_dir, &quot;/cv_holdout_predictions.csv&quot;), row.names = FALSE) cat(&quot;\\n\\n&quot;) h2o.removeAll() } 3.3.2 Inner cross-validation load(file = &quot;./rdata/var_reduct_mb_train_test_splits.RData&quot;) for (i in 1:10) { cat(paste0(&quot;Outer training set &quot;, i, &quot;\\n&quot;)) prefix &lt;- paste0(&quot;outer_&quot;, i) exp_dir &lt;- paste0(&quot;/Users/renee/Downloads/melanin_binding/&quot;, prefix) dir.create(exp_dir) train_mb_models(train_set = outer_splits[[i]][[&quot;train&quot;]], exp_dir = exp_dir, prefix = prefix) } 3.3.3 Training on whole data set load(file = &quot;./rdata/var_reduct_mb_train_test_splits.RData&quot;) prefix &lt;- &quot;whole_data_set&quot; exp_dir &lt;- paste0(&quot;/Users/renee/Downloads/melanin_binding/&quot;, prefix) dir.create(exp_dir) train_mb_models(train_set = mb_data, exp_dir = exp_dir, prefix = prefix) # Keep track of grid models dl_grid &lt;- list() gbm_grid &lt;- list() xgboost_grid &lt;- list() dl_grid_params &lt;- list() gbm_grid_params &lt;- list() xgboost_grid_params &lt;- list() for (i in 1:11) { if (i == 11) { cat(paste0(&quot;Whole data set\\n&quot;)) prefix &lt;- &quot;whole_data_set&quot; } else { cat(paste0(&quot;Outer training set &quot;, i, &quot;\\n&quot;)) prefix &lt;- paste0(&quot;outer_&quot;, i) } dir &lt;- paste0(&quot;/Users/renee/Downloads/melanin_binding/&quot;, prefix) files &lt;- list.files(dir) # Deep learning dl &lt;- files[str_detect(files, &quot;DeepLearning_model&quot;)] for (m in dl) { model &lt;- h2o.loadModel(paste0(dir, &quot;/&quot;, m)) hs &lt;- sha1(paste(c( model@allparameters$epsilon, model@allparameters$hidden, model@allparameters$hidden_dropout_ratios, model@allparameters$input_dropout_ratio, model@allparameters$rho ), collapse = &quot; &quot;)) if (hs %in% names(dl_grid)) { dl_grid[[hs]] &lt;- c(dl_grid[[hs]], m) } else { dl_grid[[hs]] &lt;- c(m) dl_grid_params &lt;- list.append( dl_grid_params, c( &quot;epsilon&quot; = model@allparameters$epsilon, &quot;hidden&quot; = paste0(&quot;[&quot;, paste(model@allparameters$hidden, collapse = &quot;,&quot;), &quot;]&quot;), &quot;hidden_dropout_ratios&quot; = paste0( &quot;[&quot;, paste(model@allparameters$hidden_dropout_ratios, collapse = &quot;,&quot; ), &quot;]&quot; ), &quot;input_dropout_ratio&quot; = model@allparameters$input_dropout_ratio, &quot;rho&quot; = model@allparameters$rho ) ) } } h2o.removeAll() # GBM gbm &lt;- files[str_detect(files, &quot;GBM_model&quot;)] for (m in gbm) { model &lt;- h2o.loadModel(paste0(dir, &quot;/&quot;, m)) hs &lt;- sha1(paste(c( model@allparameters$col_sample_rate, model@allparameters$col_sample_rate_per_tree, model@allparameters$max_depth, model@allparameters$min_rows, model@allparameters$min_split_improvement, model@allparameters$sample_rate ), collapse = &quot; &quot;)) if (hs %in% names(gbm_grid)) { gbm_grid[[hs]] &lt;- c(gbm_grid[[hs]], m) } else { gbm_grid[[hs]] &lt;- c(m) gbm_grid_params &lt;- list.append( gbm_grid_params, c( &quot;col_sample_rate&quot; = model@allparameters$col_sample_rate, &quot;col_sample_rate_per_tree&quot; = model@allparameters$col_sample_rate_per_tree, &quot;max_depth&quot; = model@allparameters$max_depth, &quot;min_rows&quot; = model@allparameters$min_rows, &quot;min_split_improvement&quot; = model@allparameters$min_split_improvement, &quot;sample_rate&quot; = model@allparameters$sample_rate ) ) } } h2o.removeAll() # XGBoost xgboost &lt;- files[str_detect(files, &quot;XGBoost_model&quot;)] for (m in xgboost) { model &lt;- h2o.loadModel(paste0(dir, &quot;/&quot;, m)) hs &lt;- sha1(paste(c( model@allparameters$booster, model@allparameters$col_sample_rate, model@allparameters$col_sample_rate_per_tree, model@allparameters$max_depth, model@allparameters$min_rows, model@allparameters$reg_alpha, model@allparameters$reg_lambda, model@allparameters$sample_rate ), collapse = &quot; &quot;)) if (hs %in% names(xgboost_grid)) { xgboost_grid[[hs]] &lt;- c(xgboost_grid[[hs]], m) } else { xgboost_grid[[hs]] &lt;- c(m) xgboost_grid_params &lt;- list.append( xgboost_grid_params, c( &quot;booster&quot; = model@allparameters$booster, &quot;col_sample_rate&quot; = model@allparameters$col_sample_rate, &quot;col_sample_rate_per_tree&quot; = model@allparameters$col_sample_rate_per_tree, &quot;max_depth&quot; = model@allparameters$max_depth, &quot;min_rows&quot; = model@allparameters$min_rows, &quot;reg_alpha&quot; = model@allparameters$reg_alpha, &quot;reg_lambda&quot; = model@allparameters$reg_lambda, &quot;sample_rate&quot; = model@allparameters$sample_rate ) ) } } h2o.removeAll() } dl_grid_params &lt;- as.data.frame(t(data.frame(dl_grid_params))) rownames(dl_grid_params) &lt;- paste(&quot;Neural network grid model&quot;, 1:nrow(dl_grid_params)) gbm_grid_params &lt;- as.data.frame(t(data.frame(gbm_grid_params))) rownames(gbm_grid_params) &lt;- paste(&quot;GBM grid model&quot;, 1:nrow(gbm_grid_params)) xgboost_grid_params &lt;- as.data.frame(t(data.frame(xgboost_grid_params))) rownames(xgboost_grid_params) &lt;- paste(&quot;XGBoost grid model&quot;, 1:nrow(xgboost_grid_params)) write.csv(dl_grid_params, &quot;./other_data/melanin_binding/neural_network_grid_params.csv&quot;) write.csv(gbm_grid_params, &quot;./other_data/melanin_binding/gbm_grid_params.csv&quot;) write.csv(xgboost_grid_params, &quot;./other_data/melanin_binding/xgboost_grid_params.csv&quot;) save(dl_grid, gbm_grid, xgboost_grid, file = &quot;./rdata/model_training_grid_models_mb.RData&quot;) 3.3.4 Model evaluation model_evaluation &lt;- function(holdout_pred, fold_asign, grid_name, grid_meta = NULL) { load(file = &quot;./rdata/var_reduct_mb_train_test_splits.RData&quot;) res_ &lt;- list() model_num &lt;- c() if (startsWith(grid_name, &quot;Super learner&quot;)) { if (grid_name == &quot;Super learner all models&quot;) { m &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_iter_0&quot;)] } else if (grid_name == &quot;Super learner final&quot;) { sl_models &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_iter&quot;)] m &lt;- sl_models[length(sl_models)] } else if (grid_name == &quot;Super learner neural network&quot;) { m &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_deeplearning&quot;)] } else if (grid_name == &quot;Super learner GBM&quot;) { m &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_gbm&quot;)] } else if (grid_name == &quot;Super learner XGBoost&quot;) { m &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_xgboost&quot;)] } mae &lt;- c() rmse &lt;- c() R2 &lt;- c() for (k in 1:10) { y_true &lt;- holdout_pred[fold_assign == k, &quot;log_intensity&quot;] y_pred &lt;- holdout_pred[fold_assign == k, m] mae &lt;- c(mae, MAE(y_pred = y_pred, y_true = y_true)) rmse &lt;- c(rmse, RMSE(y_pred = y_pred, y_true = y_true)) R2 &lt;- c(R2, R2_Score(y_pred = y_pred, y_true = y_true)) } # Re-scale to 0-100 mae &lt;- rescale(mae, to = c(0, 100), from = range(mb_data$log_intensity)) rmse &lt;- rescale(rmse, to = c(0, 100), from = range(mb_data$log_intensity)) res_ &lt;- list.append(res_, c( mean(mae, na.rm = TRUE), sem(mae), mean(rmse, na.rm = TRUE), sem(rmse), mean(R2, na.rm = TRUE), sem(R2), mae, rmse, R2 )) } else { for (j in 1:length(grid_meta)) { g &lt;- grid_meta[[j]] m &lt;- intersect(colnames(holdout_pred), g) if (length(m) == 1) { model_num &lt;- c(model_num, j) mae &lt;- c() rmse &lt;- c() R2 &lt;- c() for (k in 1:10) { y_true &lt;- holdout_pred[fold_assign == k, &quot;log_intensity&quot;] y_pred &lt;- holdout_pred[fold_assign == k, m] mae &lt;- c(mae, MAE(y_pred = y_pred, y_true = y_true)) rmse &lt;- c(rmse, RMSE(y_pred = y_pred, y_true = y_true)) R2 &lt;- c(R2, R2_Score(y_pred = y_pred, y_true = y_true)) } # Re-scale to 0-100 mae &lt;- rescale(mae, to = c(0, 100), from = range(mb_data$log_intensity)) rmse &lt;- rescale(rmse, to = c(0, 100), from = range(mb_data$log_intensity)) res_ &lt;- list.append(res_, c( mean(mae, na.rm = TRUE), sem(mae), mean(rmse, na.rm = TRUE), sem(rmse), mean(R2, na.rm = TRUE), sem(R2), mae, rmse, R2 )) } } } res &lt;- as.data.frame(t(data.frame(res_))) colnames(res) &lt;- c( &quot;Norm. MAE mean&quot;, &quot;Norm. MAE s.e.m.&quot;, &quot;Norm. RMSE mean&quot;, &quot;Norm. RMSE s.e.m.&quot;, &quot;R^2 mean&quot;, &quot;R^2 s.e.m.&quot;, paste(&quot;Norm. MAE CV&quot;, 1:10), paste(&quot;Norm. RMSE CV&quot;, 1:10), paste(&quot;R^2 CV&quot;, 1:10) ) if (nrow(res) == 1) { rownames(res) &lt;- grid_name } else { rownames(res) &lt;- paste(grid_name, model_num) } return(res) } 3.3.5 Inner loop model selection load(file = &quot;./rdata/model_training_grid_models_mb.RData&quot;) for (i in 1:10) { holdout_pred &lt;- read.csv(paste0(&quot;./other_data/melanin_binding/outer_&quot;, i, &quot;/cv_holdout_predictions.csv&quot;)) fold_assign &lt;- rep(1:10, ceiling(nrow(holdout_pred) / 10))[1:nrow(holdout_pred)] data_ &lt;- list() # Deep learning grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Neural network grid model&quot;, grid_meta = dl_grid )) # GBM grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;GBM grid model&quot;, grid_meta = gbm_grid )) # GBM default models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;GBM model&quot;, grid_meta = as.list(paste0(&quot;GBM_&quot;, 1:5)) )) # XGBoost grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XGBoost grid model&quot;, grid_meta = xgboost_grid )) # XGBoost default models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XGBoost model&quot;, grid_meta = as.list(paste0(&quot;XGBoost_&quot;, 1:3)) )) # GLM data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;GLM model&quot;, grid_meta = list(&quot;GLM&quot;))) # DRF data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;DRF model&quot;, grid_meta = list(&quot;DRF&quot;))) # XRT data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XRT model&quot;, grid_meta = list(&quot;XRT&quot;))) # Super learner all data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner all models&quot;)) # Super learner final data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner final&quot;)) # Super learner deep learning data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner neural network&quot;)) # Super learner GBM data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner GBM&quot;)) # Super learner XGBoost data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner XGBoost&quot;)) data &lt;- do.call(rbind, data_) write.csv(data, paste0(&quot;./other_data/melanin_binding/outer_&quot;, i, &quot;/cv_res.csv&quot;)) } 3.3.5.1 CV 1 3.3.5.2 CV 2 3.3.5.3 CV 3 3.3.5.4 CV 4 3.3.5.5 CV 5 3.3.5.6 CV 6 3.3.5.7 CV 7 3.3.5.8 CV 8 3.3.5.9 CV 9 3.3.5.10 CV 10 3.3.6 Final evaluation load(file = &quot;./rdata/var_reduct_mb_train_test_splits.RData&quot;) dir &lt;- &quot;/Users/renee/Downloads/melanin_binding&quot; selected_models &lt;- c( &quot;Super learner final&quot;, &quot;Super learner final&quot;, &quot;Super learner final&quot;, &quot;Super learner final&quot;, &quot;Super learner final&quot;, &quot;Super learner final&quot;, &quot;Super learner final&quot;, &quot;Super learner final&quot;, &quot;Super learner final&quot;, &quot;Super learner final&quot; ) mae &lt;- c() rmse &lt;- c() R2 &lt;- c() for (i in 1:10) { holdout_pred &lt;- read.csv(paste0(&quot;./other_data/melanin_binding/outer_&quot;, i, &quot;/cv_holdout_predictions.csv&quot;)) if (startsWith(selected_models[i], &quot;Neural network grid model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) m &lt;- intersect(colnames(holdout_pred), dl_grid[[grid_num]]) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, m)) } else if (startsWith(selected_models[i], &quot;GBM grid model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) m &lt;- intersect(colnames(holdout_pred), gbm_grid[[grid_num]]) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, m)) } else if (startsWith(selected_models[i], &quot;GBM model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/GBM_&quot;, grid_num)) } else if (startsWith(selected_models[i], &quot;XGBoost grid model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) m &lt;- intersect(colnames(holdout_pred), xgboost_grid[[grid_num]]) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, m)) } else if (startsWith(selected_models[i], &quot;XGBoost model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/XGBoost_&quot;, grid_num)) } else if (selected_models[i] == &quot;Super learner all models&quot;) { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/superlearner_iter_0&quot;)) } else if (selected_models[i] == &quot;Super learner final&quot;) { files &lt;- list.files(paste0(dir, &quot;/outer_&quot;, i)) files &lt;- files[startsWith(files, &quot;superlearner_iter&quot;)] model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, files[length(files)])) } else if (selected_models[i] == &quot;Super learner neural network&quot;) { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/superlearner_deeplearning&quot;)) } else if (selected_models[i] == &quot;Super learner GBM&quot;) { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/superlearner_gbm&quot;)) } else if (selected_models[i] == &quot;Super learner XGBoost&quot;) { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/superlearner_xgboost&quot;)) } else { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, unlist(strsplit(selected_models[i], &quot; &quot;))[1])) } tmp &lt;- as.h2o(subset(outer_splits[[i]][[&quot;test&quot;]], select = -log_intensity)) y_true &lt;- outer_splits[[i]][[&quot;test&quot;]]$log_intensity y_pred &lt;- as.data.frame(as.data.frame(h2o.predict(model, tmp)))[, 1] mae &lt;- c(mae, MAE(y_pred = y_pred, y_true = y_true)) rmse &lt;- c(rmse, RMSE(y_pred = y_pred, y_true = y_true)) R2 &lt;- c(R2, R2_Score(y_pred = y_pred, y_true = y_true)) } # Re-scale to 0-100 mae &lt;- rescale(mae, to = c(0, 100), from = range(mb_data$log_intensity)) rmse &lt;- rescale(rmse, to = c(0, 100), from = range(mb_data$log_intensity)) h2o.removeAll() save(mae, rmse, R2, file = &quot;./rdata/final_evaluation_mb.RData&quot;) load(file = &quot;./rdata/final_evaluation_mb.RData&quot;) data.frame( Metric = c(&quot;Norm. MAE&quot;, &quot;Norm. RMSE&quot;, &quot;R&lt;sup&gt;2&lt;/sup&gt;&quot;), `Mean ± s.e.m.` = c( paste0(round(mean(mae), 3), &quot; ± &quot;, round(sem(mae), 3)), paste0(round(mean(rmse), 3), &quot; ± &quot;, round(sem(rmse), 3)), paste0(round(mean(R2), 3), &quot; ± &quot;, round(sem(R2), 3)) ), check.names = FALSE ) %&gt;% datatable(escape = FALSE, rownames = FALSE) 3.3.7 Final model selection load(file = &quot;./rdata/model_training_grid_models_mb.RData&quot;) holdout_pred &lt;- read.csv(&quot;./other_data/melanin_binding/whole_data_set/cv_holdout_predictions.csv&quot;) fold_assign &lt;- rep(1:10, ceiling(nrow(holdout_pred) / 10))[1:nrow(holdout_pred)] data_ &lt;- list() # Deep learning grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Neural network grid model&quot;, grid_meta = dl_grid )) # GBM grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;GBM grid model&quot;, grid_meta = gbm_grid )) # GBM default models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;GBM model&quot;, grid_meta = as.list(paste0(&quot;GBM_&quot;, 1:5)) )) # XGBoost grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XGBoost grid model&quot;, grid_meta = xgboost_grid )) # XGBoost default models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XGBoost model&quot;, grid_meta = as.list(paste0(&quot;XGBoost_&quot;, 1:3)) )) # GLM data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;GLM model&quot;, grid_meta = list(&quot;GLM&quot;))) # DRF data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;DRF model&quot;, grid_meta = list(&quot;DRF&quot;))) # XRT data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XRT model&quot;, grid_meta = list(&quot;XRT&quot;))) # Super learner all data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner all models&quot;)) # Super learner final data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner final&quot;)) # Super learner deep learning data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner neural network&quot;)) # Super learner GBM data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner GBM&quot;)) # Super learner XGBoost data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner XGBoost&quot;)) data &lt;- do.call(rbind, data_) write.csv(data, &quot;./other_data/melanin_binding/whole_data_set/cv_res.csv&quot;) data &lt;- read.csv(&quot;./other_data/melanin_binding/whole_data_set/cv_res.csv&quot;, row.names = 1, check.names = FALSE) statistical_testing(data, metric_vec = c(&quot;Norm. MAE&quot;, &quot;Norm. RMSE&quot;, &quot;R^2&quot;), decreasing_vec = c(FALSE, FALSE, TRUE)) # statistical_testing(data, metric_vec=c(&#39;Norm. MAE&#39;, &#39;Norm. RMSE&#39;, &#39;R^2&#39;), decreasing_vec=c(FALSE, FALSE, TRUE), # output_path=&#39;./other_data/melanin_binding/whole_data_set/cv_res_statistical_testing.csv&#39;) 3.3.8 Final reduced SL h2o.init(nthreads = -1) model_path &lt;- &quot;/Users/renee/Downloads/melanin_binding/whole_data_set/superlearner_iter_3&quot; model &lt;- h2o.loadModel(model_path) load(file = &quot;./rdata/model_training_grid_models_mb.RData&quot;) data &lt;- read.csv(&quot;./other_data/melanin_binding/whole_data_set/cv_res.csv&quot;, row.names = 1, check.names = FALSE) df &lt;- as.data.frame(model@model$metalearner_model@model$coefficients_table)[c(&quot;names&quot;, &quot;standardized_coefficients&quot;)][-1, ] for (i in 1:nrow(df)) { name &lt;- df$names[i] if (startsWith(name, &quot;DeepLearning_model&quot;)) { for (j in 1:length(dl_grid)) { if (name %in% dl_grid[[j]]) break } df$names[i] &lt;- paste(&quot;Neural network grid model&quot;, j) } else if (startsWith(name, &quot;GBM_model&quot;)) { for (j in 1:length(gbm_grid)) { if (name %in% gbm_grid[[j]]) break } df$names[i] &lt;- paste(&quot;GBM grid model&quot;, j) } else if (startsWith(name, &quot;XGBoost_model&quot;)) { for (j in 1:length(xgboost_grid)) { if (name %in% xgboost_grid[[j]]) break } df$names[i] &lt;- paste(&quot;XGBoost grid model&quot;, j) } else { df$names[i] &lt;- paste(strsplit(name, &quot;_&quot;)[[1]], collapse = &quot; model &quot;) } } rownames(df) &lt;- df$names df &lt;- merge(df, data[&quot;R^2 mean&quot;], by = &quot;row.names&quot;)[, -1] df &lt;- df[df$standardized_coefficients &gt; 0, ] p1 &lt;- ggplot(df, aes(x = reorder(names, standardized_coefficients), y = standardized_coefficients, fill = `R^2 mean`)) + geom_col(color = &quot;black&quot;, size = 0.2) + scale_fill_gradient(low = &quot;white&quot;, high = &quot;red&quot;, n.breaks = 4, limits = c(0.30, 0.60), breaks = c(0.3, 0.4, 0.5, 0.6)) + geom_text(aes(label = sprintf(&quot;%.2f&quot;, `R^2 mean`)), nudge_y = -0.002, size = 3, color = &quot;white&quot;) + geom_text(aes(label = sprintf(&quot;%.3f&quot;, standardized_coefficients)), nudge_y = 0.0025, size = 3) + coord_flip() + scale_y_continuous(limits = c(0, max(df$standardized_coefficients) + 0.005), expand = c(0.01, 0)) + theme_bw() + theme( panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank(), panel.border = element_blank(), axis.line = element_line(color = &quot;black&quot;), legend.text = element_text(colour = &quot;black&quot;, size = 10), plot.title = element_text(hjust = 0.5, size = 18), plot.margin = ggplot2::margin(10, 10, 10, 0, &quot;pt&quot;), axis.title.x = element_text(colour = &quot;black&quot;, size = 18), axis.title.y = element_text(colour = &quot;black&quot;, size = 18), axis.text.x = element_text(colour = &quot;black&quot;, size = 12), axis.text.y = element_text(colour = &quot;black&quot;, size = 12), legend.title = element_text(size = 14), legend.position = c(0.85, 0.5) ) + guides(fill = guide_colorbar(title = expression(italic(&quot;R&quot;^2)))) + xlab(&quot;&quot;) + ylab(&quot;Coefficient&quot;) + ggtitle(&quot;Melanin binding&quot;) 3.4 Cell-penetration (classification) 3.4.1 Model training train_cpp_models &lt;- function(train_set, exp_dir, prefix, nfolds = 10, grid_seed = 1) { tmp &lt;- as.h2o(train_set, destination_frame = prefix) tmp[&quot;category&quot;] &lt;- as.factor(tmp[&quot;category&quot;]) y &lt;- &quot;category&quot; x &lt;- setdiff(names(tmp), y) res &lt;- as.data.frame(tmp$category) samp_factors &lt;- as.vector(mean(table(train_set$category)) / table(train_set$category)) # ------------------- # base model training # ------------------- cat(&quot;Deep learning grid 1\\n&quot;) deeplearning_1 &lt;- h2o.grid( algorithm = &quot;deeplearning&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = deeplearning_params_1, stopping_rounds = 3, balance_classes = TRUE, class_sampling_factors = samp_factors, search_criteria = list( strategy = &quot;RandomDiscrete&quot;, max_models = 100, seed = grid_seed ), keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in deeplearning_1@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;GBM grid\\n&quot;) gbm &lt;- h2o.grid( algorithm = &quot;gbm&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = gbm_params, stopping_rounds = 3, balance_classes = TRUE, class_sampling_factors = samp_factors, search_criteria = list(strategy = &quot;RandomDiscrete&quot;, max_models = 100, seed = grid_seed), keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in gbm@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;GBM 5 default models\\n&quot;) gbm_1 &lt;- h2o.gbm( model_id = &quot;GBM_1&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, balance_classes = TRUE, class_sampling_factors = samp_factors, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 6, min_rows = 1, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_2 &lt;- h2o.gbm( model_id = &quot;GBM_2&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, balance_classes = TRUE, class_sampling_factors = samp_factors, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 7, min_rows = 10, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_3 &lt;- h2o.gbm( model_id = &quot;GBM_3&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, balance_classes = TRUE, class_sampling_factors = samp_factors, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 8, min_rows = 10, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_4 &lt;- h2o.gbm( model_id = &quot;GBM_4&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, balance_classes = TRUE, class_sampling_factors = samp_factors, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 10, min_rows = 10, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_5 &lt;- h2o.gbm( model_id = &quot;GBM_5&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, balance_classes = TRUE, class_sampling_factors = samp_factors, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 15, min_rows = 100, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) for (model_id in paste0(&quot;GBM_&quot;, 1:5)) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;XGBoost grid\\n&quot;) xgboost &lt;- h2o.grid( algorithm = &quot;xgboost&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = xgboost_params, stopping_rounds = 3, search_criteria = list(strategy = &quot;RandomDiscrete&quot;, max_models = 100, seed = grid_seed), keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in xgboost@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;XGBoost 3 default models\\n&quot;) xgboost_1 &lt;- h2o.xgboost( model_id = &quot;XGBoost_1&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, booster = &quot;gbtree&quot;, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, max_depth = 10, min_rows = 5, ntrees = 10000, reg_alpha = 0, reg_lambda = 1, sample_rate = 0.6, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) xgboost_2 &lt;- h2o.xgboost( model_id = &quot;XGBoost_2&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, booster = &quot;gbtree&quot;, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, max_depth = 20, min_rows = 10, ntrees = 10000, reg_alpha = 0, reg_lambda = 1, sample_rate = 0.6, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) xgboost_3 &lt;- h2o.xgboost( model_id = &quot;XGBoost_3&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, booster = &quot;gbtree&quot;, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, max_depth = 5, min_rows = 3, ntrees = 10000, reg_alpha = 0, reg_lambda = 1, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) for (model_id in paste0(&quot;XGBoost_&quot;, 1:3)) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;DRF\\n&quot;) drf &lt;- h2o.randomForest( model_id = &quot;DRF&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, ntrees = 10000, score_tree_interval = 5, stopping_rounds = 3, balance_classes = TRUE, class_sampling_factors = samp_factors, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;DRF&quot;), path = exp_dir, force = TRUE) cat(&quot;XRT\\n&quot;) xrt &lt;- h2o.randomForest( model_id = &quot;XRT&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, ntrees = 10000, histogram_type = &quot;Random&quot;, score_tree_interval = 5, stopping_rounds = 3, balance_classes = TRUE, class_sampling_factors = samp_factors, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;XRT&quot;), path = exp_dir, force = TRUE) # ----------------------- # get holdout predictions # ----------------------- base_models &lt;- as.list(c( unlist(deeplearning_1@model_ids), unlist(gbm@model_ids), paste0(&quot;GBM_&quot;, 1:5), unlist(xgboost@model_ids), paste0(&quot;XGBoost_&quot;, 1:3), &quot;DRF&quot;, &quot;XRT&quot; )) for (model_id in base_models) { res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[&quot;penetrating&quot;], col.names = model_id )) } # ---------------------- # super learner training # ---------------------- sl_iter &lt;- 0 cat(paste0(&quot;Super learner iteration &quot;, sl_iter, &quot; (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = paste0(&quot;superlearner_iter_&quot;, sl_iter), training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list( standardize = TRUE, keep_cross_validation_predictions = TRUE, balance_classes = TRUE, class_sampling_factors = samp_factors ) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(paste0(&quot;superlearner_iter_&quot;, sl_iter)), path = exp_dir, force = TRUE) model_id &lt;- paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter) tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[&quot;penetrating&quot;], col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) # ---------------------------------- # super learner base model reduction # ---------------------------------- while (TRUE) { meta &lt;- h2o.getModel(paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter, &quot;_cv_1&quot;)) names &lt;- meta@model$coefficients_table[, &quot;names&quot;] coeffs &lt;- (meta@model$coefficients_table[, &quot;standardized_coefficients&quot;] &gt; 0) for (j in 2:nfolds) { meta &lt;- h2o.getModel(paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter, &quot;_cv_&quot;, j)) names &lt;- meta@model$coefficients_table[, &quot;names&quot;] coeffs &lt;- coeffs + (meta@model$coefficients_table[, &quot;standardized_coefficients&quot;] &gt; 0) } base_models_ &lt;- as.list(names[coeffs &gt;= ceiling(nfolds / 2) &amp; names != &quot;Intercept&quot;]) if (length(base_models_) == 0) { cat(&quot;No base models passing the threshold\\n\\n&quot;) break } if (sum(base_models %in% base_models_) == length(base_models)) { cat(&quot;No further reduction of base models\\n\\n&quot;) break } sl_iter &lt;- sl_iter + 1 base_models &lt;- base_models_ cat(paste0(&quot;Super learner iteration &quot;, sl_iter, &quot; (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = paste0(&quot;superlearner_iter_&quot;, sl_iter), training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list( standardize = TRUE, keep_cross_validation_predictions = TRUE, balance_classes = TRUE, class_sampling_factors = samp_factors ) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(paste0(&quot;superlearner_iter_&quot;, sl_iter)), path = exp_dir, force = TRUE) model_id &lt;- paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter) tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[&quot;penetrating&quot;], col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) } # ----------------------------------------- # super learner for homogeneous base models # ----------------------------------------- # DeepLearning base_models &lt;- deeplearning_1@model_ids cat(paste0(&quot;Super learner deep learning (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = &quot;superlearner_deeplearning&quot;, training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list( standardize = TRUE, keep_cross_validation_predictions = TRUE, balance_classes = TRUE, class_sampling_factors = samp_factors ) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;superlearner_deeplearning&quot;), path = exp_dir, force = TRUE) model_id &lt;- &quot;metalearner_glm_superlearner_deeplearning&quot; tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[&quot;penetrating&quot;], col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) # GBM base_models &lt;- as.list(c(unlist(gbm@model_ids), paste0(&quot;GBM_&quot;, 1:5))) cat(paste0(&quot;Super learner GBM (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = &quot;superlearner_gbm&quot;, training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list( standardize = TRUE, keep_cross_validation_predictions = TRUE, balance_classes = TRUE, class_sampling_factors = samp_factors ) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;superlearner_gbm&quot;), path = exp_dir, force = TRUE) model_id &lt;- &quot;metalearner_glm_superlearner_gbm&quot; tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[&quot;penetrating&quot;], col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) # XGBoost base_models &lt;- as.list(c(unlist(xgboost@model_ids), paste0(&quot;XGBoost_&quot;, 1:3))) cat(paste0(&quot;Super learner XGBoost (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = &quot;superlearner_xgboost&quot;, training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list( standardize = TRUE, keep_cross_validation_predictions = TRUE, balance_classes = TRUE, class_sampling_factors = samp_factors ) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;superlearner_xgboost&quot;), path = exp_dir, force = TRUE) model_id &lt;- &quot;metalearner_glm_superlearner_xgboost&quot; tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[&quot;penetrating&quot;], col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) write.csv(res, file = paste0(exp_dir, &quot;/cv_holdout_predictions.csv&quot;), row.names = FALSE) cat(&quot;\\n\\n&quot;) h2o.removeAll() } 3.4.2 Inner cross-validation load(file = &quot;./rdata/var_reduct_cpp_train_test_splits.RData&quot;) for (i in 1:10) { cat(paste0(&quot;Outer training set &quot;, i, &quot;\\n&quot;)) prefix &lt;- paste0(&quot;outer_&quot;, i) exp_dir &lt;- paste0(&quot;/Users/renee/Downloads/cell_penetration/&quot;, prefix) dir.create(exp_dir) train_cpp_models(train_set = outer_splits[[i]][[&quot;train&quot;]], exp_dir = exp_dir, prefix = prefix) } 3.4.3 Training on whole data set load(file = &quot;./rdata/var_reduct_cpp_train_test_splits.RData&quot;) prefix &lt;- &quot;whole_data_set&quot; exp_dir &lt;- paste0(&quot;/Users/renee/Downloads/cell_penetration/&quot;, prefix) dir.create(exp_dir) train_cpp_models(train_set = cpp_data, exp_dir = exp_dir, prefix = prefix) # Keep track of grid models dl_grid &lt;- list() gbm_grid &lt;- list() xgboost_grid &lt;- list() dl_grid_params &lt;- list() gbm_grid_params &lt;- list() xgboost_grid_params &lt;- list() for (i in 1:11) { if (i == 11) { cat(paste0(&quot;Whole data set\\n&quot;)) prefix &lt;- &quot;whole_data_set&quot; } else { cat(paste0(&quot;Outer training set &quot;, i, &quot;\\n&quot;)) prefix &lt;- paste0(&quot;outer_&quot;, i) } dir &lt;- paste0(&quot;/Users/renee/Downloads/cell_penetration/&quot;, prefix) files &lt;- list.files(dir) # Deep learning dl &lt;- files[str_detect(files, &quot;DeepLearning_model&quot;)] for (m in dl) { model &lt;- h2o.loadModel(paste0(dir, &quot;/&quot;, m)) hs &lt;- sha1(paste(c( model@allparameters$epsilon, model@allparameters$hidden, model@allparameters$hidden_dropout_ratios, model@allparameters$input_dropout_ratio, model@allparameters$rho ), collapse = &quot; &quot;)) if (hs %in% names(dl_grid)) { dl_grid[[hs]] &lt;- c(dl_grid[[hs]], m) } else { dl_grid[[hs]] &lt;- c(m) dl_grid_params &lt;- list.append( dl_grid_params, c( &quot;epsilon&quot; = model@allparameters$epsilon, &quot;hidden&quot; = paste0(&quot;[&quot;, paste(model@allparameters$hidden, collapse = &quot;,&quot;), &quot;]&quot;), &quot;hidden_dropout_ratios&quot; = paste0( &quot;[&quot;, paste(model@allparameters$hidden_dropout_ratios, collapse = &quot;,&quot; ), &quot;]&quot; ), &quot;input_dropout_ratio&quot; = model@allparameters$input_dropout_ratio, &quot;rho&quot; = model@allparameters$rho ) ) } } h2o.removeAll() # GBM gbm &lt;- files[str_detect(files, &quot;GBM_model&quot;)] for (m in gbm) { model &lt;- h2o.loadModel(paste0(dir, &quot;/&quot;, m)) hs &lt;- sha1(paste(c( model@allparameters$col_sample_rate, model@allparameters$col_sample_rate_per_tree, model@allparameters$max_depth, model@allparameters$min_rows, model@allparameters$min_split_improvement, model@allparameters$sample_rate ), collapse = &quot; &quot;)) if (hs %in% names(gbm_grid)) { gbm_grid[[hs]] &lt;- c(gbm_grid[[hs]], m) } else { gbm_grid[[hs]] &lt;- c(m) gbm_grid_params &lt;- list.append( gbm_grid_params, c( &quot;col_sample_rate&quot; = model@allparameters$col_sample_rate, &quot;col_sample_rate_per_tree&quot; = model@allparameters$col_sample_rate_per_tree, &quot;max_depth&quot; = model@allparameters$max_depth, &quot;min_rows&quot; = model@allparameters$min_rows, &quot;min_split_improvement&quot; = model@allparameters$min_split_improvement, &quot;sample_rate&quot; = model@allparameters$sample_rate ) ) } } h2o.removeAll() # XGBoost xgboost &lt;- files[str_detect(files, &quot;XGBoost_model&quot;)] for (m in xgboost) { model &lt;- h2o.loadModel(paste0(dir, &quot;/&quot;, m)) hs &lt;- sha1(paste(c( model@allparameters$booster, model@allparameters$col_sample_rate, model@allparameters$col_sample_rate_per_tree, model@allparameters$max_depth, model@allparameters$min_rows, model@allparameters$reg_alpha, model@allparameters$reg_lambda, model@allparameters$sample_rate ), collapse = &quot; &quot;)) if (hs %in% names(xgboost_grid)) { xgboost_grid[[hs]] &lt;- c(xgboost_grid[[hs]], m) } else { xgboost_grid[[hs]] &lt;- c(m) xgboost_grid_params &lt;- list.append( xgboost_grid_params, c( &quot;booster&quot; = model@allparameters$booster, &quot;col_sample_rate&quot; = model@allparameters$col_sample_rate, &quot;col_sample_rate_per_tree&quot; = model@allparameters$col_sample_rate_per_tree, &quot;max_depth&quot; = model@allparameters$max_depth, &quot;min_rows&quot; = model@allparameters$min_rows, &quot;reg_alpha&quot; = model@allparameters$reg_alpha, &quot;reg_lambda&quot; = model@allparameters$reg_lambda, &quot;sample_rate&quot; = model@allparameters$sample_rate ) ) } } h2o.removeAll() } dl_grid_params &lt;- as.data.frame(t(data.frame(dl_grid_params))) rownames(dl_grid_params) &lt;- paste(&quot;Neural network grid model&quot;, 1:nrow(dl_grid_params)) gbm_grid_params &lt;- as.data.frame(t(data.frame(gbm_grid_params))) rownames(gbm_grid_params) &lt;- paste(&quot;GBM grid model&quot;, 1:nrow(gbm_grid_params)) xgboost_grid_params &lt;- as.data.frame(t(data.frame(xgboost_grid_params))) rownames(xgboost_grid_params) &lt;- paste(&quot;XGBoost grid model&quot;, 1:nrow(xgboost_grid_params)) write.csv(dl_grid_params, &quot;./other_data/cell_penetration/neural_network_grid_params.csv&quot;) write.csv(gbm_grid_params, &quot;./other_data/cell_penetration/gbm_grid_params.csv&quot;) write.csv(xgboost_grid_params, &quot;./other_data/cell_penetration/xgboost_grid_params.csv&quot;) save(dl_grid, gbm_grid, xgboost_grid, file = &quot;./rdata/model_training_grid_models_cpp.RData&quot;) 3.4.4 Model evaluation model_evaluation &lt;- function(holdout_pred, fold_asign, grid_name, grid_meta = NULL) { res_ &lt;- list() model_num &lt;- c() if (startsWith(grid_name, &quot;Super learner&quot;)) { if (grid_name == &quot;Super learner all models&quot;) { m &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_iter_0&quot;)] } else if (grid_name == &quot;Super learner final&quot;) { sl_models &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_iter&quot;)] m &lt;- sl_models[length(sl_models)] } else if (grid_name == &quot;Super learner neural network&quot;) { m &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_deeplearning&quot;)] } else if (grid_name == &quot;Super learner GBM&quot;) { m &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_gbm&quot;)] } else if (grid_name == &quot;Super learner XGBoost&quot;) { m &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_xgboost&quot;)] } logloss &lt;- c() MCC &lt;- c() F1 &lt;- c() acc &lt;- c() EF &lt;- c() BEDROC &lt;- c() threshold &lt;- 0.5 for (k in 1:10) { y_true &lt;- sapply(holdout_pred[fold_assign == k, &quot;category&quot;], function(x) if (x == &quot;penetrating&quot;) 1 else 0) y_pred &lt;- holdout_pred[fold_assign == k, m] y_pred_cls &lt;- sapply(y_pred, function(x) if (x &gt;= threshold) 1 else 0) logloss &lt;- c(logloss, LogLoss(y_pred = y_pred, y_true = y_true)) MCC &lt;- c(MCC, mcc(preds = y_pred_cls, actuals = y_true)) F1 &lt;- c(F1, F1_Score(y_pred = y_pred_cls, y_true = y_true)) acc &lt;- c(acc, Accuracy(y_pred = y_pred_cls, y_true = y_true)) EF &lt;- c(EF, enrichment_factor(x = y_pred, y = y_true, top = 0.05)) BEDROC &lt;- c(BEDROC, bedroc(x = y_pred, y = y_true, alpha = 20)) } res_ &lt;- list.append(res_, c( mean(logloss, na.rm = TRUE), sem(logloss), mean(MCC, na.rm = TRUE), sem(MCC), mean(F1, na.rm = TRUE), sem(F1), mean(acc, na.rm = TRUE), sem(acc), mean(EF, na.rm = TRUE), sem(EF), mean(BEDROC, na.rm = TRUE), sem(BEDROC), logloss, MCC, F1, acc, EF, BEDROC )) } else { for (j in 1:length(grid_meta)) { g &lt;- grid_meta[[j]] m &lt;- intersect(colnames(holdout_pred), g) if (length(m) == 1) { model_num &lt;- c(model_num, j) logloss &lt;- c() MCC &lt;- c() F1 &lt;- c() acc &lt;- c() EF &lt;- c() BEDROC &lt;- c() threshold &lt;- 0.5 for (k in 1:10) { y_true &lt;- sapply(holdout_pred[fold_assign == k, &quot;category&quot;], function(x) if (x == &quot;penetrating&quot;) 1 else 0) y_pred &lt;- holdout_pred[fold_assign == k, m] y_pred_cls &lt;- sapply(y_pred, function(x) if (x &gt;= threshold) 1 else 0) logloss &lt;- c(logloss, LogLoss(y_pred = y_pred, y_true = y_true)) MCC &lt;- c(MCC, mcc(preds = y_pred_cls, actuals = y_true)) F1 &lt;- c(F1, F1_Score(y_pred = y_pred_cls, y_true = y_true)) acc &lt;- c(acc, Accuracy(y_pred = y_pred_cls, y_true = y_true)) EF &lt;- c(EF, enrichment_factor(x = y_pred, y = y_true, top = 0.05)) BEDROC &lt;- c(BEDROC, bedroc(x = y_pred, y = y_true, alpha = 20)) } res_ &lt;- list.append(res_, c( mean(logloss, na.rm = TRUE), sem(logloss), mean(MCC, na.rm = TRUE), sem(MCC), mean(F1, na.rm = TRUE), sem(F1), mean(acc, na.rm = TRUE), sem(acc), mean(EF, na.rm = TRUE), sem(EF), mean(BEDROC, na.rm = TRUE), sem(BEDROC), logloss, MCC, F1, acc, EF, BEDROC )) } } } res &lt;- as.data.frame(t(data.frame(res_))) colnames(res) &lt;- c( &quot;Log loss mean&quot;, &quot;Log loss s.e.m.&quot;, &quot;MCC mean&quot;, &quot;MCC s.e.m.&quot;, &quot;F_1 mean&quot;, &quot;F_1 s.e.m.&quot;, &quot;Accuracy mean&quot;, &quot;Accuracy s.e.m.&quot;, &quot;EF mean&quot;, &quot;EF s.e.m.&quot;, &quot;BEDROC mean&quot;, &quot;BEDROC s.e.m.&quot;, paste(&quot;Log loss CV&quot;, 1:10), paste(&quot;MCC CV&quot;, 1:10), paste(&quot;F_1 CV&quot;, 1:10), paste(&quot;Accuracy CV&quot;, 1:10), paste(&quot;EF CV&quot;, 1:10), paste(&quot;BEDROC CV&quot;, 1:10) ) if (nrow(res) == 1) { rownames(res) &lt;- grid_name } else { rownames(res) &lt;- paste(grid_name, model_num) } return(res) } 3.4.5 Inner loop model selection load(file = &quot;./rdata/model_training_grid_models_cpp.RData&quot;) for (i in 1:10) { holdout_pred &lt;- read.csv(paste0(&quot;./other_data/cell_penetration/outer_&quot;, i, &quot;/cv_holdout_predictions.csv&quot;)) fold_assign &lt;- rep(1:10, ceiling(nrow(holdout_pred) / 10))[1:nrow(holdout_pred)] data_ &lt;- list() # Deep learning grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Neural network grid model&quot;, grid_meta = dl_grid )) # GBM grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;GBM grid model&quot;, grid_meta = gbm_grid )) # GBM default models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;GBM model&quot;, grid_meta = as.list(paste0(&quot;GBM_&quot;, 1:5)) )) # XGBoost grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XGBoost grid model&quot;, grid_meta = xgboost_grid )) # XGBoost default models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XGBoost model&quot;, grid_meta = as.list(paste0(&quot;XGBoost_&quot;, 1:3)) )) # DRF data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;DRF model&quot;, grid_meta = list(&quot;DRF&quot;))) # XRT data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XRT model&quot;, grid_meta = list(&quot;XRT&quot;))) # Super learner all data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner all models&quot;)) # Super learner final data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner final&quot;)) # Super learner deep learning data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner neural network&quot;)) # Super learner GBM data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner GBM&quot;)) # Super learner XGBoost data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner XGBoost&quot;)) data &lt;- do.call(rbind, data_) write.csv(data, paste0(&quot;./other_data/cell_penetration/outer_&quot;, i, &quot;/cv_res.csv&quot;)) } 3.4.5.1 CV 1 3.4.5.2 CV 2 3.4.5.3 CV 3 3.4.5.4 CV 4 3.4.5.5 CV 5 3.4.5.6 CV 6 3.4.5.7 CV 7 3.4.5.8 CV 8 3.4.5.9 CV 9 3.4.5.10 CV 10 3.4.6 Final evaluation load(file = &quot;./rdata/var_reduct_cpp_train_test_splits.RData&quot;) load(file = &quot;./rdata/model_training_grid_models_cpp.RData&quot;) dir &lt;- &quot;/Users/renee/Downloads/cell_penetration&quot; selected_models &lt;- c( &quot;GBM grid model 30&quot;, &quot;Neural network grid model 98&quot;, &quot;Neural network grid model 72&quot;, &quot;GBM grid model 1&quot;, &quot;GBM grid model 45&quot;, &quot;GBM grid model 82&quot;, &quot;GBM grid model 72&quot;, &quot;GBM grid model 15&quot;, &quot;GBM grid model 74&quot;, &quot;GBM grid model 24&quot; ) logloss &lt;- c() MCC &lt;- c() F1 &lt;- c() acc &lt;- c() EF &lt;- c() BEDROC &lt;- c() threshold &lt;- 0.5 for (i in 1:10) { holdout_pred &lt;- read.csv(paste0(&quot;./other_data/cell_penetration/outer_&quot;, i, &quot;/cv_holdout_predictions.csv&quot;)) if (startsWith(selected_models[i], &quot;Neural network grid model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) m &lt;- intersect(colnames(holdout_pred), dl_grid[[grid_num]]) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, m)) } else if (startsWith(selected_models[i], &quot;GBM grid model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) m &lt;- intersect(colnames(holdout_pred), gbm_grid[[grid_num]]) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, m)) } else if (startsWith(selected_models[i], &quot;GBM model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/GBM_&quot;, grid_num)) } else if (startsWith(selected_models[i], &quot;XGBoost grid model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) m &lt;- intersect(colnames(holdout_pred), xgboost_grid[[grid_num]]) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, m)) } else if (startsWith(selected_models[i], &quot;XGBoost model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/XGBoost_&quot;, grid_num)) } else if (selected_models[i] == &quot;Super learner all models&quot;) { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/superlearner_iter_0&quot;)) } else if (selected_models[i] == &quot;Super learner final&quot;) { files &lt;- list.files(paste0(dir, &quot;/outer_&quot;, i)) files &lt;- files[startsWith(files, &quot;superlearner_iter&quot;)] model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, files[length(files)])) } else if (selected_models[i] == &quot;Super learner neural network&quot;) { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/superlearner_deeplearning&quot;)) } else if (selected_models[i] == &quot;Super learner GBM&quot;) { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/superlearner_gbm&quot;)) } else if (selected_models[i] == &quot;Super learner XGBoost&quot;) { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/superlearner_xgboost&quot;)) } else { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, unlist(strsplit(selected_models[i], &quot; &quot;))[1])) } tmp &lt;- as.h2o(subset(outer_splits[[i]][[&quot;test&quot;]], select = -category)) y_true &lt;- sapply(outer_splits[[i]][[&quot;test&quot;]]$category, function(x) if (x == &quot;penetrating&quot;) 1 else 0) y_pred &lt;- as.data.frame(as.data.frame(h2o.predict(model, tmp)))[, &quot;penetrating&quot;] y_pred_cls &lt;- sapply(y_pred, function(x) if (x &gt;= threshold) 1 else 0) logloss &lt;- c(logloss, LogLoss(y_pred = y_pred, y_true = y_true)) MCC &lt;- c(MCC, mcc(preds = y_pred_cls, actuals = y_true)) F1 &lt;- c(F1, F1_Score(y_pred = y_pred_cls, y_true = y_true)) acc &lt;- c(acc, Accuracy(y_pred = y_pred_cls, y_true = y_true)) EF &lt;- c(EF, enrichment_factor(x = y_pred, y = y_true, top = 0.05)) BEDROC &lt;- c(BEDROC, bedroc(x = y_pred, y = y_true, alpha = 20)) } save(logloss, MCC, F1, acc, EF, BEDROC, file = &quot;./rdata/final_evaluation_cpp.RData&quot;) load(file = &quot;./rdata/final_evaluation_cpp.RData&quot;) data.frame( Metric = c(&quot;Log loss&quot;, &quot;MCC&quot;, &quot;F&lt;sub&gt;1&lt;/sub&gt;&quot;, &quot;Accuracy&quot;, &quot;EF&quot;, &quot;BEDROC&quot;), `Mean ± s.e.m.` = c( paste0(round(mean(logloss), 3), &quot; ± &quot;, round(sem(logloss), 3)), paste0(round(mean(MCC), 3), &quot; ± &quot;, round(sem(MCC), 3)), paste0(round(mean(F1), 3), &quot; ± &quot;, round(sem(F1), 3)), paste0(round(mean(acc), 3), &quot; ± &quot;, round(sem(acc), 3)), paste0(round(mean(EF), 3), &quot; ± &quot;, round(sem(EF), 3)), paste0(round(mean(BEDROC), 3), &quot; ± &quot;, round(sem(BEDROC), 3)) ), check.names = FALSE ) %&gt;% datatable(escape = FALSE, rownames = FALSE) 3.4.7 Final model selection load(file = &quot;./rdata/model_training_grid_models_cpp.RData&quot;) holdout_pred &lt;- read.csv(&quot;./other_data/cell_penetration/whole_data_set/cv_holdout_predictions.csv&quot;) fold_assign &lt;- rep(1:10, ceiling(nrow(holdout_pred) / 10))[1:nrow(holdout_pred)] data_ &lt;- list() # Deep learning grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Neural network grid model&quot;, grid_meta = dl_grid )) # GBM grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;GBM grid model&quot;, grid_meta = gbm_grid )) # GBM default models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;GBM model&quot;, grid_meta = as.list(paste0(&quot;GBM_&quot;, 1:5)) )) # XGBoost grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XGBoost grid model&quot;, grid_meta = xgboost_grid )) # XGBoost default models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XGBoost model&quot;, grid_meta = as.list(paste0(&quot;XGBoost_&quot;, 1:3)) )) # DRF data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;DRF model&quot;, grid_meta = list(&quot;DRF&quot;))) # XRT data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XRT model&quot;, grid_meta = list(&quot;XRT&quot;))) # Super learner all data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner all models&quot;)) # Super learner final data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner final&quot;)) # Super learner deep learning data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner neural network&quot;)) # Super learner GBM data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner GBM&quot;)) # Super learner XGBoost data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner XGBoost&quot;)) data &lt;- do.call(rbind, data_) write.csv(data, &quot;./other_data/cell_penetration/whole_data_set/cv_res.csv&quot;) data &lt;- read.csv(&quot;./other_data/cell_penetration/whole_data_set/cv_res.csv&quot;, row.names = 1, check.names = FALSE) statistical_testing(data, metric_vec = c(&quot;Log loss&quot;, &quot;MCC&quot;, &quot;F_1&quot;, &quot;Accuracy&quot;), decreasing_vec = c(FALSE, TRUE, TRUE, TRUE) ) # statistical_testing(data, metric_vec=c(&#39;Log loss&#39;, &#39;MCC&#39;, &#39;F_1&#39;, &#39;Accuracy&#39;), # decreasing_vec=c(FALSE, TRUE, TRUE, TRUE), # output_path=&#39;./other_data/cell_penetration/whole_data_set/cv_res_statistical_testing.csv&#39;) 3.4.8 Final reduced SL h2o.init(nthreads = -1) model_path &lt;- &quot;/Users/renee/Downloads/cell_penetration/whole_data_set/superlearner_iter_1&quot; model &lt;- h2o.loadModel(model_path) load(file = &quot;./rdata/model_training_grid_models_cpp.RData&quot;) data &lt;- read.csv(&quot;./other_data/cell_penetration/whole_data_set/cv_res.csv&quot;, row.names = 1, check.names = FALSE) df &lt;- as.data.frame(model@model$metalearner_model@model$coefficients_table)[c(&quot;names&quot;, &quot;standardized_coefficients&quot;)][-1, ] for (i in 1:nrow(df)) { name &lt;- df$names[i] if (startsWith(name, &quot;DeepLearning_model&quot;)) { for (j in 1:length(dl_grid)) { if (name %in% dl_grid[[j]]) break } df$names[i] &lt;- paste(&quot;Neural network grid model&quot;, j) } else if (startsWith(name, &quot;GBM_model&quot;)) { for (j in 1:length(gbm_grid)) { if (name %in% gbm_grid[[j]]) break } df$names[i] &lt;- paste(&quot;GBM grid model&quot;, j) } else if (startsWith(name, &quot;XGBoost_model&quot;)) { for (j in 1:length(xgboost_grid)) { if (name %in% xgboost_grid[[j]]) break } df$names[i] &lt;- paste(&quot;XGBoost grid model&quot;, j) } else if (startsWith(name, &quot;DRF&quot;)) { df$names[i] &lt;- &quot;DRF model&quot; } else if (startsWith(name, &quot;XRT&quot;)) { df$names[i] &lt;- &quot;XRT model&quot; } else { df$names[i] &lt;- paste(strsplit(name, &quot;_&quot;)[[1]], collapse = &quot; model &quot;) } } rownames(df) &lt;- df$names df &lt;- merge(df, data[&quot;Accuracy mean&quot;], by = &quot;row.names&quot;)[, -1] df &lt;- df[df$standardized_coefficients &gt; 0, ] p2 &lt;- ggplot(df, aes(x = reorder(names, standardized_coefficients), y = standardized_coefficients, fill = `Accuracy mean`)) + geom_col(color = &quot;black&quot;, size = 0.2) + scale_fill_gradient(low = &quot;white&quot;, high = &quot;red&quot;, n.breaks = 4, limits = c(0.85, 1.00)) + geom_text(aes(label = sprintf(&quot;%.2f&quot;, `Accuracy mean`)), nudge_y = -0.0025, size = 3, color = &quot;white&quot;) + geom_text(aes(label = sprintf(&quot;%.3f&quot;, standardized_coefficients)), nudge_y = 0.0035, size = 3) + coord_flip() + scale_y_continuous(limits = c(0, max(df$standardized_coefficients) + 0.006), expand = c(0.01, 0)) + theme_bw() + theme( panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank(), panel.border = element_blank(), axis.line = element_line(color = &quot;black&quot;), legend.text = element_text(colour = &quot;black&quot;, size = 10), plot.title = element_text(hjust = 0.5, size = 18), plot.margin = ggplot2::margin(10, 10, 10, 0, &quot;pt&quot;), axis.title.x = element_text(colour = &quot;black&quot;, size = 18), axis.title.y = element_text(colour = &quot;black&quot;, size = 18), axis.text.x = element_text(colour = &quot;black&quot;, size = 12), axis.text.y = element_text(colour = &quot;black&quot;, size = 12), legend.title = element_text(size = 14), legend.position = c(0.85, 0.5) ) + guides(fill = guide_colorbar(title = &quot;Accuracy&quot;)) + xlab(&quot;&quot;) + ylab(&quot;Coefficient&quot;) + ggtitle(&quot;Cell-penetration&quot;) 3.5 Toxicity (classification) 3.5.1 Model training train_tx_models &lt;- function(train_set, exp_dir, prefix, nfolds = 10, grid_seed = 1) { tmp &lt;- as.h2o(train_set, destination_frame = prefix) tmp[&quot;category&quot;] &lt;- as.factor(tmp[&quot;category&quot;]) y &lt;- &quot;category&quot; x &lt;- setdiff(names(tmp), y) res &lt;- as.data.frame(tmp$category) samp_factors &lt;- as.vector(mean(table(train_set$category)) / table(train_set$category)) # ------------------- # base model training # ------------------- cat(&quot;Deep learning grid 1\\n&quot;) deeplearning_1 &lt;- h2o.grid( algorithm = &quot;deeplearning&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = deeplearning_params_1, stopping_rounds = 3, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5, search_criteria = list( strategy = &quot;RandomDiscrete&quot;, max_models = 100, seed = grid_seed ), keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in deeplearning_1@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;GBM grid\\n&quot;) gbm &lt;- h2o.grid( algorithm = &quot;gbm&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = gbm_params, stopping_rounds = 3, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5, search_criteria = list(strategy = &quot;RandomDiscrete&quot;, max_models = 100, seed = grid_seed), keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in gbm@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;GBM 5 default models\\n&quot;) gbm_1 &lt;- h2o.gbm( model_id = &quot;GBM_1&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 6, min_rows = 1, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_2 &lt;- h2o.gbm( model_id = &quot;GBM_2&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 7, min_rows = 10, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_3 &lt;- h2o.gbm( model_id = &quot;GBM_3&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 8, min_rows = 10, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_4 &lt;- h2o.gbm( model_id = &quot;GBM_4&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 10, min_rows = 10, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_5 &lt;- h2o.gbm( model_id = &quot;GBM_5&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 15, min_rows = 100, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) for (model_id in paste0(&quot;GBM_&quot;, 1:5)) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;XGBoost grid\\n&quot;) xgboost &lt;- h2o.grid( algorithm = &quot;xgboost&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = xgboost_params, stopping_rounds = 3, search_criteria = list(strategy = &quot;RandomDiscrete&quot;, max_models = 100, seed = grid_seed), keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in xgboost@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;XGBoost 3 default models\\n&quot;) xgboost_1 &lt;- h2o.xgboost( model_id = &quot;XGBoost_1&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, booster = &quot;gbtree&quot;, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, max_depth = 10, min_rows = 5, ntrees = 10000, reg_alpha = 0, reg_lambda = 1, sample_rate = 0.6, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) xgboost_2 &lt;- h2o.xgboost( model_id = &quot;XGBoost_2&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, booster = &quot;gbtree&quot;, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, max_depth = 20, min_rows = 10, ntrees = 10000, reg_alpha = 0, reg_lambda = 1, sample_rate = 0.6, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) xgboost_3 &lt;- h2o.xgboost( model_id = &quot;XGBoost_3&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, booster = &quot;gbtree&quot;, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, max_depth = 5, min_rows = 3, ntrees = 10000, reg_alpha = 0, reg_lambda = 1, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) for (model_id in paste0(&quot;XGBoost_&quot;, 1:3)) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;GLM\\n&quot;) glm &lt;- h2o.glm( model_id = &quot;GLM&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, alpha = c(0.0, 0.2, 0.4, 0.6, 0.8, 1.0), balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;GLM&quot;), path = exp_dir, force = TRUE) cat(&quot;DRF\\n&quot;) drf &lt;- h2o.randomForest( model_id = &quot;DRF&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, ntrees = 10000, score_tree_interval = 5, stopping_rounds = 3, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;DRF&quot;), path = exp_dir, force = TRUE) cat(&quot;XRT\\n&quot;) xrt &lt;- h2o.randomForest( model_id = &quot;XRT&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, ntrees = 10000, histogram_type = &quot;Random&quot;, score_tree_interval = 5, stopping_rounds = 3, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;XRT&quot;), path = exp_dir, force = TRUE) # ----------------------- # get holdout predictions # ----------------------- base_models &lt;- as.list(c( unlist(deeplearning_1@model_ids), unlist(gbm@model_ids), paste0(&quot;GBM_&quot;, 1:5), unlist(xgboost@model_ids), paste0(&quot;XGBoost_&quot;, 1:3), &quot;GLM&quot;, &quot;DRF&quot;, &quot;XRT&quot; )) for (model_id in base_models) { res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[3], col.names = model_id )) } # ---------------------- # super learner training # ---------------------- sl_iter &lt;- 0 cat(paste0(&quot;Super learner iteration &quot;, sl_iter, &quot; (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = paste0(&quot;superlearner_iter_&quot;, sl_iter), training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list( standardize = TRUE, keep_cross_validation_predictions = TRUE, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5 ) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(paste0(&quot;superlearner_iter_&quot;, sl_iter)), path = exp_dir, force = TRUE) model_id &lt;- paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter) tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[3], col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) # ---------------------------------- # super learner base model reduction # ---------------------------------- while (TRUE) { meta &lt;- h2o.getModel(paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter, &quot;_cv_1&quot;)) names &lt;- meta@model$coefficients_table[, &quot;names&quot;] coeffs &lt;- (meta@model$coefficients_table[, &quot;standardized_coefficients&quot;] &gt; 0) for (j in 2:nfolds) { meta &lt;- h2o.getModel(paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter, &quot;_cv_&quot;, j)) names &lt;- meta@model$coefficients_table[, &quot;names&quot;] coeffs &lt;- coeffs + (meta@model$coefficients_table[, &quot;standardized_coefficients&quot;] &gt; 0) } base_models_ &lt;- as.list(names[coeffs &gt;= ceiling(nfolds / 2) &amp; names != &quot;Intercept&quot;]) if (length(base_models_) == 0) { cat(&quot;No base models passing the threshold\\n\\n&quot;) break } if (sum(base_models %in% base_models_) == length(base_models)) { cat(&quot;No further reduction of base models\\n\\n&quot;) break } sl_iter &lt;- sl_iter + 1 base_models &lt;- base_models_ cat(paste0(&quot;Super learner iteration &quot;, sl_iter, &quot; (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = paste0(&quot;superlearner_iter_&quot;, sl_iter), training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list( standardize = TRUE, keep_cross_validation_predictions = TRUE, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5 ) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(paste0(&quot;superlearner_iter_&quot;, sl_iter)), path = exp_dir, force = TRUE) model_id &lt;- paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter) tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[3], col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) } # ----------------------------------------- # super learner for homogeneous base models # ----------------------------------------- # DeepLearning base_models &lt;- deeplearning_1@model_ids cat(paste0(&quot;Super learner deep learning (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = &quot;superlearner_deeplearning&quot;, training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list( standardize = TRUE, keep_cross_validation_predictions = TRUE, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5 ) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;superlearner_deeplearning&quot;), path = exp_dir, force = TRUE) model_id &lt;- &quot;metalearner_glm_superlearner_deeplearning&quot; tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[3], col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) # GBM base_models &lt;- as.list(c(unlist(gbm@model_ids), paste0(&quot;GBM_&quot;, 1:5))) cat(paste0(&quot;Super learner GBM (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = &quot;superlearner_gbm&quot;, training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list( standardize = TRUE, keep_cross_validation_predictions = TRUE, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5 ) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;superlearner_gbm&quot;), path = exp_dir, force = TRUE) model_id &lt;- &quot;metalearner_glm_superlearner_gbm&quot; tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[3], col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) # XGBoost base_models &lt;- as.list(c(unlist(xgboost@model_ids), paste0(&quot;XGBoost_&quot;, 1:3))) cat(paste0(&quot;Super learner XGBoost (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = &quot;superlearner_xgboost&quot;, training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list( standardize = TRUE, keep_cross_validation_predictions = TRUE, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5 ) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;superlearner_xgboost&quot;), path = exp_dir, force = TRUE) model_id &lt;- &quot;metalearner_glm_superlearner_xgboost&quot; tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[3], col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) write.csv(res, file = paste0(exp_dir, &quot;/cv_holdout_predictions.csv&quot;), row.names = FALSE) cat(&quot;\\n\\n&quot;) h2o.removeAll() } 3.5.2 Inner cross-validation load(file = &quot;./rdata/var_reduct_tx_train_test_splits.RData&quot;) for (i in 1:10) { cat(paste0(&quot;Outer training set &quot;, i, &quot;\\n&quot;)) prefix &lt;- paste0(&quot;outer_&quot;, i) exp_dir &lt;- paste0(&quot;/Users/renee/Downloads/toxicity/&quot;, prefix) dir.create(exp_dir) train_tx_models(train_set = outer_splits[[i]][[&quot;train&quot;]], exp_dir = exp_dir, prefix = prefix) } 3.5.3 Training on whole data set load(file = &quot;./rdata/var_reduct_tx_train_test_splits.RData&quot;) prefix &lt;- &quot;whole_data_set&quot; exp_dir &lt;- paste0(&quot;/Users/renee/Downloads/toxicity/&quot;, prefix) dir.create(exp_dir) train_tx_models(train_set = tx_data, exp_dir = exp_dir, prefix = prefix) # Keep track of grid models dl_grid &lt;- list() gbm_grid &lt;- list() xgboost_grid &lt;- list() dl_grid_params &lt;- list() gbm_grid_params &lt;- list() xgboost_grid_params &lt;- list() for (i in 1:11) { if (i == 11) { cat(paste0(&quot;Whole data set\\n&quot;)) prefix &lt;- &quot;whole_data_set&quot; } else { cat(paste0(&quot;Outer training set &quot;, i, &quot;\\n&quot;)) prefix &lt;- paste0(&quot;outer_&quot;, i) } dir &lt;- paste0(&quot;/Users/renee/Downloads/toxicity/&quot;, prefix) files &lt;- list.files(dir) # Deep learning dl &lt;- files[str_detect(files, &quot;DeepLearning_model&quot;)] for (m in dl) { model &lt;- h2o.loadModel(paste0(dir, &quot;/&quot;, m)) hs &lt;- sha1(paste(c( model@allparameters$epsilon, model@allparameters$hidden, model@allparameters$hidden_dropout_ratios, model@allparameters$input_dropout_ratio, model@allparameters$rho ), collapse = &quot; &quot;)) if (hs %in% names(dl_grid)) { dl_grid[[hs]] &lt;- c(dl_grid[[hs]], m) } else { dl_grid[[hs]] &lt;- c(m) dl_grid_params &lt;- list.append( dl_grid_params, c( &quot;epsilon&quot; = model@allparameters$epsilon, &quot;hidden&quot; = paste0(&quot;[&quot;, paste(model@allparameters$hidden, collapse = &quot;,&quot;), &quot;]&quot;), &quot;hidden_dropout_ratios&quot; = paste0( &quot;[&quot;, paste(model@allparameters$hidden_dropout_ratios, collapse = &quot;,&quot; ), &quot;]&quot; ), &quot;input_dropout_ratio&quot; = model@allparameters$input_dropout_ratio, &quot;rho&quot; = model@allparameters$rho ) ) } } h2o.removeAll() # GBM gbm &lt;- files[str_detect(files, &quot;GBM_model&quot;)] for (m in gbm) { model &lt;- h2o.loadModel(paste0(dir, &quot;/&quot;, m)) hs &lt;- sha1(paste(c( model@allparameters$col_sample_rate, model@allparameters$col_sample_rate_per_tree, model@allparameters$max_depth, model@allparameters$min_rows, model@allparameters$min_split_improvement, model@allparameters$sample_rate ), collapse = &quot; &quot;)) if (hs %in% names(gbm_grid)) { gbm_grid[[hs]] &lt;- c(gbm_grid[[hs]], m) } else { gbm_grid[[hs]] &lt;- c(m) gbm_grid_params &lt;- list.append( gbm_grid_params, c( &quot;col_sample_rate&quot; = model@allparameters$col_sample_rate, &quot;col_sample_rate_per_tree&quot; = model@allparameters$col_sample_rate_per_tree, &quot;max_depth&quot; = model@allparameters$max_depth, &quot;min_rows&quot; = model@allparameters$min_rows, &quot;min_split_improvement&quot; = model@allparameters$min_split_improvement, &quot;sample_rate&quot; = model@allparameters$sample_rate ) ) } } h2o.removeAll() # XGBoost xgboost &lt;- files[str_detect(files, &quot;XGBoost_model&quot;)] for (m in xgboost) { model &lt;- h2o.loadModel(paste0(dir, &quot;/&quot;, m)) hs &lt;- sha1(paste(c( model@allparameters$booster, model@allparameters$col_sample_rate, model@allparameters$col_sample_rate_per_tree, model@allparameters$max_depth, model@allparameters$min_rows, model@allparameters$reg_alpha, model@allparameters$reg_lambda, model@allparameters$sample_rate ), collapse = &quot; &quot;)) if (hs %in% names(xgboost_grid)) { xgboost_grid[[hs]] &lt;- c(xgboost_grid[[hs]], m) } else { xgboost_grid[[hs]] &lt;- c(m) xgboost_grid_params &lt;- list.append( xgboost_grid_params, c( &quot;booster&quot; = model@allparameters$booster, &quot;col_sample_rate&quot; = model@allparameters$col_sample_rate, &quot;col_sample_rate_per_tree&quot; = model@allparameters$col_sample_rate_per_tree, &quot;max_depth&quot; = model@allparameters$max_depth, &quot;min_rows&quot; = model@allparameters$min_rows, &quot;reg_alpha&quot; = model@allparameters$reg_alpha, &quot;reg_lambda&quot; = model@allparameters$reg_lambda, &quot;sample_rate&quot; = model@allparameters$sample_rate ) ) } } h2o.removeAll() } dl_grid_params &lt;- as.data.frame(t(data.frame(dl_grid_params))) rownames(dl_grid_params) &lt;- paste(&quot;Neural network grid model&quot;, 1:nrow(dl_grid_params)) gbm_grid_params &lt;- as.data.frame(t(data.frame(gbm_grid_params))) rownames(gbm_grid_params) &lt;- paste(&quot;GBM grid model&quot;, 1:nrow(gbm_grid_params)) xgboost_grid_params &lt;- as.data.frame(t(data.frame(xgboost_grid_params))) rownames(xgboost_grid_params) &lt;- paste(&quot;XGBoost grid model&quot;, 1:nrow(xgboost_grid_params)) write.csv(dl_grid_params, &quot;./other_data/toxicity/neural_network_grid_params.csv&quot;) write.csv(gbm_grid_params, &quot;./other_data/toxicity/gbm_grid_params.csv&quot;) write.csv(xgboost_grid_params, &quot;./other_data/toxicity/xgboost_grid_params.csv&quot;) save(dl_grid, gbm_grid, xgboost_grid, file = &quot;./rdata/model_training_grid_models_tx.RData&quot;) 3.5.4 Model evaluation model_evaluation &lt;- function(holdout_pred, fold_asign, grid_name, grid_meta = NULL) { res_ &lt;- list() model_num &lt;- c() if (startsWith(grid_name, &quot;Super learner&quot;)) { if (grid_name == &quot;Super learner all models&quot;) { m &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_iter_0&quot;)] } else if (grid_name == &quot;Super learner final&quot;) { sl_models &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_iter&quot;)] m &lt;- sl_models[length(sl_models)] } else if (grid_name == &quot;Super learner neural network&quot;) { m &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_deeplearning&quot;)] } else if (grid_name == &quot;Super learner GBM&quot;) { m &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_gbm&quot;)] } else if (grid_name == &quot;Super learner XGBoost&quot;) { m &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_xgboost&quot;)] } logloss &lt;- c() MCC &lt;- c() F1 &lt;- c() acc &lt;- c() EF &lt;- c() BEDROC &lt;- c() threshold &lt;- 0.5 for (k in 1:10) { y_true &lt;- holdout_pred[fold_assign == k, &quot;category&quot;] y_pred &lt;- holdout_pred[fold_assign == k, m] y_pred_cls &lt;- sapply(y_pred, function(x) if (x &gt;= threshold) 1 else 0) logloss &lt;- c(logloss, LogLoss(y_pred = y_pred, y_true = y_true)) MCC &lt;- c(MCC, mcc(preds = y_pred_cls, actuals = y_true)) F1 &lt;- c(F1, F1_Score(y_pred = y_pred_cls, y_true = y_true)) acc &lt;- c(acc, Accuracy(y_pred = y_pred_cls, y_true = y_true)) EF &lt;- c(EF, enrichment_factor(x = y_pred, y = y_true, top = 0.05)) BEDROC &lt;- c(BEDROC, bedroc(x = y_pred, y = y_true, alpha = 20)) } res_ &lt;- list.append(res_, c( mean(logloss, na.rm = TRUE), sem(logloss), mean(MCC, na.rm = TRUE), sem(MCC), mean(F1, na.rm = TRUE), sem(F1), mean(acc, na.rm = TRUE), sem(acc), mean(EF, na.rm = TRUE), sem(EF), mean(BEDROC, na.rm = TRUE), sem(BEDROC), logloss, MCC, F1, acc, EF, BEDROC )) } else { for (j in 1:length(grid_meta)) { g &lt;- grid_meta[[j]] m &lt;- intersect(colnames(holdout_pred), g) if (length(m) == 1) { model_num &lt;- c(model_num, j) logloss &lt;- c() MCC &lt;- c() F1 &lt;- c() acc &lt;- c() EF &lt;- c() BEDROC &lt;- c() threshold &lt;- 0.5 for (k in 1:10) { y_true &lt;- holdout_pred[fold_assign == k, &quot;category&quot;] y_pred &lt;- holdout_pred[fold_assign == k, m] y_pred_cls &lt;- sapply(y_pred, function(x) if (x &gt;= threshold) 1 else 0) logloss &lt;- c(logloss, LogLoss(y_pred = y_pred, y_true = y_true)) MCC &lt;- c(MCC, mcc(preds = y_pred_cls, actuals = y_true)) F1 &lt;- c(F1, F1_Score(y_pred = y_pred_cls, y_true = y_true)) acc &lt;- c(acc, Accuracy(y_pred = y_pred_cls, y_true = y_true)) EF &lt;- c(EF, enrichment_factor(x = y_pred, y = y_true, top = 0.05)) BEDROC &lt;- c(BEDROC, bedroc(x = y_pred, y = y_true, alpha = 20)) } res_ &lt;- list.append(res_, c( mean(logloss, na.rm = TRUE), sem(logloss), mean(MCC, na.rm = TRUE), sem(MCC), mean(F1, na.rm = TRUE), sem(F1), mean(acc, na.rm = TRUE), sem(acc), mean(EF, na.rm = TRUE), sem(EF), mean(BEDROC, na.rm = TRUE), sem(BEDROC), logloss, MCC, F1, acc, EF, BEDROC )) } } } res &lt;- as.data.frame(t(data.frame(res_))) colnames(res) &lt;- c( &quot;Log loss mean&quot;, &quot;Log loss s.e.m.&quot;, &quot;MCC mean&quot;, &quot;MCC s.e.m.&quot;, &quot;F_1 mean&quot;, &quot;F_1 s.e.m.&quot;, &quot;Accuracy mean&quot;, &quot;Accuracy s.e.m.&quot;, &quot;EF mean&quot;, &quot;EF s.e.m.&quot;, &quot;BEDROC mean&quot;, &quot;BEDROC s.e.m.&quot;, paste(&quot;Log loss CV&quot;, 1:10), paste(&quot;MCC CV&quot;, 1:10), paste(&quot;F_1 CV&quot;, 1:10), paste(&quot;Accuracy CV&quot;, 1:10), paste(&quot;EF CV&quot;, 1:10), paste(&quot;BEDROC CV&quot;, 1:10) ) if (nrow(res) == 1) { rownames(res) &lt;- grid_name } else { rownames(res) &lt;- paste(grid_name, model_num) } return(res) } 3.5.5 Inner loop model selection load(file = &quot;./rdata/model_training_grid_models_tx.RData&quot;) for (i in 1:10) { holdout_pred &lt;- read.csv(paste0(&quot;./other_data/toxicity/outer_&quot;, i, &quot;/cv_holdout_predictions.csv&quot;)) fold_assign &lt;- rep(1:10, ceiling(nrow(holdout_pred) / 10))[1:nrow(holdout_pred)] data_ &lt;- list() # Deep learning grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Neural network grid model&quot;, grid_meta = dl_grid )) # GBM grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;GBM grid model&quot;, grid_meta = gbm_grid )) # GBM default models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;GBM model&quot;, grid_meta = as.list(paste0(&quot;GBM_&quot;, 1:5)) )) # XGBoost grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XGBoost grid model&quot;, grid_meta = xgboost_grid )) # XGBoost default models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XGBoost model&quot;, grid_meta = as.list(paste0(&quot;XGBoost_&quot;, 1:3)) )) # GLM data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;GLM model&quot;, grid_meta = list(&quot;GLM&quot;))) # DRF data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;DRF model&quot;, grid_meta = list(&quot;DRF&quot;))) # XRT data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XRT model&quot;, grid_meta = list(&quot;XRT&quot;))) # Super learner all data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner all models&quot;)) # Super learner final data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner final&quot;)) # Super learner deep learning data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner neural network&quot;)) # Super learner GBM data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner GBM&quot;)) # Super learner XGBoost data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner XGBoost&quot;)) data &lt;- do.call(rbind, data_) write.csv(data, paste0(&quot;./other_data/toxicity/outer_&quot;, i, &quot;/cv_res.csv&quot;)) } 3.5.5.1 CV 1 3.5.5.2 CV 2 3.5.5.3 CV 3 3.5.5.4 CV 4 3.5.5.5 CV 5 3.5.5.6 CV 6 3.5.5.7 CV 7 3.5.5.8 CV 8 3.5.5.9 CV 9 3.5.5.10 CV 10 3.5.6 Final evaluation load(file = &quot;./rdata/var_reduct_tx_train_test_splits.RData&quot;) load(file = &quot;./rdata/model_training_grid_models_tx.RData&quot;) dir &lt;- &quot;/Users/renee/Downloads/toxicity&quot; selected_models &lt;- c( &quot;Super learner XGBoost&quot;, &quot;Super learner final&quot;, &quot;Super learner final&quot;, &quot;Super learner final&quot;, &quot;Super learner final&quot;, &quot;Super learner final&quot;, &quot;Super learner final&quot;, &quot;Super learner final&quot;, &quot;Super learner final&quot;, &quot;Super learner final&quot; ) logloss &lt;- c() MCC &lt;- c() F1 &lt;- c() acc &lt;- c() EF &lt;- c() BEDROC &lt;- c() threshold &lt;- 0.5 for (i in 1:10) { holdout_pred &lt;- read.csv(paste0(&quot;./other_data/toxicity/outer_&quot;, i, &quot;/cv_holdout_predictions.csv&quot;)) if (startsWith(selected_models[i], &quot;Neural network grid model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) m &lt;- intersect(colnames(holdout_pred), dl_grid[[grid_num]]) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, m)) } else if (startsWith(selected_models[i], &quot;GBM grid model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) m &lt;- intersect(colnames(holdout_pred), gbm_grid[[grid_num]]) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, m)) } else if (startsWith(selected_models[i], &quot;GBM model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/GBM_&quot;, grid_num)) } else if (startsWith(selected_models[i], &quot;XGBoost grid model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) m &lt;- intersect(colnames(holdout_pred), xgboost_grid[[grid_num]]) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, m)) } else if (startsWith(selected_models[i], &quot;XGBoost model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/XGBoost_&quot;, grid_num)) } else if (selected_models[i] == &quot;Super learner all models&quot;) { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/superlearner_iter_0&quot;)) } else if (selected_models[i] == &quot;Super learner final&quot;) { files &lt;- list.files(paste0(dir, &quot;/outer_&quot;, i)) files &lt;- files[startsWith(files, &quot;superlearner_iter&quot;)] model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, files[length(files)])) } else if (selected_models[i] == &quot;Super learner neural network&quot;) { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/superlearner_deeplearning&quot;)) } else if (selected_models[i] == &quot;Super learner GBM&quot;) { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/superlearner_gbm&quot;)) } else if (selected_models[i] == &quot;Super learner XGBoost&quot;) { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/superlearner_xgboost&quot;)) } else { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, unlist(strsplit(selected_models[i], &quot; &quot;))[1])) } tmp &lt;- as.h2o(subset(outer_splits[[i]][[&quot;test&quot;]], select = -category)) y_true &lt;- as.numeric(outer_splits[[i]][[&quot;test&quot;]]$category) - 1 y_pred &lt;- as.data.frame(as.data.frame(h2o.predict(model, tmp)))[, 3] y_pred_cls &lt;- sapply(y_pred, function(x) if (x &gt;= threshold) 1 else 0) logloss &lt;- c(logloss, LogLoss(y_pred = y_pred, y_true = y_true)) MCC &lt;- c(MCC, mcc(preds = y_pred_cls, actuals = y_true)) F1 &lt;- c(F1, F1_Score(y_pred = y_pred_cls, y_true = y_true)) acc &lt;- c(acc, Accuracy(y_pred = y_pred_cls, y_true = y_true)) EF &lt;- c(EF, enrichment_factor(x = y_pred, y = y_true, top = 0.05)) BEDROC &lt;- c(BEDROC, bedroc(x = y_pred, y = y_true, alpha = 20)) } save(logloss, MCC, F1, acc, EF, BEDROC, file = &quot;./rdata/final_evaluation_tx.RData&quot;) load(file = &quot;./rdata/final_evaluation_tx.RData&quot;) data.frame( Metric = c(&quot;Log loss&quot;, &quot;MCC&quot;, &quot;F&lt;sub&gt;1&lt;/sub&gt;&quot;, &quot;Accuracy&quot;, &quot;EF&quot;, &quot;BEDROC&quot;), `Mean ± s.e.m.` = c( paste0(round(mean(logloss), 3), &quot; ± &quot;, round(sem(logloss), 3)), paste0(round(mean(MCC), 3), &quot; ± &quot;, round(sem(MCC), 3)), paste0(round(mean(F1), 3), &quot; ± &quot;, round(sem(F1), 3)), paste0(round(mean(acc), 3), &quot; ± &quot;, round(sem(acc), 3)), paste0(round(mean(EF), 3), &quot; ± &quot;, round(sem(EF), 3)), paste0(round(mean(BEDROC), 3), &quot; ± &quot;, round(sem(BEDROC), 3)) ), check.names = FALSE ) %&gt;% datatable(escape = FALSE, rownames = FALSE) 3.5.7 Final model selection load(file = &quot;./rdata/model_training_grid_models_tx.RData&quot;) holdout_pred &lt;- read.csv(&quot;./other_data/toxicity/whole_data_set/cv_holdout_predictions.csv&quot;) fold_assign &lt;- rep(1:10, ceiling(nrow(holdout_pred) / 10))[1:nrow(holdout_pred)] data_ &lt;- list() # Deep learning grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Neural network grid model&quot;, grid_meta = dl_grid )) # GBM grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;GBM grid model&quot;, grid_meta = gbm_grid )) # GBM default models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;GBM model&quot;, grid_meta = as.list(paste0(&quot;GBM_&quot;, 1:5)) )) # XGBoost grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XGBoost grid model&quot;, grid_meta = xgboost_grid )) # XGBoost default models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XGBoost model&quot;, grid_meta = as.list(paste0(&quot;XGBoost_&quot;, 1:3)) )) # GLM data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;GLM model&quot;, grid_meta = list(&quot;GLM&quot;))) # DRF data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;DRF model&quot;, grid_meta = list(&quot;DRF&quot;))) # XRT data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XRT model&quot;, grid_meta = list(&quot;XRT&quot;))) # Super learner all data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner all models&quot;)) # Super learner final data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner final&quot;)) # Super learner deep learning data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner neural network&quot;)) # Super learner GBM data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner GBM&quot;)) # Super learner XGBoost data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner XGBoost&quot;)) data &lt;- do.call(rbind, data_) write.csv(data, &quot;./other_data/toxicity/whole_data_set/cv_res.csv&quot;) data &lt;- read.csv(&quot;./other_data/toxicity/whole_data_set/cv_res.csv&quot;, row.names = 1, check.names = FALSE) statistical_testing(data, metric_vec = c(&quot;Log loss&quot;, &quot;MCC&quot;, &quot;F_1&quot;, &quot;Accuracy&quot;), decreasing_vec = c(FALSE, TRUE, TRUE, TRUE) ) # statistical_testing(data, metric_vec=c(&#39;Log loss&#39;, &#39;MCC&#39;, &#39;F_1&#39;, &#39;Accuracy&#39;), # decreasing_vec=c(FALSE, TRUE, TRUE, TRUE), # output_path=&#39;./other_data/toxicity/whole_data_set/cv_res_statistical_testing.csv&#39;) 3.5.8 Final reduced SL h2o.init(nthreads = -1) model_path &lt;- &quot;/Users/renee/Downloads/toxicity/whole_data_set/superlearner_iter_3&quot; model &lt;- h2o.loadModel(model_path) load(file = &quot;./rdata/model_training_grid_models_tx.RData&quot;) data &lt;- read.csv(&quot;./other_data/toxicity/whole_data_set/cv_res.csv&quot;, row.names = 1, check.names = FALSE) df &lt;- as.data.frame(model@model$metalearner_model@model$coefficients_table)[c(&quot;names&quot;, &quot;standardized_coefficients&quot;)][-1, ] for (i in 1:nrow(df)) { name &lt;- df$names[i] if (startsWith(name, &quot;DeepLearning_model&quot;)) { for (j in 1:length(dl_grid)) { if (name %in% dl_grid[[j]]) break } df$names[i] &lt;- paste(&quot;Neural network grid model&quot;, j) } else if (startsWith(name, &quot;GBM_model&quot;)) { for (j in 1:length(gbm_grid)) { if (name %in% gbm_grid[[j]]) break } df$names[i] &lt;- paste(&quot;GBM grid model&quot;, j) } else if (startsWith(name, &quot;XGBoost_model&quot;)) { for (j in 1:length(xgboost_grid)) { if (name %in% xgboost_grid[[j]]) break } df$names[i] &lt;- paste(&quot;XGBoost grid model&quot;, j) } else { df$names[i] &lt;- paste(strsplit(name, &quot;_&quot;)[[1]], collapse = &quot; model &quot;) } } rownames(df) &lt;- df$names df &lt;- merge(df, data[&quot;Accuracy mean&quot;], by = &quot;row.names&quot;)[, -1] df &lt;- df[df$standardized_coefficients &gt; 0, ] p3 &lt;- ggplot(df, aes(x = reorder(names, standardized_coefficients), y = standardized_coefficients, fill = `Accuracy mean`)) + geom_col(color = &quot;black&quot;, size = 0.2) + scale_fill_gradient(low = &quot;white&quot;, high = &quot;red&quot;, n.breaks = 4, limits = c(0.85, 1.00)) + geom_text(aes(label = sprintf(&quot;%.2f&quot;, `Accuracy mean`)), nudge_y = -0.008, size = 3, color = &quot;white&quot;) + geom_text(aes(label = sprintf(&quot;%.3f&quot;, standardized_coefficients)), nudge_y = 0.01, size = 3) + coord_flip() + scale_y_continuous(limits = c(0, max(df$standardized_coefficients) + 0.02), expand = c(0.01, 0)) + theme_bw() + theme( panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank(), panel.border = element_blank(), axis.line = element_line(color = &quot;black&quot;), legend.text = element_text(colour = &quot;black&quot;, size = 10), plot.title = element_text(hjust = 0.5, size = 18), plot.margin = ggplot2::margin(10, 10, 10, 0, &quot;pt&quot;), axis.title.x = element_text(colour = &quot;black&quot;, size = 18), axis.title.y = element_text(colour = &quot;black&quot;, size = 18), axis.text.x = element_text(colour = &quot;black&quot;, size = 12), axis.text.y = element_text(colour = &quot;black&quot;, size = 12), legend.title = element_text(size = 14), legend.position = c(0.85, 0.5) ) + guides(fill = guide_colorbar(title = &quot;Accuracy&quot;)) + xlab(&quot;&quot;) + ylab(&quot;Coefficient&quot;) + ggtitle(&quot;Non-toxic&quot;) # Close h2o server # h2o.shutdown() sessionInfo() ## R version 4.2.2 (2022-10-31) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur ... 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] reshape2_1.4.4 DT_0.24 digest_0.6.29 stringr_1.4.1 ## [5] rlist_0.4.6.2 enrichvs_0.0.5 scales_1.2.1 mltools_0.3.5 ## [9] MLmetrics_1.1.1 h2o_3.38.0.2 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_1.0.9 highr_0.9 plyr_1.8.7 bslib_0.4.0 ## [5] compiler_4.2.2 jquerylib_0.1.4 R.methodsS3_1.8.2 R.utils_2.12.0 ## [9] bitops_1.0-7 tools_4.2.2 lifecycle_1.0.1 jsonlite_1.8.0 ## [13] evaluate_0.16 R.cache_0.16.0 lattice_0.20-45 rlang_1.0.4 ## [17] Matrix_1.5-1 cli_3.3.0 rstudioapi_0.14 crosstalk_1.2.0 ## [21] yaml_2.3.5 xfun_0.32 fastmap_1.1.0 styler_1.8.0 ## [25] knitr_1.40 vctrs_0.4.1 htmlwidgets_1.5.4 sass_0.4.2 ## [29] grid_4.2.2 data.table_1.14.2 R6_2.5.1 rmarkdown_2.16 ## [33] bookdown_0.28 purrr_0.3.4 magrittr_2.0.3 codetools_0.2-18 ## [37] htmltools_0.5.3 colorspace_2.0-3 stringi_1.7.8 munsell_0.5.0 ## [41] RCurl_1.98-1.8 cachem_1.0.6 R.oo_1.25.0 "],["id_04_model_validation_and_interpretation.html", "Section 4 Model validation and interpretation 4.1 Model validation 4.2 Overall model interpretation 4.3 Multifunctional peptide selection 4.4 Explanation of HR97 predictions 4.5 Peptide design space visualization", " Section 4 Model validation and interpretation library(h2o) library(ggplot2) library(ggpubr) library(ggbeeswarm) library(reticulate) use_condaenv(&quot;/Users/renee/Library/r-miniconda/envs/peptide_engineering/bin/python&quot;) library(reshape2) library(scales) library(ggforce) library(cowplot) library(plotly) library(ggrepel) import h2o import pandas as pd import numpy as np import shap import matplotlib.pyplot as plt import session_info 4.1 Model validation 4.1.1 Melaning binding # ================ # predicted values # ================ mb_pred_ &lt;- matrix( c( -0.0202, 1.3650, 2.1745, 2.3456, 2.5907, 2.6903, 2.9842, 3.1325, 3.5015, 3.6640, 3.7181, 4.0701, 0.3223, 1.5153, 2.1196, 2.4047, 2.5032, 2.8177, 3.0653, 3.1491, 3.5578, 3.6465, 3.5599, 3.9509, 0.6579, 2.1797, 1.9685, 2.3709, 2.6702, 3.0694, 2.9323, 3.3150, 3.4798, 3.5661, 3.6981, 3.9730, -0.0589, 1.3490, 2.4402, 2.1269, 2.5097, 2.6128, 2.9359, 3.3530, 3.8094, 3.5260, 3.7525, 3.8405, 0.6532, 1.3684, 2.6170, 2.5270, 2.5228, 2.9967, 3.1125, 3.2106, 3.5952, 3.7345, 3.7353, 3.8400, 0.2941, 1.1896, 2.1987, 2.7585, 3.1348, 2.9791, 3.2625, 3.0559, 3.5153, 3.6829, 3.9875, 4.0330, 0.0156, 1.1113, 1.9551, 2.3677, 2.4250, 4.0111, 3.4891, 3.6521, 3.6634, 3.2731, 3.4504, 3.7587, 0.0209, 1.9146, 2.2865, 2.5308, 2.7193, 3.0322, 3.1020, 3.3582, 3.2993, 3.8534, 4.9542, 3.5464 ), nrow = 8, byrow = TRUE ) new_mb_pred_ &lt;- c( 3.9318, 3.9366, 3.5531, 3.4385, 3.8434, 3.3893, 4.4159, 4.7549, 4.0004, 3.2785, 4.5784, 4.6026, 3.6229, 3.8261, 3.6111, 5.0536, 4.8169, 5.9021, 4.0992, 4.3451, 5.9626, 4.2730, 4.4115, 4.8889, 4.3264, 4.4830, 4.2847, 4.6585, 4.4568, 4.8513, 4.1523 ) mb_pred &lt;- c(as.vector(mb_pred_), new_mb_pred_) for (i in 1:127) { if (mb_pred[i] &lt; 0) mb_pred[i] &lt;- NA } # Re-scale to 0-100 load(file = &quot;./rdata/var_reduct_mb_train_test_splits.RData&quot;) mb_pred &lt;- rescale(mb_pred, to = c(0, 100), from = range(mb_data$log_intensity)) mb_pred[mb_pred &gt; 100] &lt;- 100 # =================== # Experimental values # =================== replicate_1_1 &lt;- matrix( c( 0.9775, 50.1718, 39.9822, 66.0486, 46.0249, 40.8353, 58.6552, 72.8258, 55.1955, 59.8874, 78.4182, 77.8258, -3.6197, 25.3377, 56.6884, 58.1339, -0.8945, 26.5936, 64.4609, 63.2050, 68.3472, 71.3566, 80.4325, 70.3140, -34.7571, 26.0249, 52.3519, 70.4562, 63.1813, 69.1291, 73.1339, 65.6220, 73.9396, 76.5699, 75.1955, 76.0249, -1.3922, 35.4325, 4.0344, 16.4514, 28.4656, 38.9870, 71.5699, 66.8306, 71.0723, 45.0059, 83.4182, 75.9301, -6.0604, 19.9111, 20.2666, 39.2476, 36.3329, 70.1955, 73.9633, 72.4467, 75.5746, 59.9348, 55.4799, 75.8590, 1.9727, 28.0628, -29.7097, 35.4562, 58.8211, 49.7927, 61.7121, 77.8732, 77.2097, 77.2571, 54.8874, 76.0249, -23.3353, 33.2998, 45.9064, 36.1671, 36.4751, 74.5557, 70.6220, 48.2998, 69.6742, 76.9727, 65.1244, 75.9064, 2.2571, 27.1860, -1.6055, 42.3519, 48.7500, 47.8969, 62.2334, 70.6220, 79.7927, 75.9538, 77.4467, 81.0960 ), nrow = 8, byrow = TRUE ) replicate_1_2 &lt;- matrix( c( -7.7192, 54.0818, 45.1007, 62.8495, 50.1481, 37.6126, 59.5083, 71.7121, 57.7073, 59.4135, 77.0201, 82.3756, 21.4277, 10.8353, 54.6268, 65.7168, 15.6220, 35.1007, 64.8637, 67.8258, 66.5225, 72.7073, 83.1576, 72.7784, -53.7855, 2.6600, 42.8732, 32.9443, 48.1576, 71.0960, 73.4656, 66.9964, 72.1386, 78.6789, 78.7500, 77.1386, 34.9822, 37.1623, 25.8590, 34.7453, 36.9964, 54.2239, 72.8969, 67.5415, 73.5604, 52.8021, 88.1339, 80.0296, 1.1434, 30.6694, 32.8969, 45.5983, 48.6552, 70.5036, 74.7927, 76.0249, 75.5983, 62.6600, 61.3803, 81.4277, 27.4467, 30.3614, 24.8164, 25.8116, 63.9633, 49.7927, 60.6457, 79.8637, 77.3282, 79.8164, 55.0770, 77.1386, 7.9206, 44.0107, 57.2571, 50.1955, 45.6220, 76.2618, 71.7358, 51.6410, 72.9680, 76.6410, 64.1291, 76.5225, -3.7618, 32.5178, 28.9159, 47.3756, 50.5983, 49.4609, 62.7547, 71.7358, 83.3709, 81.7832, 83.3235, 82.9917 ), nrow = 8, byrow = TRUE ) replicate_2_1 &lt;- matrix( c( -21.6291, 12.7784, 51.7595, 30.0770, 33.6078, 28.2524, 47.4467, 51.8780, 45.4088, 64.9585, 64.9585, 69.3187, 24.2476, 7.0675, 56.8780, 71.7595, 25.8116, 40.1481, 69.2476, 71.3803, 74.9585, 73.1813, 81.8069, 74.8400, 44.5557, 31.2618, 43.6552, 39.0818, 48.3235, 67.8495, 76.2855, 64.5557, 74.5083, 76.0723, 75.4799, 75.5746, 17.3045, 45.8590, -7.5533, 34.7690, 28.3945, 41.9491, 73.3709, 69.8874, 70.1244, 62.9206, 71.8306, 78.6552, 17.0675, 34.8874, 19.2002, 35.7168, 43.5130, 2.1386, 70.8353, 50.4325, 73.8922, 68.7974, 76.4277, 77.3756, 51.4751, 2.5652, 13.2998, 25.7405, 58.7500, 50.5036, 64.6979, 76.6410, 67.4467, 80.3614, 63.3945, 75.5746, -41.2500, 44.1528, 51.4040, 35.3377, 57.3993, 78.3945, 72.7784, 56.4751, 75.5746, 74.5320, 68.5604, 75.0533, 15.5036, 31.6173, 11.4040, 49.9111, 49.6505, 48.0865, 61.4040, 72.7784, 79.9822, 77.9443, 71.5699, 79.8164 ), nrow = 8, byrow = TRUE ) replicate_2_2 &lt;- matrix( c( 13.5367, 25.8827, 48.0628, 76.2855, 58.0628, 54.2713, 69.7927, 78.9633, 55.6457, 78.2287, 80.5983, 83.6789, 51.5699, 6.1671, 50.8116, 73.3472, 28.4419, 26.0960, 73.4893, 76.5699, 74.5320, 78.7263, 85.4799, 80.1718, 41.7595, 48.1339, 39.0818, 43.8211, 47.9917, 65.2429, 79.1765, 73.6078, 78.5604, 82.7547, 81.2855, 82.6126, 38.0154, 30.6931, 42.3045, 41.2145, 30.7642, 57.1386, 75.6220, 70.5746, 72.1386, 66.0960, 78.2287, 80.9775, 19.6742, 25.1481, 36.0012, 28.8685, 33.3472, 70.0533, 73.1102, 55.5509, 73.3945, 74.7216, 80.4325, 80.8827, 46.7832, 28.2050, 30.6220, 30.2903, 60.5036, 34.1291, 66.9964, 77.6363, 76.3803, 85.6694, 68.1102, 82.6126, 6.4040, 24.1528, 64.3661, 47.6126, 63.5604, 81.3566, 79.8164, 61.2382, 78.0628, 78.5604, 70.7405, 76.9017, 25.3140, 42.5889, 40.6694, 50.2192, 53.0154, 47.0912, 66.6173, 79.8164, 83.3709, 82.6363, 77.1623, 84.3898 ), nrow = 8, byrow = TRUE ) replicate_1_1 &lt;- as.vector(replicate_1_1) replicate_1_2 &lt;- as.vector(replicate_1_2) replicate_2_1 &lt;- as.vector(replicate_2_1) replicate_2_2 &lt;- as.vector(replicate_2_2) mb_true_ &lt;- c() for (i in 1:96) { res_ &lt;- mean(c( mean(c(replicate_1_1[i], replicate_1_2[i])), mean(c(replicate_2_1[i], replicate_2_2[i])) )) if (res_ &gt;= 0) { mb_true_ &lt;- c(mb_true_, res_) } else { mb_true_ &lt;- c(mb_true_, NA) } } new_replicate_1 &lt;- c( 79.9064, 80.2179, 75.9474, 79.1975, 71.6238, 53.5235, 83.5678, 81.2452, 74.2512, 68.4291, 90.2855, 89.2512, 92.8356, 80.2853, 62.0912, 89.2853, 70.1267, 90.2891, 63.2853, 70.6812, 89.2582, 85.2567, 78.3682, 90.5732, 82.5253, 84.2912, 82.5923, 90.8613, 73.2862, 90.4823, 59.2715 ) new_replicate_2 &lt;- c( 77.6257, 82.2692, 71.8684, 79.1338, 67.7994, 52.3258, 88.2912, 79.6823, 78.5257, 62.5212, 81.2842, 83.5812, 90.2512, 73.4821, 63.6843, 88.2965, 72.0582, 93.1925, 62.9142, 78.0396, 86.9323, 82.5323, 72.6429, 83.6736, 89.5191, 86.8432, 81.5171, 92.1856, 92.6736, 93.9124, 95.3929 ) new_replicate_3 &lt;- c( 79.7602, 83.5192, 74.6645, 80.8854, 67.3918, 49.5821, 93.1263, 84.2590, 76.9253, 60.4827, 93.4825, 85.9901, 88.5705, 79.0125, 60.1825, 90.5812, 71.2281, 89.9251, 64.2681, 72.5712, 83.9235, 84.2719, 71.1281, 95.2182, 90.0191, 89.1812, 82.7195, 89.9501, 83.8581, 85.3725, 93.2801 ) new_mb_true_ &lt;- c() for (i in 1:31) { res_ &lt;- mean(c(new_replicate_1[i], new_replicate_2[i], new_replicate_3[i])) if (res_ &gt;= 0) { new_mb_true_ &lt;- c(new_mb_true_, res_) } else { new_mb_true_ &lt;- c(new_mb_true_, NA) } } mb_true &lt;- c(mb_true_, new_mb_true_) mb_df &lt;- data.frame(mb_pred = mb_pred, mb_true = mb_true) mb_df &lt;- mb_df[complete.cases(mb_df), ] p1 &lt;- ggscatter(mb_df, x = &quot;mb_pred&quot;, y = &quot;mb_true&quot;, alpha = 0.8, size = 2.5, stroke = 0.2, color = &quot;black&quot;, fill = &quot;grey70&quot;, shape = 21, add = &quot;reg.line&quot;, conf.int = TRUE, conf.int.level = 0.95, cor.coef = TRUE, add.params = list(color = &quot;grey25&quot;, fill = &quot;grey70&quot;), cor.coeff.args = list( method = &quot;pearson&quot;, label.x = 2, label.y = 95, label.sep = &quot;\\n&quot;, size = 4.5 ) ) + theme_bw() + theme( panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank(), panel.border = element_blank(), axis.line = element_line(color = &quot;black&quot;), axis.title.x = element_text(colour = &quot;black&quot;, size = 15), axis.title.y = element_text(colour = &quot;black&quot;, size = 15), axis.text.x = element_text(colour = &quot;black&quot;, size = 12), axis.text.y = element_text(colour = &quot;black&quot;, size = 12) ) + xlab(&quot;Predicted peptide bound\\nto b-mNPs (%)&quot;) + ylab(&quot;Measured peptide bound\\nto mNPs (%)&quot;) + xlim(-5, 105) + ylim(-5, 105) 4.1.2 Melanin binding and cell-penetration # ================ # predicted values # ================ cpp_pred_ &lt;- matrix( c( &quot;Non-CPP&quot;, &quot;Non-CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;Non-CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;Non-CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;Non-CPP&quot;, &quot;Non-CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;Non-CPP&quot;, &quot;Non-CPP&quot;, &quot;Non-CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;Non-CPP&quot;, &quot;Non-CPP&quot;, &quot;Non-CPP&quot;, &quot;Non-CPP&quot;, &quot;Non-CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot; ), nrow = 8, byrow = TRUE ) new_cpp_pred_ &lt;- c( &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot;, &quot;CPP&quot; ) cpp_pred &lt;- c(as.vector(cpp_pred_), new_cpp_pred_) # =================== # Experimental values # =================== non_induced_replicate_1 &lt;- matrix( c( 31.3344, 24.1344, 25.3344, 45.4944, 16.0944, 146.6544, 20.2944, 18.9744, 15.3744, 12.0144, 19.8144, 15.3744, 20.8944, 20.1744, 17.2944, 63.1344, 15.6144, 15.6144, 32.4144, 16.5744, 17.7744, 12.1344, 14.7744, 55.9344, 19.4544, 21.3744, 29.1744, 25.9344, 89.5344, 14.8944, 19.9344, 11.4144, 29.5344, 13.5744, 13.3344, 33.9744, 13.2144, 8.1744, 29.7744, -24.8256, 14.2944, 18.4944, 15.7344, 14.6544, 14.1744, 10.9344, 9.1344, 55.6944, 13.9344, 12.8544, 36.7344, 20.8944, 15.2544, 17.5344, 13.3344, 17.4144, 20.7744, 10.9344, 15.8544, 19.4544, 10.4544, 20.2944, 25.9344, 19.9344, 18.8544, 15.8544, 15.7344, 20.2944, 13.4544, 7.5744, 11.7744, 33.9744, 17.7744, 24.8544, 23.6544, 19.5744, 19.9344, 77.5344, 24.0144, 24.8544, 24.4944, 29.5344, 31.5744, 53.8944, 9.7344, 10.5744, 25.3344, 15.7344, 27.9744, 27.4944, 19.6944, 19.4544, 27.4944, 24.4944, 24.6144, 26.2944 ), nrow = 8, byrow = TRUE ) non_induced_replicate_2 &lt;- matrix( c( 30.3744, 28.4544, 26.1744, 30.0144, 25.6944, 131.5344, 31.8144, 21.9744, 30.4944, 20.2944, 17.7744, 26.7744, 27.4944, 20.6544, 21.3744, 44.2944, 23.6544, 28.5744, 26.5344, 22.0944, 26.7744, 17.5344, 18.9744, 63.0144, 25.6944, 22.8144, 27.6144, 26.7744, 88.9344, 30.8544, 24.1344, 10.9344, 19.9344, 19.5744, 21.4944, 25.2144, 12.9744, 15.0144, 17.7744, 14.8944, 13.5744, 24.4944, 27.6144, 13.9344, 23.1744, 17.7744, 15.7344, 45.6144, 16.0944, 23.1744, 17.7744, 15.7344, 20.6544, 27.6144, 34.0944, 24.6144, 22.8144, 14.4144, 26.7744, 25.6944, 12.0144, 31.8144, 16.4544, 21.6144, 30.2544, 18.7344, 24.9744, 29.7744, 15.1344, 15.1344, 27.8544, 25.2144, 23.2944, 28.2144, 30.6144, 22.0944, 19.9344, 63.0144, 22.8144, 22.2144, 21.6144, 19.9344, 23.0544, 30.9744, 14.0544, 23.7744, 20.6544, 11.0544, 23.8944, 31.4544, 20.6544, 15.4944, 31.4544, 26.0544, 33.0144, 37.0944 ), nrow = 8, byrow = TRUE ) non_induced_replicate_3 &lt;- matrix( c( 27.3744, 16.3344, 17.5344, 33.7344, 13.8144, 110.8944, 33.4944, 37.2144, 34.8144, 20.8944, 39.4944, 36.1344, 15.9744, 13.4544, 18.6144, 28.2144, 13.0944, 12.8544, 46.3344, 27.6144, 64.6944, 3.4944, 25.5744, 40.8144, 13.8144, 13.5744, 35.7744, 16.4544, 54.4944, 14.2944, 30.7344, 23.6544, 41.0544, 8.5344, 23.1744, 52.2144, 32.5344, 19.4544, 27.2544, 27.1344, 20.5344, 27.3744, 30.6144, 25.6944, 27.1344, 28.3344, 29.4144, 27.3744, 30.0144, 19.5744, 31.3344, 31.9344, 25.6944, 26.7744, 36.4944, 25.8144, 35.2944, 7.8144, 45.1344, 21.7344, 19.4544, 33.4944, 22.8144, 19.8144, 21.9744, 22.8144, 24.8544, 29.4144, 31.3344, 33.3744, 42.9744, 52.2144, 12.4944, 15.3744, 27.3744, 10.4544, 7.9344, 64.3344, 36.0144, 68.7744, 27.6144, 41.0544, 47.5344, 70.2144, 20.4144, 10.8144, 28.0944, 20.5344, 29.0544, 37.2144, 27.9744, 14.1744, 37.2144, 29.4144, 23.1744, 25.5744 ), nrow = 8, byrow = TRUE ) non_induced_true_ &lt;- c() for (i in 1:96) { res_ &lt;- mean(c(non_induced_replicate_1[i], non_induced_replicate_2[i], non_induced_replicate_3[i])) non_induced_true_ &lt;- c(non_induced_true_, res_) } induced_replicate_1 &lt;- matrix( c( 171.4944, 32.1744, 23.6544, 24.7344, 47.1744, 404.5344, 224.2944, 71.8944, 442.4544, 394.5744, 294.8544, 464.8944, 39.4944, 24.2544, 59.4144, 55.6944, 31.9344, 182.2944, 195.2544, 434.1744, 486.8544, 406.8144, 25.4544, 524.5344, 28.8144, 76.2144, 160.6944, 32.6544, 424.8144, 60.4944, 474.1344, 281.2944, 240.2544, 226.8144, 386.1744, 235.2144, 20.7744, 28.9344, 43.6944, 34.2144, 20.6544, 45.9744, 297.0144, 30.9744, 540.3744, 235.4544, 618.3744, 395.7744, 24.7344, 13.9344, 30.2544, 23.5344, 14.8944, 17.1744, 40.5744, 132.4944, 410.4144, 53.2944, 352.8144, 427.2144, 13.3344, 224.2944, 28.2144, 48.0144, 62.5344, 25.6944, 352.6944, 245.6544, 80.8944, 313.5744, 414.6144, 235.2144, 20.8944, 29.8944, 36.9744, 19.6944, 12.7344, 422.1744, 341.8944, 120.2544, 333.6144, 281.2944, 534.7344, 356.2944, 15.7344, 66.4944, 33.3744, 21.0144, 73.5744, 17.8944, 108.9744, 116.1744, 618.3744, 538.0944, 554.0544, 502.4544 ), nrow = 8, byrow = TRUE ) induced_replicate_2 &lt;- matrix( c( 39.1344, 30.1344, 34.5744, 367.2144, 36.7344, 185.4144, 192.9744, 434.2944, 376.2144, 305.4144, 192.9744, 420.6144, 41.4144, 19.4544, 101.6544, 370.0944, 43.9344, 231.1344, 110.1744, 476.8944, 395.6544, 315.6144, 75.9744, 435.2544, 26.8944, 60.6144, 29.8944, 200.1744, 460.5744, 265.9344, 541.6944, 310.2144, 282.6144, 404.4144, 168.9744, 196.2144, 12.6144, 137.7744, 16.0944, 16.2144, 47.8944, 24.7344, 274.4544, 62.0544, 247.9344, 209.2944, 393.4944, 423.3744, 22.2144, 21.3744, 11.1744, 9.1344, 128.4144, 19.6944, 58.9344, 233.1744, 437.4144, 58.6944, 361.0944, 458.8944, 16.8144, 33.3744, 12.3744, 24.3744, 134.1744, 25.9344, 364.4544, 207.8544, 91.2144, 383.6544, 434.0544, 196.2144, 11.8944, 21.7344, 17.4144, 16.8144, 19.9344, 525.4944, 422.4144, 519.9744, 379.5744, 282.6144, 616.9344, 450.0144, 19.2144, 42.7344, 19.4544, 25.9344, 54.7344, 42.9744, 73.2144, 135.4944, 524.0544, 471.7344, 528.4944, 511.0944 ), nrow = 8, byrow = TRUE ) induced_replicate_3 &lt;- matrix( c( 30.3744, 20.2944, 233.8944, 83.8944, 24.2544, 25.3344, 136.2144, 374.8944, 334.5744, 399.6144, 383.6544, 544.6944, 18.9744, 21.1344, 20.0544, 401.7744, 22.6944, 62.7744, 51.4944, 430.6944, 303.7344, 466.6944, 329.6544, 567.7344, 21.8544, 75.3744, 51.9744, 150.3744, 411.4944, 122.0544, 458.2944, 127.8144, 38.4144, 269.8944, 51.9744, 366.9744, 12.1344, 6.6144, 8.8944, 16.8144, 13.3344, 24.4944, 276.9744, 77.4144, 163.6944, 297.8544, 423.9744, 492.8544, 13.9344, 1.9344, 10.4544, 10.5744, 19.0944, 17.6544, 33.0144, 86.7744, 43.2144, 316.0944, 382.8144, 351.9744, 1.4544, 136.2144, 9.9744, 13.4544, 101.5344, 18.1344, 403.9344, 183.0144, 389.7744, 70.4544, 444.1344, 149.2944, 7.5744, 15.3744, 141.7344, 15.0144, 12.1344, 423.4944, 422.7744, 94.8144, 269.8944, 289.2144, 473.7744, 364.5744, 0.8544, 25.5744, 14.4144, 8.4144, 37.3344, 37.2144, 250.5744, 159.3744, 608.5344, 466.2144, 524.4144, 510.4944 ), nrow = 8, byrow = TRUE ) induced_true_ &lt;- c() for (i in 1:96) { res_ &lt;- mean(c(induced_replicate_1[i], induced_replicate_2[i], induced_replicate_3[i])) induced_true_ &lt;- c(induced_true_, res_) } new_non_induced_replicate_1 &lt;- c( 94.6865, 84.2145, 58.6890, 69.1610, 35.1270, 12.4261, 19.8169, 12.4261, 12.4368, 19.8169, 12.4261, 19.8169, 12.4368, 10.1149, 8.6644, 38.8384, 12.0117, 23.1802, 9.9236, 10.7153, 55.7665, 14.1476, 24.8167, 11.4644, 21.2568, 41.3144, 11.8948, 17.2878, 21.1505, 12.9894, 20.8902 ) new_non_induced_replicate_2 &lt;- c( 88.1415, 77.0150, 52.1440, 43.6355, 35.1270, 11.6132, 28.8335, 11.6132, 13.2019, 28.8335, 11.6132, 28.8335, 13.2019, 11.6717, 10.0671, 30.3106, 13.4888, 34.5612, 12.9309, 12.6759, 42.8393, 17.0753, 38.7481, 11.7673, 24.1153, 35.5229, 13.7651, 22.8933, 25.4702, 14.9234, 29.0832 ) new_non_induced_replicate_3 &lt;- c( 64.5795, 63.2705, 48.2170, 52.1440, 27.9275, 15.0881, 40.0817, 15.0881, 13.9936, 40.0817, 15.0881, 40.0817, 13.9936, 8.5847, 13.4835, 64.9266, 13.2444, 56.1065, 10.6090, 13.8660, 69.1400, 21.0974, 67.0891, 10.8056, 25.8900, 55.3361, 16.4695, 24.6201, 42.7755, 14.1317, 49.6828 ) new_non_induced_true_ &lt;- c() for (i in 1:31) { res_ &lt;- mean(c(new_non_induced_replicate_1[i], new_non_induced_replicate_2[i], new_non_induced_replicate_3[i])) new_non_induced_true_ &lt;- c(new_non_induced_true_, res_) } new_induced_replicate_1 &lt;- c( 775.3412, 608.3964, 376.3855, 403.2684, 450.2610, 104.3842, 610.9376, 104.3842, 206.0770, 610.9376, 104.3842, 610.9376, 206.0770, 61.42746, 63.9640, 725.5767, 182.7868, 436.4094, 89.5931, 105.8666, 643.6163, 295.6471, 536.2245, 141.8066, 415.8205, 549.5332, 258.9823, 532.0408, 386.370, 313.6335, 596.1610 ) new_induced_replicate_2 &lt;- c( 720.8794, 670.3267, 402.6897, 361.0000, 423.5832, 95.4568, 353.5596, 95.4568, 173.0359, 353.5596, 95.4568, 353.5596, 173.0359, 70.6842, 58.2979, 560.2724, 235.8898, 367.2965, 83.7953, 102.8688, 552.6956, 249.6267, 461.8738, 114.8928, 335.8037, 455.3512, 205.2205, 327.7658, 417.4017, 186.3116, 851.0120 ) new_induced_replicate_3 &lt;- c( 783.3711, 690.3612, 393.6819, 493.6712, 480.9125, 86.2659, 417.0393, 86.2659, 159.3978, 417.0393, 86.2659, 417.0393, 159.3978, 60.1427, 53.6201, 512.8355, 120.0977, 394.4739, 100.9911, 112.4550, 577.1388, 243.4006, 552.0697, 118.0552, 376.5862, 513.8896, 224.0635, 387.4242, 380.8028, 221.7246, 766.4130 ) new_induced_true_ &lt;- c() for (i in 1:31) { res_ &lt;- mean(c(new_induced_replicate_1[i], new_induced_replicate_2[i], new_induced_replicate_3[i])) new_induced_true_ &lt;- c(new_induced_true_, res_) } cpp_true_non_induced &lt;- c(non_induced_true_, new_non_induced_true_) cpp_true_induced &lt;- c(induced_true_, new_induced_true_) mb_cp_df &lt;- data.frame( mb_true = rep(mb_true, 2), cpp_true = c(cpp_true_non_induced, cpp_true_induced), mb_pred = rep(mb_pred, 2), cpp_pred = factor(rep(cpp_pred, 2), levels = c(&quot;Non-CPP&quot;, &quot;CPP&quot;)), cell_group = as.factor(c( rep(&quot;Non-induced&quot;, length(cpp_true_non_induced)), rep(&quot;Induced&quot;, length(cpp_true_induced)) )) ) 4.1.3 Melanin-induced cells mb_cp_df$log_cpp_true &lt;- log10(mb_cp_df$cpp_true) p2_1 &lt;- ggscatter(mb_cp_df[mb_cp_df$cell_group == &quot;Induced&quot;, ], x = &quot;mb_true&quot;, y = &quot;log_cpp_true&quot;, alpha = 0.7, size = 3, stroke = 0.3, color = &quot;black&quot;, fill = &quot;cpp_pred&quot;, shape = &quot;cpp_pred&quot;, add = &quot;reg.line&quot;, conf.int = TRUE, conf.int.level = 0.95, cor.coef = TRUE, add.params = list(color = &quot;grey25&quot;), cor.coeff.args = list( method = &quot;pearson&quot;, label.x = 2.2, label.y = 2.85, label.sep = &quot;\\n&quot;, size = 5 ) ) + facet_grid(. ~ cpp_pred) + scale_fill_manual(values = c(&quot;#1f77b4&quot;, &quot;#d62728&quot;)) + scale_shape_manual(values = c(24, 21)) + scale_y_continuous( limits = c(0.63, 3), breaks = c(1:3, log10(10^(1:4) / 2)), labels = sapply( c(10^(1:3), 10^(1:4) / 2), function(x) format(x, big.mark = &quot;,&quot;, scientific = FALSE) ) ) + annotation_logticks(sides = &quot;l&quot;) + guides(fill = &quot;none&quot;, shape = &quot;none&quot;) + theme_bw() + theme( panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.text.x = element_text(colour = &quot;black&quot;, size = 17), axis.text.y = element_text(colour = &quot;black&quot;, size = 17), strip.text = element_text(colour = &quot;black&quot;, size = 20), panel.border = element_rect(colour = &quot;black&quot;), strip.text.y = element_blank(), axis.title = element_text(colour = &quot;black&quot;, size = 22) ) + xlab(&quot;Peptide bound to mNPs (%)&quot;) + ylab(&quot;Peptide (pMol / 100K cells)&quot;) set.seed(1) pos &lt;- position_jitter(width = 0.1, seed = 16) p2_2 &lt;- ggplot(mb_cp_df[mb_cp_df$cell_group == &quot;Induced&quot;, ], aes(x = cpp_pred, y = log_cpp_true)) + geom_boxplot(aes(color = cpp_pred), outlier.color = NA, lwd = 1.5, show.legend = FALSE) + geom_beeswarm(aes(fill = cpp_pred, shape = cpp_pred), color = &quot;black&quot;, alpha = 0.7, size = 3, stroke = 0.3, cex = 2, priority = &quot;random&quot; ) + stat_compare_means( label = &quot;p.signif&quot;, size = 5, method = &quot;wilcox.test&quot;, label.x.npc = 0.2, label.y = 2.92, hide.ns = TRUE ) + stat_compare_means( label = &quot;p.format&quot;, size = 5, method = &quot;wilcox.test&quot;, label.x.npc = 0.2, label.y = 2.82, hide.ns = TRUE ) + scale_color_manual(values = c(&quot;#cae6fa&quot;, &quot;#f5b8b8&quot;)) + scale_fill_manual(values = c(&quot;#1f77b4&quot;, &quot;#d62728&quot;)) + scale_shape_manual(values = c(24, 21)) + guides(fill = guide_legend(override.aes = list(shape = c(24, 21))), shape = &quot;none&quot;) + scale_y_continuous( limits = c(0.63, 3), breaks = c(1:3, log10(10^(1:4) / 2)), labels = c(10^(1:3), 10^(1:4) / 2) ) + annotation_logticks(sides = &quot;l&quot;) + theme_bw() + theme( panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_rect(colour = &quot;black&quot;), strip.text = element_text(colour = &quot;black&quot;, size = 20), axis.text.x = element_text(colour = &quot;black&quot;, size = 18), legend.text = element_text(colour = &quot;black&quot;, size = 15), axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank(), plot.margin = unit(c(1.15, 0.15, 0.53, 0), &quot;cm&quot;), legend.position = c(0.5, 1.05), legend.direction = &quot;horizontal&quot;, legend.key = element_rect(fill = NA) ) + xlab(&quot;&quot;) + labs(fill = &quot;&quot;, color = &quot;&quot;, shape = &quot;&quot;) p2 &lt;- ggarrange(p2_1, p2_2, widths = c(2.39, 1.02)) 4.1.4 Non-melanin induced cells p3_1 &lt;- ggscatter(mb_cp_df[mb_cp_df$cell_group == &quot;Non-induced&quot;, ], x = &quot;mb_true&quot;, y = &quot;log_cpp_true&quot;, alpha = 0.7, size = 3, stroke = 0.3, color = &quot;black&quot;, fill = &quot;cpp_pred&quot;, shape = &quot;cpp_pred&quot;, add = &quot;reg.line&quot;, conf.int = TRUE, conf.int.level = 0.95, cor.coef = TRUE, add.params = list(color = &quot;grey25&quot;), cor.coeff.args = list( method = &quot;pearson&quot;, label.x = 2.2, label.y = 2.85, label.sep = &quot;\\n&quot;, size = 5 ) ) + facet_grid(. ~ cpp_pred) + scale_fill_manual(values = c(&quot;#1f77b4&quot;, &quot;#d62728&quot;)) + scale_shape_manual(values = c(24, 21)) + scale_y_continuous( limits = c(0.63, 3), breaks = c(1:3, log10(10^(1:4) / 2)), labels = sapply( c(10^(1:3), 10^(1:4) / 2), function(x) format(x, big.mark = &quot;,&quot;, scientific = FALSE) ) ) + annotation_logticks(sides = &quot;l&quot;) + guides(fill = &quot;none&quot;, shape = &quot;none&quot;) + theme_bw() + theme( panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.text.x = element_text(colour = &quot;black&quot;, size = 17), axis.text.y = element_text(colour = &quot;black&quot;, size = 17), strip.text = element_text(colour = &quot;black&quot;, size = 20), panel.border = element_rect(colour = &quot;black&quot;), strip.text.y = element_blank(), axis.title = element_text(colour = &quot;black&quot;, size = 22) ) + xlab(&quot;Peptide bound to mNPs (%)&quot;) + ylab(&quot;Peptide (pMol / 100K cells)&quot;) set.seed(1) pos &lt;- position_jitter(width = 0.1, seed = 16) p3_2 &lt;- ggplot(mb_cp_df[mb_cp_df$cell_group == &quot;Non-induced&quot;, ], aes(x = cpp_pred, y = log_cpp_true)) + geom_boxplot(aes(color = cpp_pred), outlier.color = NA, lwd = 1.5, show.legend = FALSE) + geom_beeswarm(aes(fill = cpp_pred, shape = cpp_pred), color = &quot;black&quot;, alpha = 0.7, size = 3, stroke = 0.3, cex = 2, priority = &quot;random&quot; ) + stat_compare_means( label = &quot;p.signif&quot;, size = 5, method = &quot;wilcox.test&quot;, label.x.npc = 0.2, label.y = 2.92, hide.ns = TRUE ) + stat_compare_means( label = &quot;p.format&quot;, size = 5, method = &quot;wilcox.test&quot;, label.x.npc = 0.2, label.y = 2.82, hide.ns = TRUE ) + scale_color_manual(values = c(&quot;#cae6fa&quot;, &quot;#f5b8b8&quot;)) + scale_fill_manual(values = c(&quot;#1f77b4&quot;, &quot;#d62728&quot;)) + scale_shape_manual(values = c(24, 21)) + guides(fill = guide_legend(override.aes = list(shape = c(24, 21))), shape = &quot;none&quot;) + scale_y_continuous( limits = c(0.63, 3), breaks = c(1:3, log10(10^(1:4) / 2)), labels = c(10^(1:3), 10^(1:4) / 2) ) + annotation_logticks(sides = &quot;l&quot;) + theme_bw() + theme( panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_rect(colour = &quot;black&quot;), strip.text = element_text(colour = &quot;black&quot;, size = 20), axis.text.x = element_text(colour = &quot;black&quot;, size = 18), legend.text = element_text(colour = &quot;black&quot;, size = 15), axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank(), plot.margin = unit(c(1.15, 0.15, 0.53, 0), &quot;cm&quot;), legend.position = c(0.5, 1.05), legend.direction = &quot;horizontal&quot;, legend.key = element_rect(fill = NA) ) + xlab(&quot;&quot;) + labs(fill = &quot;&quot;, color = &quot;&quot;, shape = &quot;&quot;) p3 &lt;- ggarrange(p3_1, p3_2, widths = c(2.39, 1.02)) 4.2 Overall model interpretation 4.2.1 Melanin binding (regression) # Save train and test sets in csv format load(file = &quot;./rdata/var_reduct_mb_train_test_splits.RData&quot;) for (i in 1:10) { prefix &lt;- paste0(&quot;outer_&quot;, i) exp_dir &lt;- paste0(&quot;/Users/renee/Downloads/melanin_binding/&quot;, prefix) write.csv(outer_splits[[i]]$train, file = paste0(exp_dir, &quot;/train_set.csv&quot;)) write.csv(outer_splits[[i]]$test, file = paste0(exp_dir, &quot;/test_set.csv&quot;)) } exp_dir &lt;- paste0(&quot;/Users/renee/Downloads/melanin_binding/whole_data_set&quot;) write.csv(mb_data, file = paste0(exp_dir, &quot;/train_set.csv&quot;)) h2o.init(nthreads=-1) dir_path = &#39;/Users/renee/Downloads/melanin_binding&#39; model_names = [&#39;superlearner_iter_3&#39;, &#39;superlearner_iter_2&#39;, &#39;superlearner_iter_2&#39;, &#39;superlearner_iter_3&#39;, &#39;superlearner_iter_3&#39;, &#39;superlearner_iter_3&#39;, &#39;superlearner_iter_3&#39;, &#39;superlearner_iter_3&#39;, &#39;superlearner_iter_4&#39;, &#39;superlearner_iter_3&#39;] shap_res = [] for i in range(10): print(&#39;Iter &#39; + str(i + 1) + &#39;:&#39;) train = pd.read_csv(dir_path + &#39;/outer_&#39; + str(i + 1) + &#39;/train_set.csv&#39;, index_col=0) X_train = train.iloc[:, :-1] test = pd.read_csv(dir_path + &#39;/outer_&#39; + str(i + 1) + &#39;/test_set.csv&#39;, index_col=0) X_test = test.iloc[:, :-1] model = h2o.load_model(dir_path + &#39;/outer_&#39; + str(i + 1) + &#39;/&#39; + model_names[i]) def model_predict_(data): data = pd.DataFrame(data, columns=X_train.columns) h2o_data = h2o.H2OFrame(data) res = model.predict(h2o_data) res = res.as_data_frame() return(res) np.random.seed(1) X_train_ = X_train.iloc[np.random.choice(X_train.shape[0], 100, replace=False), :] explainer = shap.KernelExplainer(model_predict_, X_train_, link=&#39;identity&#39;) shap_values = explainer.shap_values(X_test, nsamples=1000) shap_res.append(shap_values) h2o.remove_all() shap_res = pd.DataFrame(np.vstack(shap_res), columns=X_train.columns) shap_res.to_csv(&#39;./other_data/mb_shap_values_cv_data_sets.csv&#39;, index=False) load(file = &quot;./rdata/var_reduct_mb_train_test_splits.RData&quot;) data &lt;- do.call(rbind, lapply(outer_splits, function(x) x$test[-ncol(mb_data)])) data &lt;- apply(data, 2, function(x) rank(x)) data &lt;- apply(data, 2, function(x) rescale(x, to = c(0, 100))) shap_values &lt;- as.matrix(read.csv(&quot;./other_data/mb_shap_values_cv_data_sets.csv&quot;, check.names = FALSE)) shap_values &lt;- as.data.frame(rescale(shap_values, to = c(0, 100), from = range(mb_data$log_intensity))) var_diff &lt;- apply(shap_values, 2, function(x) max(x) - min(x)) var_diff &lt;- var_diff[order(-var_diff)] df &lt;- data.frame( variable = melt(shap_values)$variable, shap = melt(shap_values)$value, variable_value = melt(data)$value ) top_vars &lt;- names(var_diff)[1:20] df &lt;- df[df$variable %in% top_vars, ] df$variable &lt;- factor(df$variable, levels = rev(top_vars), labels = rev(top_vars)) p1 &lt;- ggplot(df, aes(x = shap, y = variable)) + geom_hline(yintercept = top_vars, linetype = &quot;dotted&quot;, color = &quot;grey80&quot;) + geom_vline(xintercept = 0, color = &quot;grey80&quot;, size = 1) + geom_point(aes(fill = variable_value), color = &quot;grey30&quot;, shape = 21, alpha = 0.3, size = 2, position = &quot;auto&quot;, stroke = 0.1) + scale_fill_gradient2(low = &quot;#1f77b4&quot;, mid = &quot;white&quot;, high = &quot;#d62728&quot;, midpoint = 50, breaks = c(0, 100), labels = c(&quot;Low&quot;, &quot;High&quot;)) + theme_bw() + theme( plot.margin = ggplot2::margin(1.5, 8, 1.5, -3), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank(), panel.border = element_blank(), axis.line = element_line(color = &quot;black&quot;), plot.title = element_text(hjust = 0.5, size = 20), axis.title.x = element_text(hjust = 0.5, colour = &quot;black&quot;, size = 20), axis.title.y = element_text(colour = &quot;black&quot;, size = 20), axis.text.x = element_text(colour = &quot;black&quot;, size = 17), axis.text.y = element_text(colour = &quot;black&quot;, size = 16), legend.title = element_text(size = 20), legend.text = element_text(colour = &quot;black&quot;, size = 16), legend.title.align = 0.5 ) + guides( fill = guide_colourbar(&quot;Variable\\nvalue\\n&quot;, ticks = FALSE, barheight = 10, barwidth = 0.5), color = &quot;none&quot; ) + xlab(&quot;SHAP value\\n(variable contribution to model prediction)&quot;) + ylab(&quot;&quot;) + ggtitle(&quot;Melanin binding&quot;) 4.2.2 Cell-penetration (classification) # Save train and test sets in csv format load(file = &quot;./rdata/var_reduct_cpp_train_test_splits.RData&quot;) for (i in 1:10) { prefix &lt;- paste0(&quot;outer_&quot;, i) exp_dir &lt;- paste0(&quot;/Users/renee/Downloads/cell_penetration/&quot;, prefix) write.csv(outer_splits[[i]]$train, file = paste0(exp_dir, &quot;/train_set.csv&quot;)) write.csv(outer_splits[[i]]$test, file = paste0(exp_dir, &quot;/test_set.csv&quot;)) } exp_dir &lt;- paste0(&quot;/Users/renee/Downloads/cell_penetration/whole_data_set&quot;) write.csv(cpp_data, file = paste0(exp_dir, &quot;/train_set.csv&quot;)) h2o.init(nthreads=-1) dir_path = &#39;/Users/renee/Downloads/cell_penetration&#39; model_names = [&#39;GBM_model_1669777615732_57632&#39;, &#39;GBM_model_1669777615732_225117&#39;, &#39;GBM_model_1669777615732_387368&#39;, &#39;GBM_model_1669777615732_552677&#39;, &#39;GBM_model_1669777615732_717390&#39;, &#39;GBM_model_1669777538393_67146&#39;, &#39;GBM_model_1669777538393_246060&#39;, &#39;GBM_model_1669777538393_420437&#39;, &#39;GBM_model_1669777538393_602358&#39;, &#39;GBM_model_1669777538393_782798&#39;] shap_res = [] for i in range(10): print(&#39;Iter &#39; + str(i + 1) + &#39;:&#39;) train = pd.read_csv(dir_path + &#39;/outer_&#39; + str(i + 1) + &#39;/train_set.csv&#39;, index_col=0) X_train = train.iloc[:, :-1] test = pd.read_csv(dir_path + &#39;/outer_&#39; + str(i + 1) + &#39;/test_set.csv&#39;, index_col=0) X_test = test.iloc[:, :-1] model = h2o.load_model(dir_path + &#39;/outer_&#39; + str(i + 1) + &#39;/&#39; + model_names[i]) def model_predict_(data): data = pd.DataFrame(data, columns=X_train.columns) h2o_data = h2o.H2OFrame(data) res = model.predict(h2o_data) res = res.as_data_frame().iloc[:,2] return(res) np.random.seed(1) X_train_ = X_train.iloc[np.random.choice(X_train.shape[0], 100, replace=False), :] explainer = shap.KernelExplainer(model_predict_, X_train_, link=&#39;identity&#39;) shap_values = explainer.shap_values(X_test, nsamples=1000) shap_res.append(shap_values) h2o.remove_all() shap_res = pd.DataFrame(np.vstack(shap_res), columns=X_train.columns) shap_res.to_csv(&#39;./other_data/cpp_shap_values_cv_data_sets.csv&#39;, index=False) load(file = &quot;./rdata/var_reduct_cpp_train_test_splits.RData&quot;) data &lt;- do.call(rbind, lapply(outer_splits, function(x) x$test[-ncol(cpp_data)])) data &lt;- apply(data, 2, function(x) rank(x)) data &lt;- apply(data, 2, function(x) rescale(x, to = c(0, 100))) shap_values &lt;- read.csv(&quot;./other_data/cpp_shap_values_cv_data_sets.csv&quot;, check.names = FALSE) * 100 var_diff &lt;- apply(shap_values, 2, function(x) max(x) - min(x)) var_diff &lt;- var_diff[order(-var_diff)] df &lt;- data.frame( variable = melt(shap_values)$variable, shap = melt(shap_values)$value, variable_value = melt(data)$value ) top_vars &lt;- names(var_diff)[1:11] df &lt;- df[df$variable %in% top_vars, ] df$variable &lt;- factor(df$variable, levels = rev(top_vars), labels = rev(top_vars)) p2 &lt;- ggplot(df, aes(x = shap, y = variable)) + geom_hline(yintercept = top_vars, linetype = &quot;dotted&quot;, color = &quot;grey80&quot;) + geom_vline(xintercept = 0, color = &quot;grey80&quot;, size = 1) + geom_point(aes(fill = variable_value), color = &quot;grey30&quot;, shape = 21, alpha = 0.5, size = 2, position = &quot;auto&quot;, stroke = 0.1) + scale_fill_gradient2(low = &quot;#1f77b4&quot;, mid = &quot;white&quot;, high = &quot;#d62728&quot;, midpoint = 50, breaks = c(0, 100), labels = c(&quot;Low&quot;, &quot;High&quot;)) + theme_bw() + theme( plot.margin = ggplot2::margin(1.5, 8, 1.5, -3), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank(), panel.border = element_blank(), axis.line = element_line(color = &quot;black&quot;), plot.title = element_text(hjust = 0.5, size = 20), axis.title.x = element_text(hjust = 0.5, colour = &quot;black&quot;, size = 20), axis.title.y = element_text(colour = &quot;black&quot;, size = 20), axis.text.x = element_text(colour = &quot;black&quot;, size = 17), axis.text.y = element_text(colour = &quot;black&quot;, size = 16), legend.title = element_text(size = 20), legend.text = element_text(colour = &quot;black&quot;, size = 16), legend.title.align = 0.5 ) + guides( fill = guide_colourbar(&quot;Variable\\nvalue\\n&quot;, ticks = FALSE, barheight = 10, barwidth = 0.5), color = &quot;none&quot; ) + xlab(&quot;SHAP value\\n(variable contribution to model prediction)&quot;) + ylab(&quot;&quot;) + ggtitle(&quot;Cell-penetration&quot;) 4.2.3 Toxicity (classification) # Save train and test sets in csv format load(file = &quot;./rdata/var_reduct_tx_train_test_splits.RData&quot;) for (i in 1:10) { prefix &lt;- paste0(&quot;outer_&quot;, i) exp_dir &lt;- paste0(&quot;/Users/renee/Downloads/toxicity/&quot;, prefix) write.csv(outer_splits[[i]]$train, file = paste0(exp_dir, &quot;/train_set.csv&quot;)) write.csv(outer_splits[[i]]$test, file = paste0(exp_dir, &quot;/test_set.csv&quot;)) } exp_dir &lt;- paste0(&quot;/Users/renee/Downloads/toxicity/whole_data_set&quot;) write.csv(tx_data, file = paste0(exp_dir, &quot;/train_set.csv&quot;)) h2o.init(nthreads=-1) dir_path = &#39;/Users/renee/Downloads/toxicity&#39; model_names = [&#39;superlearner_iter_4&#39;, &#39;superlearner_iter_3&#39;, &#39;superlearner_iter_3&#39;, &#39;superlearner_iter_3&#39;, &#39;superlearner_iter_3&#39;, &#39;superlearner_iter_3&#39;, &#39;superlearner_iter_3&#39;, &#39;superlearner_iter_4&#39;, &#39;superlearner_iter_3&#39;, &#39;superlearner_iter_4&#39;] shap_res = [] for i in range(10): print(&#39;Iter &#39; + str(i + 1) + &#39;:&#39;) train = pd.read_csv(dir_path + &#39;/outer_&#39; + str(i + 1) + &#39;/train_set.csv&#39;, index_col=0) X_train = train.iloc[:, :-1] test = pd.read_csv(dir_path + &#39;/outer_&#39; + str(i + 1) + &#39;/test_set.csv&#39;, index_col=0) X_test = test.iloc[:, :-1] model = h2o.load_model(dir_path + &#39;/outer_&#39; + str(i + 1) + &#39;/&#39; + model_names[i]) def model_predict_(data): data = pd.DataFrame(data, columns=X_train.columns) h2o_data = h2o.H2OFrame(data) res = model.predict(h2o_data) res = res.as_data_frame().iloc[:,2] # 0:toxic; 1:non-toxic return(res) np.random.seed(1) X_train_ = X_train.iloc[np.random.choice(X_train.shape[0], 100, replace=False), :] explainer = shap.KernelExplainer(model_predict_, X_train_, link=&#39;identity&#39;) shap_values = explainer.shap_values(X_test, nsamples=1000) shap_res.append(shap_values) h2o.remove_all() shap_res = pd.DataFrame(np.vstack(shap_res), columns=X_train.columns) shap_res.to_csv(&#39;./other_data/tx_shap_values_cv_data_sets.csv&#39;, index=False) load(file = &quot;./rdata/var_reduct_tx_train_test_splits.RData&quot;) data &lt;- do.call(rbind, lapply(outer_splits, function(x) x$test[-ncol(tx_data)])) data &lt;- apply(data, 2, function(x) rank(x)) data &lt;- apply(data, 2, function(x) rescale(x, to = c(0, 100))) shap_values &lt;- read.csv(&quot;./other_data/tx_shap_values_cv_data_sets.csv&quot;, check.names = FALSE) * 100 set.seed(12) ind &lt;- sample(1:nrow(data), 2500) data &lt;- data[ind, ] shap_values &lt;- shap_values[ind, ] var_diff &lt;- apply(shap_values, 2, function(x) max(x) - min(x)) var_diff &lt;- var_diff[order(-var_diff)] df &lt;- data.frame( variable = melt(shap_values)$variable, shap = melt(shap_values)$value, variable_value = melt(data)$value ) top_vars &lt;- names(var_diff)[1:20] df &lt;- df[df$variable %in% top_vars, ] df$variable &lt;- factor(df$variable, levels = rev(top_vars), labels = rev(top_vars)) p3 &lt;- ggplot(df, aes(x = shap, y = variable)) + geom_hline(yintercept = top_vars, linetype = &quot;dotted&quot;, color = &quot;grey80&quot;) + geom_vline(xintercept = 0, color = &quot;grey80&quot;, size = 1) + geom_point(aes(fill = variable_value), color = &quot;grey30&quot;, shape = 21, alpha = 0.3, size = 2, position = &quot;auto&quot;, stroke = 0.1) + scale_fill_gradient2(low = &quot;#1f77b4&quot;, mid = &quot;white&quot;, high = &quot;#d62728&quot;, midpoint = 50, breaks = c(0, 100), labels = c(&quot;Low&quot;, &quot;High&quot;)) + theme_bw() + theme( plot.margin = ggplot2::margin(1.5, 8, 1.5, -3), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank(), panel.border = element_blank(), axis.line = element_line(color = &quot;black&quot;), plot.title = element_text(hjust = 0.5, size = 20), axis.title.x = element_text(hjust = 0.5, colour = &quot;black&quot;, size = 20), axis.title.y = element_text(colour = &quot;black&quot;, size = 20), axis.text.x = element_text(colour = &quot;black&quot;, size = 17), axis.text.y = element_text(colour = &quot;black&quot;, size = 16), legend.title = element_text(size = 20), legend.text = element_text(colour = &quot;black&quot;, size = 16), legend.title.align = 0.5 ) + guides( fill = guide_colourbar(&quot;Variable\\nvalue\\n&quot;, ticks = FALSE, barheight = 10, barwidth = 0.5), color = &quot;none&quot; ) + xlab(&quot;SHAP value\\n(variable contribution to model prediction)&quot;) + ylab(&quot;&quot;) + ggtitle(&quot;Non-toxic&quot;) 4.3 Multifunctional peptide selection load(file = paste0(&quot;./rdata/tsne_res.RData&quot;)) load(file = paste0(&quot;./rdata/var_reduct_mb_train_test_splits.RData&quot;)) tsne_df$mb_predict &lt;- rescale(tsne_df$mb_predict, to = c(0, 100), from = range(mb_data$log_intensity)) tsne_df$mb_predict[tsne_df$mb_predict &gt; 100] &lt;- 100 tsne_df_ &lt;- tsne_df[tsne_df$source == &quot;Validation control&quot; | tsne_df$source == &quot;Validation set&quot;, ] tsne_df_$note[tsne_df_$note == &quot;TAT&quot;] &lt;- NA tsne_df_$color &lt;- factor(sapply(tsne_df_$note, function(x) if (is.na(x)) 0 else 1)) p &lt;- plot_ly(tsne_df_, x = ~in_vitro_mb_value, y = ~in_vitro_cp_value, z = ~non_tx_prob, text = ~note, color = ~color, colors = c(&quot;grey70&quot;, &quot;black&quot;), width = 700, height = 600) %&gt;% add_trace( marker = list(size = 5, line = list(color = &quot;black&quot;, width = 0.5)), type = &quot;scatter3d&quot;, mode = &quot;text+markers&quot; ) %&gt;% layout( scene = list( xaxis = list( title = &quot;Peptide bound to mNPs (%)&quot;, range = c(0, 100) ), yaxis = list( title = &quot;Peptide (pMol / 100K cells)&quot;, type = &quot;log&quot;, ticktext = list( &quot;&quot;, &quot;5&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;10&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;50&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;100&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;500&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;1,000&quot; ), tickvals = list( 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000 ), range = c(0.63, 3) ), zaxis = list( title = &quot;Non-toxic prediction&quot;, range = c(0, 100) ) ), showlegend = FALSE, autosize = TRUE ) p 4.4 Explanation of HR97 predictions 4.4.1 Melanin binding h2o.init(nthreads=-1) dir_path = &#39;/Users/renee/Downloads/melanin_binding&#39; model_name = &#39;superlearner_iter_3&#39; train = pd.read_csv(dir_path + &#39;/whole_data_set/train_set.csv&#39;, index_col=0) X_train = train.iloc[:, :-1] X_test = pd.read_csv(&#39;./other_data/HR97_ml_input.csv&#39;, index_col=0).loc[:,X_train.columns] model = h2o.load_model(dir_path + &#39;/whole_data_set/&#39; + model_name) def model_predict_(data): data = pd.DataFrame(data, columns=X_train.columns) h2o_data = h2o.H2OFrame(data) res = model.predict(h2o_data) res = res.as_data_frame() return(res) np.random.seed(1) X_train_ = X_train.iloc[np.random.choice(X_train.shape[0], 100, replace=False), :] explainer = shap.KernelExplainer(model_predict_, X_train_, link=&#39;identity&#39;) shap_res = explainer.shap_values(X_test, nsamples=1000) h2o.remove_all() exp = shap.Explanation(shap_res * 100 / 3.89063, explainer.expected_value, data=X_test.iloc[0:1, :], \\ feature_names=X_train.columns) fig, ax = plt.subplots(figsize=(10, 7)) shap.plots.waterfall(exp[0], max_display=20, show=False) yticklabels = plt.gca().get_yticklabels() for i in range(int(len(yticklabels) / 2)): label = yticklabels[i] label_ = label.get_text().split(&#39; = &#39;) if len(label_) &gt; 1: yticklabels[i].set_text(&#39; = &#39; + label_[0]) yticklabels[int(i + len(yticklabels) / 2)].set_text(&#39; = &#39;.join(label_[::-1])) plt.gca().set_yticklabels(yticklabels) fig.subplots_adjust(left=0.33, bottom=0.167, right=0.93, top=0.920) fig.set_figwidth(10) fig.set_figheight(7) plt.text(40, -4.8, &#39;SHAP value\\n(variable contribution to model prediction)&#39;, horizontalalignment=&#39;center&#39;, fontsize=14) plt.show() 4.4.2 Cell-penetration h2o.init(nthreads=-1) dir_path = &#39;/Users/renee/Downloads/cell_penetration&#39; model_name = &#39;GBM_model_1669777615732_886421&#39; train = pd.read_csv(dir_path + &#39;/whole_data_set/train_set.csv&#39;, index_col=0) X_train = train.iloc[:, :-1] X_test = pd.read_csv(&#39;./other_data/HR97_ml_input.csv&#39;, index_col=0).loc[:,X_train.columns] model = h2o.load_model(dir_path + &#39;/whole_data_set/&#39; + model_name) def model_predict_(data): data = pd.DataFrame(data, columns=X_train.columns) h2o_data = h2o.H2OFrame(data) res = model.predict(h2o_data) res = res.as_data_frame().iloc[:,2] return(res) np.random.seed(1) X_train_ = X_train.iloc[np.random.choice(X_train.shape[0], 100, replace=False), :] explainer = shap.KernelExplainer(model_predict_, X_train_, link=&#39;identity&#39;) shap_res = explainer.shap_values(X_test, nsamples=1000) h2o.remove_all() exp = shap.Explanation(shap_res * 100, explainer.expected_value * 100, data=X_test.iloc[0:1, :], \\ feature_names=X_train.columns) fig, ax = plt.subplots(figsize=(10, 5)) shap.plots.waterfall(exp[0], max_display=11, show=False) yticklabels = plt.gca().get_yticklabels() for i in range(int(len(yticklabels) / 2)): label = yticklabels[i] label_ = label.get_text().split(&#39; = &#39;) if len(label_) &gt; 1: yticklabels[i].set_text(&#39; = &#39; + label_[0]) yticklabels[int(i + len(yticklabels) / 2)].set_text(&#39; = &#39;.join(label_[::-1])) plt.gca().set_yticklabels(yticklabels) fig.subplots_adjust(left=0.32, bottom=0.18, right=0.927, top=0.925) fig.set_figwidth(10) fig.set_figheight(5) plt.text(72, -3.5, &#39;SHAP value\\n(variable contribution to model prediction)&#39;, horizontalalignment=&#39;center&#39;, fontsize=14) plt.show() 4.4.3 Toxicity h2o.init(nthreads=-1) dir_path = &#39;/Users/renee/Downloads/toxicity&#39; model_name = &#39;superlearner_iter_3&#39; train = pd.read_csv(dir_path + &#39;/whole_data_set/train_set.csv&#39;, index_col=0) X_train = train.iloc[:, :-1] X_test = pd.read_csv(&#39;./other_data/HR97_ml_input.csv&#39;, index_col=0).loc[:,X_train.columns] model = h2o.load_model(dir_path + &#39;/whole_data_set/&#39; + model_name) def model_predict_(data): data = pd.DataFrame(data, columns=X_train.columns) h2o_data = h2o.H2OFrame(data) res = model.predict(h2o_data) res = res.as_data_frame().iloc[:,2] return(res) np.random.seed(1) X_train_ = X_train.iloc[np.random.choice(X_train.shape[0], 100, replace=False), :] explainer = shap.KernelExplainer(model_predict_, X_train_, link=&#39;identity&#39;) shap_res = explainer.shap_values(X_test, nsamples=1000) h2o.remove_all() exp = shap.Explanation(shap_res * 100, explainer.expected_value * 100, data=X_test.iloc[0:1, :], \\ feature_names=X_train.columns) fig, ax = plt.subplots(figsize=(10, 5)) shap.plots.waterfall(exp[0], max_display=11, show=False) yticklabels = plt.gca().get_yticklabels() for i in range(int(len(yticklabels) / 2)): label = yticklabels[i] label_ = label.get_text().split(&#39; = &#39;) if len(label_) &gt; 1: yticklabels[i].set_text(&#39; = &#39; + label_[0]) yticklabels[int(i + len(yticklabels) / 2)].set_text(&#39; = &#39;.join(label_[::-1])) plt.gca().set_yticklabels(yticklabels) fig.subplots_adjust(left=0.395, bottom=0.18, right=0.927, top=0.925) fig.set_figwidth(10) fig.set_figheight(5) plt.text(80, -3.5, &#39;SHAP value\\n(variable contribution to model prediction)&#39;, horizontalalignment=&#39;center&#39;, fontsize=14) plt.show() 4.5 Peptide design space visualization p1 &lt;- ggplot(tsne_df, aes(x = `tSNE 1`, y = `tSNE 2`, fill = source)) + geom_point(color = &quot;grey30&quot;, shape = 21, alpha = 0.8, size = 2.5, stroke = 0.1) + geom_text_repel(aes(label = note), color = &quot;black&quot;, size = 4.5, nudge_x = -10, nudge_y = 10, point.padding = 0.1, force = 100) + scale_fill_manual(values = c(&quot;#FF380D&quot;, &quot;#0DACFF&quot;, &quot;#7134F7&quot;, &quot;grey90&quot;, &quot;#FEFA0A&quot;, &quot;grey20&quot;)) + theme_bw() + theme( plot.margin = ggplot2::margin(5, 10, 5, 10), plot.background = element_rect(fill = &quot;transparent&quot;, color = NA), panel.background = element_rect(fill = &quot;transparent&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank(), panel.border = element_blank(), axis.line = element_line(color = &quot;black&quot;), plot.title = element_text(hjust = 0.5, size = 16), axis.title.x = element_text(hjust = 0.5, colour = &quot;black&quot;, size = 14), axis.title.y = element_text(colour = &quot;black&quot;, size = 14), axis.text.x = element_text(colour = &quot;black&quot;, size = 12), axis.text.y = element_text(colour = &quot;black&quot;, size = 12), legend.title = element_blank(), legend.text = element_text(colour = &quot;black&quot;, size = 10), legend.title.align = 0.5, legend.position = &quot;top&quot;, legend.background = element_rect(fill = &quot;transparent&quot;, color = &quot;black&quot;, linetype = &quot;solid&quot;, size = 0.1) ) + guides(fill = guide_legend(nrow = 2, byrow = TRUE)) p2 &lt;- ggplot(tsne_df, aes(x = `tSNE 1`, y = `tSNE 2`, fill = mb_predict)) + geom_point(color = &quot;grey30&quot;, shape = 21, alpha = 0.8, size = 2.5, stroke = 0.1) + geom_text_repel(aes(label = note), color = &quot;black&quot;, size = 4.5, nudge_x = -10, nudge_y = 10, point.padding = 0.1, force = 100) + scale_fill_gradient2(low = &quot;#0DACFF&quot;, mid = &quot;white&quot;, high = &quot;#FF380D&quot;, midpoint = 50) + theme_bw() + theme( plot.margin = ggplot2::margin(5, 10, 5, 10), plot.background = element_rect(fill = &quot;transparent&quot;, color = NA), panel.background = element_rect(fill = &quot;transparent&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank(), panel.border = element_blank(), axis.line = element_line(color = &quot;black&quot;), plot.title = element_text(hjust = 0.5, size = 16), axis.title.x = element_text(hjust = 0.5, colour = &quot;black&quot;, size = 14), axis.title.y = element_text(colour = &quot;black&quot;, size = 14), axis.text.x = element_text(colour = &quot;black&quot;, size = 12), axis.text.y = element_text(colour = &quot;black&quot;, size = 12), legend.title = element_text(colour = &quot;black&quot;, size = 12, angle = 90), legend.text = element_text(colour = &quot;black&quot;, size = 10), legend.title.align = 0.5, legend.position = c(0.91, 0.82), legend.background = element_blank() ) + guides(fill = guide_colorbar( title = &quot;Predicted peptide \\n bound to b-mNPs (%)&quot;, title.position = &quot;left&quot; )) + ggtitle(&quot;Melanin binding prediction&quot;) p3 &lt;- ggplot(tsne_df, aes(x = `tSNE 1`, y = `tSNE 2`, fill = cpp_predict)) + geom_point(color = &quot;grey30&quot;, shape = 21, alpha = 0.8, size = 2.5, stroke = 0.1) + geom_text_repel(aes(label = note), color = &quot;black&quot;, size = 4.5, nudge_x = -10, nudge_y = 10, point.padding = 0.1, force = 100) + scale_fill_manual(values = c(&quot;#FF380D&quot;, &quot;#0DACFF&quot;)) + theme_bw() + theme( plot.margin = ggplot2::margin(5, 10, 5, 10), plot.background = element_rect(fill = &quot;transparent&quot;, color = NA), panel.background = element_rect(fill = &quot;transparent&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank(), panel.border = element_blank(), axis.line = element_line(color = &quot;black&quot;), plot.title = element_text(hjust = 0.5, size = 16), axis.title.x = element_text(hjust = 0.5, colour = &quot;black&quot;, size = 14), axis.title.y = element_text(colour = &quot;black&quot;, size = 14), axis.text.x = element_text(colour = &quot;black&quot;, size = 12), axis.text.y = element_text(colour = &quot;black&quot;, size = 12), legend.title = element_blank(), legend.text = element_text(colour = &quot;black&quot;, size = 10), legend.title.align = 0.5, legend.position = c(0.90, 0.93), legend.background = element_rect(fill = &quot;transparent&quot;, color = &quot;black&quot;, linetype = &quot;solid&quot;, size = 0.1) ) + ggtitle(&quot;Cell-penetration prediction&quot;) p4 &lt;- ggplot(tsne_df, aes(x = `tSNE 1`, y = `tSNE 2`, fill = tx_predict)) + geom_point(color = &quot;grey30&quot;, shape = 21, alpha = 0.8, size = 2.5, stroke = 0.1) + geom_text_repel(aes(label = note), color = &quot;black&quot;, size = 4.5, nudge_x = -10, nudge_y = 10, point.padding = 0.1, force = 100) + scale_fill_manual(values = c(&quot;#7d5ef7&quot;, &quot;#FF380D&quot;)) + theme_bw() + theme( plot.margin = ggplot2::margin(5, 5, 5, 5), plot.background = element_rect(fill = &quot;transparent&quot;, color = NA), panel.background = element_rect(fill = &quot;transparent&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank(), panel.border = element_blank(), axis.line = element_line(color = &quot;black&quot;), plot.title = element_text(hjust = 0.5, size = 16), axis.title.x = element_text(hjust = 0.5, colour = &quot;black&quot;, size = 14), axis.title.y = element_text(colour = &quot;black&quot;, size = 14), axis.text.x = element_text(colour = &quot;black&quot;, size = 12), axis.text.y = element_text(colour = &quot;black&quot;, size = 12), legend.title = element_blank(), legend.text = element_text(colour = &quot;black&quot;, size = 10), legend.title.align = 0.5, legend.position = c(0.90, 0.93), legend.background = element_rect(fill = &quot;transparent&quot;, color = &quot;black&quot;, linetype = &quot;solid&quot;, size = 0.1) ) + ggtitle(&quot;Non-toxic prediction&quot;) combined &lt;- plot_grid(p1, plot_grid(NULL, p2, nrow = 2, rel_heights = c(0.08, 0.85)), p3, p4, nrow = 2, rel_heights = c(0.48, 0.45), labels = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;) ) sessionInfo() ## R version 4.2.2 (2022-10-31) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur ... 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] ggrepel_0.9.1 plotly_4.10.0 cowplot_1.1.1 ggforce_0.3.4 ## [5] scales_1.2.1 reshape2_1.4.4 reticulate_1.25 ggbeeswarm_0.6.0 ## [9] ggpubr_0.4.0 ggplot2_3.3.6 h2o_3.38.0.2 ## ## loaded via a namespace (and not attached): ## [1] httr_1.4.4 sass_0.4.2 tidyr_1.2.0 viridisLite_0.4.1 ## [5] jsonlite_1.8.0 here_1.0.1 carData_3.0-5 R.utils_2.12.0 ## [9] bslib_0.4.0 assertthat_0.2.1 highr_0.9 vipor_0.4.5 ## [13] yaml_2.3.5 pillar_1.8.1 backports_1.4.1 lattice_0.20-45 ## [17] glue_1.6.2 digest_0.6.29 ggsignif_0.6.3 polyclip_1.10-0 ## [21] colorspace_2.0-3 htmltools_0.5.3 Matrix_1.5-1 R.oo_1.25.0 ## [25] plyr_1.8.7 pkgconfig_2.0.3 broom_1.0.0 bookdown_0.28 ## [29] purrr_0.3.4 tweenr_2.0.1 tibble_3.1.8 styler_1.8.0 ## [33] generics_0.1.3 farver_2.1.1 car_3.1-0 cachem_1.0.6 ## [37] withr_2.5.0 lazyeval_0.2.2 cli_3.3.0 magrittr_2.0.3 ## [41] evaluate_0.16 R.methodsS3_1.8.2 fansi_1.0.3 R.cache_0.16.0 ## [45] MASS_7.3-58.1 rstatix_0.7.0 beeswarm_0.4.0 tools_4.2.2 ## [49] data.table_1.14.2 lifecycle_1.0.1 stringr_1.4.1 munsell_0.5.0 ## [53] compiler_4.2.2 jquerylib_0.1.4 rlang_1.0.4 grid_4.2.2 ## [57] RCurl_1.98-1.8 rstudioapi_0.14 htmlwidgets_1.5.4 crosstalk_1.2.0 ## [61] bitops_1.0-7 rmarkdown_2.16 codetools_0.2-18 gtable_0.3.0 ## [65] abind_1.4-5 DBI_1.1.3 R6_2.5.1 knitr_1.40 ## [69] dplyr_1.0.9 fastmap_1.1.0 utf8_1.2.2 rprojroot_2.0.3 ## [73] stringi_1.7.8 Rcpp_1.0.9 vctrs_0.4.1 png_0.1-7 ## [77] tidyselect_1.1.2 xfun_0.32 session_info.show() ## ----- ## h2o 3.38.0.2 ## matplotlib 3.6.2 ## numpy 1.23.5 ## pandas 1.5.2 ## session_info 1.0.0 ## shap 0.41.0 ## ----- ## Python 3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 09:04:40) [Clang 14.0.6 ] ## macOS-10.16-x86_64-i386-64bit ## ----- ## Session information updated at 2023-01-20 23:42 "],["id_05_adversarial_computational_control.html", "Section 5 Adversarial computational control 5.1 General code/functions 5.2 Melanin binding (regression) 5.3 Cell-penetration (classification) 5.4 Toxicity (classification) 5.5 Overall model interpretation", " Section 5 Adversarial computational control library(h2o) library(MLmetrics) library(mltools) library(scales) library(enrichvs) library(rlist) library(stringr) library(digest) library(DT) library(reshape2) library(ggplot2) library(ggforce) library(reticulate) use_condaenv(&quot;/Users/renee/Library/r-miniconda/envs/peptide_engineering/bin/python&quot;) 5.1 General code/functions # h2o.init(nthreads=-1, max_mem_size=&#39;100G&#39;, port=54321) h2o.init(nthreads = -1) h2o.removeAll() # DeepLearning Grid 1 deeplearning_params_1 &lt;- list( activation = &quot;RectifierWithDropout&quot;, epochs = 10000, # early stopping epsilon = c(1e-6, 1e-7, 1e-8, 1e-9), hidden = list(c(50), c(200), c(500)), hidden_dropout_ratios = list(c(0.1), c(0.2), c(0.3), c(0.4), c(0.5)), input_dropout_ratio = c(0, 0.05, 0.1, 0.15, 0.2), rho = c(0.9, 0.95, 0.99) ) # DeepLearning Grid 2 deeplearning_params_2 &lt;- list( activation = &quot;RectifierWithDropout&quot;, epochs = 10000, # early stopping epsilon = c(1e-6, 1e-7, 1e-8, 1e-9), hidden = list(c(50, 50), c(200, 200), c(500, 500)), hidden_dropout_ratios = list( c(0.1, 0.1), c(0.2, 0.2), c(0.3, 0.3), c(0.4, 0.4), c(0.5, 0.5) ), input_dropout_ratio = c(0, 0.05, 0.1, 0.15, 0.2), rho = c(0.9, 0.95, 0.99) ) # DeepLearning Grid 3 deeplearning_params_3 &lt;- list( activation = &quot;RectifierWithDropout&quot;, epochs = 10000, # early stopping epsilon = c(1e-6, 1e-7, 1e-8, 1e-9), hidden = list(c(50, 50, 50), c(200, 200, 200), c(500, 500, 500)), hidden_dropout_ratios = list( c(0.1, 0.1, 0.1), c(0.2, 0.2, 0.2), c(0.3, 0.3, 0.3), c(0.4, 0.4, 0.4), c(0.5, 0.5, 0.5) ), input_dropout_ratio = c(0, 0.05, 0.1, 0.15, 0.2), rho = c(0.9, 0.95, 0.99) ) # GBM gbm_params &lt;- list( col_sample_rate = c(0.4, 0.7, 1.0), col_sample_rate_per_tree = c(0.4, 0.7, 1.0), learn_rate = 0.1, max_depth = c(3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17), min_rows = c(1, 5, 10, 15, 30, 100), min_split_improvement = c(1e-4, 1e-5), ntrees = 10000, # early stopping sample_rate = c(0.5, 0.6, 0.7, 0.8, 0.9, 1.0) ) # XGBoost xgboost_params &lt;- list( booster = c(&quot;gbtree&quot;, &quot;dart&quot;), col_sample_rate = c(0.6, 0.8, 1.0), col_sample_rate_per_tree = c(0.7, 0.8, 0.9, 1.0), max_depth = c(5, 10, 15, 20), min_rows = c(0.01, 0.1, 1.0, 3.0, 5.0, 10.0, 15.0, 20.0), ntrees = 10000, # early stopping reg_alpha = c(0.001, 0.01, 0.1, 1, 10, 100), reg_lambda = c(0.001, 0.01, 0.1, 0.5, 1), sample_rate = c(0.6, 0.8, 1.0) ) sem &lt;- function(x, na.rm = TRUE) sd(x, na.rm) / sqrt(length(na.omit(x))) statistical_testing &lt;- function(data, metric_vec, decreasing_vec, p_value_threshold = 0.05, num_entries = 10, output_path = NULL) { for (i in 1:length(metric_vec)) { metric &lt;- metric_vec[i] ranked_models &lt;- rownames(data[order(data[, paste(metric, &quot;mean&quot;)], decreasing = decreasing_vec[i]), ]) pval &lt;- c() for (j in 2:length(ranked_models)) { if (sum(is.na(data[ranked_models[j], paste(metric, &quot;CV&quot;, 1:10)]))) { pval &lt;- c(pval, NA) } else { pval &lt;- c(pval, wilcox.test(as.numeric(data[ranked_models[1], paste(metric, &quot;CV&quot;, 1:10)]), as.numeric(data[ranked_models[j], paste(metric, &quot;CV&quot;, 1:10)]), exact = FALSE )$p.value) } } adj_pval &lt;- c(NA, p.adjust(pval, method = &quot;BH&quot;, n = length(pval))) df &lt;- data.frame(adj_pval) rownames(df) &lt;- ranked_models colnames(df) &lt;- paste(metric, &quot;adj. &lt;i&gt;p&lt;/i&gt; value&quot;) data &lt;- merge(data, df, by = &quot;row.names&quot;, all = TRUE) rownames(data) &lt;- data$Row.names data$Row.names &lt;- NULL } for (i in 1:length(metric_vec)) { if (decreasing_vec[i]) { data[paste(metric_vec[i], &quot;rank&quot;)] &lt;- rank(-data[, paste(metric_vec[i], &quot;mean&quot;)], ties.method = &quot;average&quot;) } else { data[paste(metric_vec[i], &quot;rank&quot;)] &lt;- rank(data[, paste(metric_vec[i], &quot;mean&quot;)], ties.method = &quot;average&quot;) } } data[&quot;Rank sum&quot;] &lt;- rowSums(data[(ncol(data) - length(metric_vec) + 1):ncol(data)]) data &lt;- data[order(data$`Rank sum`), ] competitive &lt;- rowSums(data[paste(metric_vec, &quot;adj. &lt;i&gt;p&lt;/i&gt; value&quot;)] &lt; p_value_threshold) == 0 competitive[is.na(competitive)] &lt;- TRUE data &lt;- data[competitive, ] if (!is.null(output_path)) { data[c( paste(metric_vec, &quot;adj. &lt;i&gt;p&lt;/i&gt; value&quot;), paste(metric_vec, &quot;rank&quot;), &quot;Rank sum&quot;, melt(sapply(metric_vec, function(x) paste(x, c(&quot;mean&quot;, &quot;s.e.m.&quot;))))$value )] %&gt;% round(digits = 4) %&gt;% write.csv(., file = output_path) } data[c(paste(metric_vec, &quot;adj. &lt;i&gt;p&lt;/i&gt; value&quot;), paste(metric_vec, &quot;rank&quot;), &quot;Rank sum&quot;)] %&gt;% round(digits = 4) %&gt;% datatable(options = list(pageLength = num_entries), escape = FALSE) } 5.2 Melanin binding (regression) load(file = &quot;./rdata/var_reduct_mb_train_test_splits.RData&quot;) set.seed(32) # Shuffle the response variable of cross-validation training sets for (i in 1:10) { response &lt;- outer_splits[[i]]$train$log_intensity response &lt;- sample(response) outer_splits[[i]]$train$log_intensity &lt;- response } # Shuffle the response variable of the whole data set response &lt;- mb_data$log_intensity response &lt;- sample(response) mb_data$log_intensity &lt;- response save(mb_data, outer_splits, file = &quot;./rdata/var_reduct_mb_train_test_splits_y_shuffled.RData&quot; ) 5.2.1 Model training train_mb_models &lt;- function(train_set, exp_dir, prefix, nfolds = 10, grid_seed = 1) { tmp &lt;- as.h2o(train_set, destination_frame = prefix) y &lt;- &quot;log_intensity&quot; x &lt;- setdiff(names(tmp), y) res &lt;- as.data.frame(tmp$log_intensity) # ------------------- # base model training # ------------------- cat(&quot;Deep learning grid 1\\n&quot;) deeplearning_1 &lt;- h2o.grid( algorithm = &quot;deeplearning&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = deeplearning_params_1, stopping_rounds = 3, keep_cross_validation_models = FALSE, search_criteria = list( strategy = &quot;RandomDiscrete&quot;, max_models = 100, seed = grid_seed ), fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in deeplearning_1@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;Deep learning grid 2\\n&quot;) deeplearning_2 &lt;- h2o.grid( algorithm = &quot;deeplearning&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = deeplearning_params_2, stopping_rounds = 3, keep_cross_validation_models = FALSE, search_criteria = list( strategy = &quot;RandomDiscrete&quot;, max_models = 100, seed = grid_seed ), fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in deeplearning_2@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;Deep learning grid 3\\n&quot;) deeplearning_3 &lt;- h2o.grid( algorithm = &quot;deeplearning&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = deeplearning_params_3, stopping_rounds = 3, keep_cross_validation_models = FALSE, search_criteria = list( strategy = &quot;RandomDiscrete&quot;, max_models = 100, seed = grid_seed ), fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in deeplearning_3@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;GBM grid\\n&quot;) gbm &lt;- h2o.grid( algorithm = &quot;gbm&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = gbm_params, stopping_rounds = 3, search_criteria = list(strategy = &quot;RandomDiscrete&quot;, max_models = 300, seed = grid_seed), keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in gbm@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;GBM 5 default models\\n&quot;) gbm_1 &lt;- h2o.gbm( model_id = &quot;GBM_1&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 6, min_rows = 1, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_2 &lt;- h2o.gbm( model_id = &quot;GBM_2&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 7, min_rows = 10, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_3 &lt;- h2o.gbm( model_id = &quot;GBM_3&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 8, min_rows = 10, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_4 &lt;- h2o.gbm( model_id = &quot;GBM_4&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 10, min_rows = 10, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_5 &lt;- h2o.gbm( model_id = &quot;GBM_5&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 15, min_rows = 100, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) for (model_id in paste0(&quot;GBM_&quot;, 1:5)) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;XGBoost grid\\n&quot;) xgboost &lt;- h2o.grid( algorithm = &quot;xgboost&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = xgboost_params, stopping_rounds = 3, search_criteria = list(strategy = &quot;RandomDiscrete&quot;, max_models = 300, seed = grid_seed), keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in xgboost@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;XGBoost 3 default models\\n&quot;) xgboost_1 &lt;- h2o.xgboost( model_id = &quot;XGBoost_1&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, booster = &quot;gbtree&quot;, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, max_depth = 10, min_rows = 5, ntrees = 10000, reg_alpha = 0, reg_lambda = 1, sample_rate = 0.6, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) xgboost_2 &lt;- h2o.xgboost( model_id = &quot;XGBoost_2&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, booster = &quot;gbtree&quot;, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, max_depth = 20, min_rows = 10, ntrees = 10000, reg_alpha = 0, reg_lambda = 1, sample_rate = 0.6, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) xgboost_3 &lt;- h2o.xgboost( model_id = &quot;XGBoost_3&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, booster = &quot;gbtree&quot;, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, max_depth = 5, min_rows = 3, ntrees = 10000, reg_alpha = 0, reg_lambda = 1, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) for (model_id in paste0(&quot;XGBoost_&quot;, 1:3)) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;GLM\\n&quot;) glm &lt;- h2o.glm( model_id = &quot;GLM&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, alpha = c(0.0, 0.2, 0.4, 0.6, 0.8, 1.0), keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;GLM&quot;), path = exp_dir, force = TRUE) cat(&quot;DRF\\n&quot;) drf &lt;- h2o.randomForest( model_id = &quot;DRF&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, ntrees = 10000, score_tree_interval = 5, stopping_rounds = 3, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;DRF&quot;), path = exp_dir, force = TRUE) cat(&quot;XRT\\n&quot;) xrt &lt;- h2o.randomForest( model_id = &quot;XRT&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, ntrees = 10000, histogram_type = &quot;Random&quot;, score_tree_interval = 5, stopping_rounds = 3, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;XRT&quot;), path = exp_dir, force = TRUE) # ----------------------- # get holdout predictions # ----------------------- base_models &lt;- as.list(c( unlist(deeplearning_1@model_ids), unlist(deeplearning_2@model_ids), unlist(deeplearning_3@model_ids), unlist(gbm@model_ids), paste0(&quot;GBM_&quot;, 1:5), unlist(xgboost@model_ids), paste0(&quot;XGBoost_&quot;, 1:3), &quot;GLM&quot;, &quot;DRF&quot;, &quot;XRT&quot; )) for (model_id in base_models) { res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id)), col.names = model_id )) } # ---------------------- # super learner training # ---------------------- sl_iter &lt;- 0 cat(paste0(&quot;Super learner iteration &quot;, sl_iter, &quot; (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = paste0(&quot;superlearner_iter_&quot;, sl_iter), training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list(standardize = TRUE, keep_cross_validation_predictions = TRUE) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(paste0(&quot;superlearner_iter_&quot;, sl_iter)), path = exp_dir, force = TRUE) model_id &lt;- paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter) tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id)), col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) # ---------------------------------- # super learner base model reduction # ---------------------------------- while (TRUE) { meta &lt;- h2o.getModel(paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter, &quot;_cv_1&quot;)) names &lt;- meta@model$coefficients_table[, &quot;names&quot;] coeffs &lt;- (meta@model$coefficients_table[, &quot;standardized_coefficients&quot;] &gt; 0) for (j in 2:nfolds) { meta &lt;- h2o.getModel(paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter, &quot;_cv_&quot;, j)) names &lt;- meta@model$coefficients_table[, &quot;names&quot;] coeffs &lt;- coeffs + (meta@model$coefficients_table[, &quot;standardized_coefficients&quot;] &gt; 0) } base_models_ &lt;- as.list(names[coeffs &gt;= ceiling(nfolds / 2) &amp; names != &quot;Intercept&quot;]) if (length(base_models_) == 0) { cat(&quot;No base models passing the threshold\\n\\n&quot;) break } if (sum(base_models %in% base_models_) == length(base_models)) { cat(&quot;No further reduction of base models\\n\\n&quot;) break } sl_iter &lt;- sl_iter + 1 base_models &lt;- base_models_ cat(paste0(&quot;Super learner iteration &quot;, sl_iter, &quot; (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = paste0(&quot;superlearner_iter_&quot;, sl_iter), training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list(standardize = TRUE, keep_cross_validation_predictions = TRUE) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(paste0(&quot;superlearner_iter_&quot;, sl_iter)), path = exp_dir, force = TRUE) model_id &lt;- paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter) tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id)), col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) } # ----------------------------------------- # super learner for homogeneous base models # ----------------------------------------- # DeepLearning base_models &lt;- as.list(c( unlist(deeplearning_1@model_ids), unlist(deeplearning_2@model_ids), unlist(deeplearning_3@model_ids) )) cat(paste0(&quot;Super learner deep learning (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = &quot;superlearner_deeplearning&quot;, training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list(standardize = TRUE, keep_cross_validation_predictions = TRUE) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;superlearner_deeplearning&quot;), path = exp_dir, force = TRUE) model_id &lt;- &quot;metalearner_glm_superlearner_deeplearning&quot; tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id)), col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) # GBM base_models &lt;- as.list(c(unlist(gbm@model_ids), paste0(&quot;GBM_&quot;, 1:5))) cat(paste0(&quot;Super learner GBM (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = &quot;superlearner_gbm&quot;, training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list(standardize = TRUE, keep_cross_validation_predictions = TRUE) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;superlearner_gbm&quot;), path = exp_dir, force = TRUE) model_id &lt;- &quot;metalearner_glm_superlearner_gbm&quot; tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id)), col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) # XGBoost base_models &lt;- as.list(c(unlist(xgboost@model_ids), paste0(&quot;XGBoost_&quot;, 1:3))) cat(paste0(&quot;Super learner XGBoost (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = &quot;superlearner_xgboost&quot;, training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list(standardize = TRUE, keep_cross_validation_predictions = TRUE) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;superlearner_xgboost&quot;), path = exp_dir, force = TRUE) model_id &lt;- &quot;metalearner_glm_superlearner_xgboost&quot; tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id)), col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) write.csv(res, file = paste0(exp_dir, &quot;/cv_holdout_predictions.csv&quot;), row.names = FALSE) cat(&quot;\\n\\n&quot;) h2o.removeAll() } 5.2.2 Inner cross-validation load(file = &quot;./rdata/var_reduct_mb_train_test_splits_y_shuffled.RData&quot;) for (i in 1:10) { cat(paste0(&quot;Outer training set &quot;, i, &quot;\\n&quot;)) prefix &lt;- paste0(&quot;outer_&quot;, i) exp_dir &lt;- paste0(&quot;/Users/renee/Downloads/melanin_binding_adversarial/&quot;, prefix) dir.create(exp_dir) train_mb_models(train_set = outer_splits[[i]][[&quot;train&quot;]], exp_dir = exp_dir, prefix = prefix) } # Keep track of grid models dl_grid &lt;- list() gbm_grid &lt;- list() xgboost_grid &lt;- list() dl_grid_params &lt;- list() gbm_grid_params &lt;- list() xgboost_grid_params &lt;- list() for (i in 1:10) { cat(paste0(&quot;Outer training set &quot;, i, &quot;\\n&quot;)) prefix &lt;- paste0(&quot;outer_&quot;, i) dir &lt;- paste0(&quot;/Users/renee/Downloads/melanin_binding_adversarial/&quot;, prefix) files &lt;- list.files(dir) # Deep learning dl &lt;- files[str_detect(files, &quot;DeepLearning_model&quot;)] for (m in dl) { model &lt;- h2o.loadModel(paste0(dir, &quot;/&quot;, m)) hs &lt;- sha1(paste(c( model@allparameters$epsilon, model@allparameters$hidden, model@allparameters$hidden_dropout_ratios, model@allparameters$input_dropout_ratio, model@allparameters$rho ), collapse = &quot; &quot;)) if (hs %in% names(dl_grid)) { dl_grid[[hs]] &lt;- c(dl_grid[[hs]], m) } else { dl_grid[[hs]] &lt;- c(m) dl_grid_params &lt;- list.append( dl_grid_params, c( &quot;epsilon&quot; = model@allparameters$epsilon, &quot;hidden&quot; = paste0(&quot;[&quot;, paste(model@allparameters$hidden, collapse = &quot;,&quot;), &quot;]&quot;), &quot;hidden_dropout_ratios&quot; = paste0( &quot;[&quot;, paste(model@allparameters$hidden_dropout_ratios, collapse = &quot;,&quot; ), &quot;]&quot; ), &quot;input_dropout_ratio&quot; = model@allparameters$input_dropout_ratio, &quot;rho&quot; = model@allparameters$rho ) ) } } h2o.removeAll() # GBM gbm &lt;- files[str_detect(files, &quot;GBM_model&quot;)] for (m in gbm) { model &lt;- h2o.loadModel(paste0(dir, &quot;/&quot;, m)) hs &lt;- sha1(paste(c( model@allparameters$col_sample_rate, model@allparameters$col_sample_rate_per_tree, model@allparameters$max_depth, model@allparameters$min_rows, model@allparameters$min_split_improvement, model@allparameters$sample_rate ), collapse = &quot; &quot;)) if (hs %in% names(gbm_grid)) { gbm_grid[[hs]] &lt;- c(gbm_grid[[hs]], m) } else { gbm_grid[[hs]] &lt;- c(m) gbm_grid_params &lt;- list.append( gbm_grid_params, c( &quot;col_sample_rate&quot; = model@allparameters$col_sample_rate, &quot;col_sample_rate_per_tree&quot; = model@allparameters$col_sample_rate_per_tree, &quot;max_depth&quot; = model@allparameters$max_depth, &quot;min_rows&quot; = model@allparameters$min_rows, &quot;min_split_improvement&quot; = model@allparameters$min_split_improvement, &quot;sample_rate&quot; = model@allparameters$sample_rate ) ) } } h2o.removeAll() # XGBoost xgboost &lt;- files[str_detect(files, &quot;XGBoost_model&quot;)] for (m in xgboost) { model &lt;- h2o.loadModel(paste0(dir, &quot;/&quot;, m)) hs &lt;- sha1(paste(c( model@allparameters$booster, model@allparameters$col_sample_rate, model@allparameters$col_sample_rate_per_tree, model@allparameters$max_depth, model@allparameters$min_rows, model@allparameters$reg_alpha, model@allparameters$reg_lambda, model@allparameters$sample_rate ), collapse = &quot; &quot;)) if (hs %in% names(xgboost_grid)) { xgboost_grid[[hs]] &lt;- c(xgboost_grid[[hs]], m) } else { xgboost_grid[[hs]] &lt;- c(m) xgboost_grid_params &lt;- list.append( xgboost_grid_params, c( &quot;booster&quot; = model@allparameters$booster, &quot;col_sample_rate&quot; = model@allparameters$col_sample_rate, &quot;col_sample_rate_per_tree&quot; = model@allparameters$col_sample_rate_per_tree, &quot;max_depth&quot; = model@allparameters$max_depth, &quot;min_rows&quot; = model@allparameters$min_rows, &quot;reg_alpha&quot; = model@allparameters$reg_alpha, &quot;reg_lambda&quot; = model@allparameters$reg_lambda, &quot;sample_rate&quot; = model@allparameters$sample_rate ) ) } } h2o.removeAll() } dl_grid_params &lt;- as.data.frame(t(data.frame(dl_grid_params))) rownames(dl_grid_params) &lt;- paste(&quot;Neural network grid model&quot;, 1:nrow(dl_grid_params)) gbm_grid_params &lt;- as.data.frame(t(data.frame(gbm_grid_params))) rownames(gbm_grid_params) &lt;- paste(&quot;GBM grid model&quot;, 1:nrow(gbm_grid_params)) xgboost_grid_params &lt;- as.data.frame(t(data.frame(xgboost_grid_params))) rownames(xgboost_grid_params) &lt;- paste(&quot;XGBoost grid model&quot;, 1:nrow(xgboost_grid_params)) write.csv(dl_grid_params, &quot;./other_data/melanin_binding_adversarial/neural_network_grid_params.csv&quot;) write.csv(gbm_grid_params, &quot;./other_data/melanin_binding_adversarial/gbm_grid_params.csv&quot;) write.csv(xgboost_grid_params, &quot;./other_data/melanin_binding_adversarial/xgboost_grid_params.csv&quot;) save(dl_grid, gbm_grid, xgboost_grid, file = &quot;./rdata/model_training_grid_models_mb_ad.RData&quot;) 5.2.3 Model evaluation model_evaluation &lt;- function(holdout_pred, fold_asign, grid_name, grid_meta = NULL) { load(file = &quot;./rdata/var_reduct_mb_train_test_splits_y_shuffled.RData&quot;) res_ &lt;- list() model_num &lt;- c() if (startsWith(grid_name, &quot;Super learner&quot;)) { if (grid_name == &quot;Super learner all models&quot;) { m &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_iter_0&quot;)] } else if (grid_name == &quot;Super learner final&quot;) { sl_models &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_iter&quot;)] m &lt;- sl_models[length(sl_models)] } else if (grid_name == &quot;Super learner neural network&quot;) { m &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_deeplearning&quot;)] } else if (grid_name == &quot;Super learner GBM&quot;) { m &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_gbm&quot;)] } else if (grid_name == &quot;Super learner XGBoost&quot;) { m &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_xgboost&quot;)] } mae &lt;- c() rmse &lt;- c() R2 &lt;- c() for (k in 1:10) { y_true &lt;- holdout_pred[fold_assign == k, &quot;log_intensity&quot;] y_pred &lt;- holdout_pred[fold_assign == k, m] mae &lt;- c(mae, MAE(y_pred = y_pred, y_true = y_true)) rmse &lt;- c(rmse, RMSE(y_pred = y_pred, y_true = y_true)) R2 &lt;- c(R2, R2_Score(y_pred = y_pred, y_true = y_true)) } # Re-scale to 0-100 mae &lt;- rescale(mae, to = c(0, 100), from = range(mb_data$log_intensity)) rmse &lt;- rescale(rmse, to = c(0, 100), from = range(mb_data$log_intensity)) res_ &lt;- list.append(res_, c( mean(mae, na.rm = TRUE), sem(mae), mean(rmse, na.rm = TRUE), sem(rmse), mean(R2, na.rm = TRUE), sem(R2), mae, rmse, R2 )) } else { for (j in 1:length(grid_meta)) { g &lt;- grid_meta[[j]] m &lt;- intersect(colnames(holdout_pred), g) if (length(m) == 1) { model_num &lt;- c(model_num, j) mae &lt;- c() rmse &lt;- c() R2 &lt;- c() for (k in 1:10) { y_true &lt;- holdout_pred[fold_assign == k, &quot;log_intensity&quot;] y_pred &lt;- holdout_pred[fold_assign == k, m] mae &lt;- c(mae, MAE(y_pred = y_pred, y_true = y_true)) rmse &lt;- c(rmse, RMSE(y_pred = y_pred, y_true = y_true)) R2 &lt;- c(R2, R2_Score(y_pred = y_pred, y_true = y_true)) } # Re-scale to 0-100 mae &lt;- rescale(mae, to = c(0, 100), from = range(mb_data$log_intensity)) rmse &lt;- rescale(rmse, to = c(0, 100), from = range(mb_data$log_intensity)) res_ &lt;- list.append(res_, c( mean(mae, na.rm = TRUE), sem(mae), mean(rmse, na.rm = TRUE), sem(rmse), mean(R2, na.rm = TRUE), sem(R2), mae, rmse, R2 )) } } } res &lt;- as.data.frame(t(data.frame(res_))) colnames(res) &lt;- c( &quot;Norm. MAE mean&quot;, &quot;Norm. MAE s.e.m.&quot;, &quot;Norm. RMSE mean&quot;, &quot;Norm. RMSE s.e.m.&quot;, &quot;R^2 mean&quot;, &quot;R^2 s.e.m.&quot;, paste(&quot;Norm. MAE CV&quot;, 1:10), paste(&quot;Norm. RMSE CV&quot;, 1:10), paste(&quot;R^2 CV&quot;, 1:10) ) if (nrow(res) == 1) { rownames(res) &lt;- grid_name } else { rownames(res) &lt;- paste(grid_name, model_num) } return(res) } 5.2.4 Inner loop model selection load(file = &quot;./rdata/model_training_grid_models_mb_ad.RData&quot;) for (i in 1:10) { holdout_pred &lt;- read.csv(paste0(&quot;./other_data/melanin_binding_adversarial/outer_&quot;, i, &quot;/cv_holdout_predictions.csv&quot;)) fold_assign &lt;- rep(1:10, ceiling(nrow(holdout_pred) / 10))[1:nrow(holdout_pred)] data_ &lt;- list() # Deep learning grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Neural network grid model&quot;, grid_meta = dl_grid )) # GBM grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;GBM grid model&quot;, grid_meta = gbm_grid )) # GBM default models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;GBM model&quot;, grid_meta = as.list(paste0(&quot;GBM_&quot;, 1:5)) )) # XGBoost grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XGBoost grid model&quot;, grid_meta = xgboost_grid )) # XGBoost default models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XGBoost model&quot;, grid_meta = as.list(paste0(&quot;XGBoost_&quot;, 1:3)) )) # GLM data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;GLM model&quot;, grid_meta = list(&quot;GLM&quot;))) # DRF data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;DRF model&quot;, grid_meta = list(&quot;DRF&quot;))) # XRT data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XRT model&quot;, grid_meta = list(&quot;XRT&quot;))) # Super learner all data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner all models&quot;)) # Super learner final data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner final&quot;)) # Super learner deep learning data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner neural network&quot;)) # Super learner GBM data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner GBM&quot;)) # Super learner XGBoost data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner XGBoost&quot;)) data &lt;- do.call(rbind, data_) write.csv(data, paste0(&quot;./other_data/melanin_binding_adversarial/outer_&quot;, i, &quot;/cv_res.csv&quot;)) } 5.2.4.1 CV 1 5.2.4.2 CV 2 5.2.4.3 CV 3 5.2.4.4 CV 4 5.2.4.5 CV 5 5.2.4.6 CV 6 5.2.4.7 CV 7 5.2.4.8 CV 8 5.2.4.9 CV 9 5.2.4.10 CV 10 5.2.5 Final evaluation load(file = &quot;./rdata/var_reduct_mb_train_test_splits_y_shuffled.RData&quot;) load(file = &quot;./rdata/model_training_grid_models_mb_ad.RData&quot;) dir &lt;- &quot;/Users/renee/Downloads/melanin_binding_adversarial&quot; selected_models &lt;- c( &quot;Neural network grid model 113&quot;, &quot;Neural network grid model 187&quot;, &quot;Neural network grid model 145&quot;, &quot;Neural network grid model 215&quot;, &quot;Super learner final&quot;, &quot;Neural network grid model 188&quot;, &quot;Neural network grid model 176&quot;, &quot;Neural network grid model 201&quot;, &quot;Neural network grid model 139&quot;, &quot;Neural network grid model 160&quot; ) mae &lt;- c() rmse &lt;- c() R2 &lt;- c() for (i in 1:10) { holdout_pred &lt;- read.csv(paste0(&quot;./other_data/melanin_binding_adversarial/outer_&quot;, i, &quot;/cv_holdout_predictions.csv&quot;)) if (startsWith(selected_models[i], &quot;Neural network grid model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) m &lt;- intersect(colnames(holdout_pred), dl_grid[[grid_num]]) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, m)) } else if (startsWith(selected_models[i], &quot;GBM grid model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) m &lt;- intersect(colnames(holdout_pred), gbm_grid[[grid_num]]) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, m)) } else if (startsWith(selected_models[i], &quot;GBM model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/GBM_&quot;, grid_num)) } else if (startsWith(selected_models[i], &quot;XGBoost grid model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) m &lt;- intersect(colnames(holdout_pred), xgboost_grid[[grid_num]]) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, m)) } else if (startsWith(selected_models[i], &quot;XGBoost model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/XGBoost_&quot;, grid_num)) } else if (selected_models[i] == &quot;Super learner all models&quot;) { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/superlearner_iter_0&quot;)) } else if (selected_models[i] == &quot;Super learner final&quot;) { files &lt;- list.files(paste0(dir, &quot;/outer_&quot;, i)) files &lt;- files[startsWith(files, &quot;superlearner_iter&quot;)] model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, files[length(files)])) } else if (selected_models[i] == &quot;Super learner neural network&quot;) { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/superlearner_deeplearning&quot;)) } else if (selected_models[i] == &quot;Super learner GBM&quot;) { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/superlearner_gbm&quot;)) } else if (selected_models[i] == &quot;Super learner XGBoost&quot;) { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/superlearner_xgboost&quot;)) } else { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, unlist(strsplit(selected_models[i], &quot; &quot;))[1])) } tmp &lt;- as.h2o(subset(outer_splits[[i]][[&quot;test&quot;]], select = -log_intensity)) y_true &lt;- outer_splits[[i]][[&quot;test&quot;]]$log_intensity y_pred &lt;- as.data.frame(as.data.frame(h2o.predict(model, tmp)))[, 1] mae &lt;- c(mae, MAE(y_pred = y_pred, y_true = y_true)) rmse &lt;- c(rmse, RMSE(y_pred = y_pred, y_true = y_true)) R2 &lt;- c(R2, R2_Score(y_pred = y_pred, y_true = y_true)) } # Re-scale to 0-100 mae &lt;- rescale(mae, to = c(0, 100), from = range(mb_data$log_intensity)) rmse &lt;- rescale(rmse, to = c(0, 100), from = range(mb_data$log_intensity)) h2o.removeAll() save(mae, rmse, R2, file = &quot;./rdata/final_evaluation_mb_ad.RData&quot;) load(file = &quot;./rdata/final_evaluation_mb_ad.RData&quot;) data.frame( Metric = c(&quot;Norm. MAE&quot;, &quot;Norm. RMSE&quot;, &quot;R&lt;sup&gt;2&lt;/sup&gt;&quot;), `Mean ± s.e.m.` = c( paste0(round(mean(mae), 3), &quot; ± &quot;, round(sem(mae), 3)), paste0(round(mean(rmse), 3), &quot; ± &quot;, round(sem(rmse), 3)), paste0(round(mean(R2), 3), &quot; ± &quot;, round(sem(R2), 3)) ), check.names = FALSE ) %&gt;% datatable(escape = FALSE, rownames = FALSE) 5.3 Cell-penetration (classification) load(file = &quot;./rdata/var_reduct_cpp_train_test_splits.RData&quot;) set.seed(32) # Shuffle the response variable of cross-validation training sets for (i in 1:10) { response &lt;- outer_splits[[i]]$train$category response &lt;- sample(response) outer_splits[[i]]$train$category &lt;- response } # Shuffle the response variable of the whole data set response &lt;- cpp_data$category response &lt;- sample(response) cpp_data$category &lt;- response save(cpp_data, outer_splits, file = &quot;./rdata/var_reduct_cpp_train_test_splits_y_shuffled.RData&quot; ) 5.3.1 Model training train_cpp_models &lt;- function(train_set, exp_dir, prefix, nfolds = 10, grid_seed = 1) { tmp &lt;- as.h2o(train_set, destination_frame = prefix) tmp[&quot;category&quot;] &lt;- as.factor(tmp[&quot;category&quot;]) y &lt;- &quot;category&quot; x &lt;- setdiff(names(tmp), y) res &lt;- as.data.frame(tmp$category) samp_factors &lt;- as.vector(mean(table(train_set$category)) / table(train_set$category)) # ------------------- # base model training # ------------------- cat(&quot;Deep learning grid 1\\n&quot;) deeplearning_1 &lt;- h2o.grid( algorithm = &quot;deeplearning&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = deeplearning_params_1, stopping_rounds = 3, balance_classes = TRUE, class_sampling_factors = samp_factors, search_criteria = list( strategy = &quot;RandomDiscrete&quot;, max_models = 100, seed = grid_seed ), keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in deeplearning_1@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;GBM grid\\n&quot;) gbm &lt;- h2o.grid( algorithm = &quot;gbm&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = gbm_params, stopping_rounds = 3, balance_classes = TRUE, class_sampling_factors = samp_factors, search_criteria = list(strategy = &quot;RandomDiscrete&quot;, max_models = 100, seed = grid_seed), keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in gbm@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;GBM 5 default models\\n&quot;) gbm_1 &lt;- h2o.gbm( model_id = &quot;GBM_1&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, balance_classes = TRUE, class_sampling_factors = samp_factors, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 6, min_rows = 1, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_2 &lt;- h2o.gbm( model_id = &quot;GBM_2&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, balance_classes = TRUE, class_sampling_factors = samp_factors, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 7, min_rows = 10, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_3 &lt;- h2o.gbm( model_id = &quot;GBM_3&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, balance_classes = TRUE, class_sampling_factors = samp_factors, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 8, min_rows = 10, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_4 &lt;- h2o.gbm( model_id = &quot;GBM_4&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, balance_classes = TRUE, class_sampling_factors = samp_factors, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 10, min_rows = 10, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_5 &lt;- h2o.gbm( model_id = &quot;GBM_5&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, balance_classes = TRUE, class_sampling_factors = samp_factors, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 15, min_rows = 100, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) for (model_id in paste0(&quot;GBM_&quot;, 1:5)) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;XGBoost grid\\n&quot;) xgboost &lt;- h2o.grid( algorithm = &quot;xgboost&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = xgboost_params, stopping_rounds = 3, search_criteria = list(strategy = &quot;RandomDiscrete&quot;, max_models = 100, seed = grid_seed), keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in xgboost@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;XGBoost 3 default models\\n&quot;) xgboost_1 &lt;- h2o.xgboost( model_id = &quot;XGBoost_1&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, booster = &quot;gbtree&quot;, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, max_depth = 10, min_rows = 5, ntrees = 10000, reg_alpha = 0, reg_lambda = 1, sample_rate = 0.6, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) xgboost_2 &lt;- h2o.xgboost( model_id = &quot;XGBoost_2&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, booster = &quot;gbtree&quot;, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, max_depth = 20, min_rows = 10, ntrees = 10000, reg_alpha = 0, reg_lambda = 1, sample_rate = 0.6, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) xgboost_3 &lt;- h2o.xgboost( model_id = &quot;XGBoost_3&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, booster = &quot;gbtree&quot;, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, max_depth = 5, min_rows = 3, ntrees = 10000, reg_alpha = 0, reg_lambda = 1, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) for (model_id in paste0(&quot;XGBoost_&quot;, 1:3)) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;DRF\\n&quot;) drf &lt;- h2o.randomForest( model_id = &quot;DRF&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, ntrees = 10000, score_tree_interval = 5, stopping_rounds = 3, balance_classes = TRUE, class_sampling_factors = samp_factors, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;DRF&quot;), path = exp_dir, force = TRUE) cat(&quot;XRT\\n&quot;) xrt &lt;- h2o.randomForest( model_id = &quot;XRT&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, ntrees = 10000, histogram_type = &quot;Random&quot;, score_tree_interval = 5, stopping_rounds = 3, balance_classes = TRUE, class_sampling_factors = samp_factors, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;XRT&quot;), path = exp_dir, force = TRUE) # ----------------------- # get holdout predictions # ----------------------- base_models &lt;- as.list(c( unlist(deeplearning_1@model_ids), unlist(gbm@model_ids), paste0(&quot;GBM_&quot;, 1:5), unlist(xgboost@model_ids), paste0(&quot;XGBoost_&quot;, 1:3), &quot;DRF&quot;, &quot;XRT&quot; )) for (model_id in base_models) { res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[&quot;penetrating&quot;], col.names = model_id )) } # ---------------------- # super learner training # ---------------------- sl_iter &lt;- 0 cat(paste0(&quot;Super learner iteration &quot;, sl_iter, &quot; (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = paste0(&quot;superlearner_iter_&quot;, sl_iter), training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list( standardize = TRUE, keep_cross_validation_predictions = TRUE, balance_classes = TRUE, class_sampling_factors = samp_factors ) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(paste0(&quot;superlearner_iter_&quot;, sl_iter)), path = exp_dir, force = TRUE) model_id &lt;- paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter) tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[&quot;penetrating&quot;], col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) # ---------------------------------- # super learner base model reduction # ---------------------------------- while (TRUE) { meta &lt;- h2o.getModel(paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter, &quot;_cv_1&quot;)) names &lt;- meta@model$coefficients_table[, &quot;names&quot;] coeffs &lt;- (meta@model$coefficients_table[, &quot;standardized_coefficients&quot;] &gt; 0) for (j in 2:nfolds) { meta &lt;- h2o.getModel(paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter, &quot;_cv_&quot;, j)) names &lt;- meta@model$coefficients_table[, &quot;names&quot;] coeffs &lt;- coeffs + (meta@model$coefficients_table[, &quot;standardized_coefficients&quot;] &gt; 0) } base_models_ &lt;- as.list(names[coeffs &gt;= ceiling(nfolds / 2) &amp; names != &quot;Intercept&quot;]) if (length(base_models_) == 0) { cat(&quot;No base models passing the threshold\\n\\n&quot;) break } if (sum(base_models %in% base_models_) == length(base_models)) { cat(&quot;No further reduction of base models\\n\\n&quot;) break } sl_iter &lt;- sl_iter + 1 base_models &lt;- base_models_ cat(paste0(&quot;Super learner iteration &quot;, sl_iter, &quot; (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = paste0(&quot;superlearner_iter_&quot;, sl_iter), training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list( standardize = TRUE, keep_cross_validation_predictions = TRUE, balance_classes = TRUE, class_sampling_factors = samp_factors ) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(paste0(&quot;superlearner_iter_&quot;, sl_iter)), path = exp_dir, force = TRUE) model_id &lt;- paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter) tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[&quot;penetrating&quot;], col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) } # ----------------------------------------- # super learner for homogeneous base models # ----------------------------------------- # DeepLearning base_models &lt;- deeplearning_1@model_ids cat(paste0(&quot;Super learner deep learning (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = &quot;superlearner_deeplearning&quot;, training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list( standardize = TRUE, keep_cross_validation_predictions = TRUE, balance_classes = TRUE, class_sampling_factors = samp_factors ) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;superlearner_deeplearning&quot;), path = exp_dir, force = TRUE) model_id &lt;- &quot;metalearner_glm_superlearner_deeplearning&quot; tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[&quot;penetrating&quot;], col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) # GBM base_models &lt;- as.list(c(unlist(gbm@model_ids), paste0(&quot;GBM_&quot;, 1:5))) cat(paste0(&quot;Super learner GBM (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = &quot;superlearner_gbm&quot;, training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list( standardize = TRUE, keep_cross_validation_predictions = TRUE, balance_classes = TRUE, class_sampling_factors = samp_factors ) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;superlearner_gbm&quot;), path = exp_dir, force = TRUE) model_id &lt;- &quot;metalearner_glm_superlearner_gbm&quot; tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[&quot;penetrating&quot;], col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) # XGBoost base_models &lt;- as.list(c(unlist(xgboost@model_ids), paste0(&quot;XGBoost_&quot;, 1:3))) cat(paste0(&quot;Super learner XGBoost (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = &quot;superlearner_xgboost&quot;, training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list( standardize = TRUE, keep_cross_validation_predictions = TRUE, balance_classes = TRUE, class_sampling_factors = samp_factors ) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;superlearner_xgboost&quot;), path = exp_dir, force = TRUE) model_id &lt;- &quot;metalearner_glm_superlearner_xgboost&quot; tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[&quot;penetrating&quot;], col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) write.csv(res, file = paste0(exp_dir, &quot;/cv_holdout_predictions.csv&quot;), row.names = FALSE) cat(&quot;\\n\\n&quot;) h2o.removeAll() } 5.3.2 Inner cross-validation load(file = &quot;./rdata/var_reduct_cpp_train_test_splits_y_shuffled.RData&quot;) for (i in 1:10) { cat(paste0(&quot;Outer training set &quot;, i, &quot;\\n&quot;)) prefix &lt;- paste0(&quot;outer_&quot;, i) exp_dir &lt;- paste0(&quot;/Users/renee/Downloads/cell_penetration_adversarial/&quot;, prefix) dir.create(exp_dir) train_cpp_models(train_set = outer_splits[[i]][[&quot;train&quot;]], exp_dir = exp_dir, prefix = prefix) } # Keep track of grid models dl_grid &lt;- list() gbm_grid &lt;- list() xgboost_grid &lt;- list() dl_grid_params &lt;- list() gbm_grid_params &lt;- list() xgboost_grid_params &lt;- list() for (i in 1:10) { cat(paste0(&quot;Outer training set &quot;, i, &quot;\\n&quot;)) prefix &lt;- paste0(&quot;outer_&quot;, i) dir &lt;- paste0(&quot;/Users/renee/Downloads/cell_penetration_adversarial/&quot;, prefix) files &lt;- list.files(dir) # Deep learning dl &lt;- files[str_detect(files, &quot;DeepLearning_model&quot;)] for (m in dl) { model &lt;- h2o.loadModel(paste0(dir, &quot;/&quot;, m)) hs &lt;- sha1(paste(c( model@allparameters$epsilon, model@allparameters$hidden, model@allparameters$hidden_dropout_ratios, model@allparameters$input_dropout_ratio, model@allparameters$rho ), collapse = &quot; &quot;)) if (hs %in% names(dl_grid)) { dl_grid[[hs]] &lt;- c(dl_grid[[hs]], m) } else { dl_grid[[hs]] &lt;- c(m) dl_grid_params &lt;- list.append( dl_grid_params, c( &quot;epsilon&quot; = model@allparameters$epsilon, &quot;hidden&quot; = paste0(&quot;[&quot;, paste(model@allparameters$hidden, collapse = &quot;,&quot;), &quot;]&quot;), &quot;hidden_dropout_ratios&quot; = paste0( &quot;[&quot;, paste(model@allparameters$hidden_dropout_ratios, collapse = &quot;,&quot; ), &quot;]&quot; ), &quot;input_dropout_ratio&quot; = model@allparameters$input_dropout_ratio, &quot;rho&quot; = model@allparameters$rho ) ) } } h2o.removeAll() # GBM gbm &lt;- files[str_detect(files, &quot;GBM_model&quot;)] for (m in gbm) { model &lt;- h2o.loadModel(paste0(dir, &quot;/&quot;, m)) hs &lt;- sha1(paste(c( model@allparameters$col_sample_rate, model@allparameters$col_sample_rate_per_tree, model@allparameters$max_depth, model@allparameters$min_rows, model@allparameters$min_split_improvement, model@allparameters$sample_rate ), collapse = &quot; &quot;)) if (hs %in% names(gbm_grid)) { gbm_grid[[hs]] &lt;- c(gbm_grid[[hs]], m) } else { gbm_grid[[hs]] &lt;- c(m) gbm_grid_params &lt;- list.append( gbm_grid_params, c( &quot;col_sample_rate&quot; = model@allparameters$col_sample_rate, &quot;col_sample_rate_per_tree&quot; = model@allparameters$col_sample_rate_per_tree, &quot;max_depth&quot; = model@allparameters$max_depth, &quot;min_rows&quot; = model@allparameters$min_rows, &quot;min_split_improvement&quot; = model@allparameters$min_split_improvement, &quot;sample_rate&quot; = model@allparameters$sample_rate ) ) } } h2o.removeAll() # XGBoost xgboost &lt;- files[str_detect(files, &quot;XGBoost_model&quot;)] for (m in xgboost) { model &lt;- h2o.loadModel(paste0(dir, &quot;/&quot;, m)) hs &lt;- sha1(paste(c( model@allparameters$booster, model@allparameters$col_sample_rate, model@allparameters$col_sample_rate_per_tree, model@allparameters$max_depth, model@allparameters$min_rows, model@allparameters$reg_alpha, model@allparameters$reg_lambda, model@allparameters$sample_rate ), collapse = &quot; &quot;)) if (hs %in% names(xgboost_grid)) { xgboost_grid[[hs]] &lt;- c(xgboost_grid[[hs]], m) } else { xgboost_grid[[hs]] &lt;- c(m) xgboost_grid_params &lt;- list.append( xgboost_grid_params, c( &quot;booster&quot; = model@allparameters$booster, &quot;col_sample_rate&quot; = model@allparameters$col_sample_rate, &quot;col_sample_rate_per_tree&quot; = model@allparameters$col_sample_rate_per_tree, &quot;max_depth&quot; = model@allparameters$max_depth, &quot;min_rows&quot; = model@allparameters$min_rows, &quot;reg_alpha&quot; = model@allparameters$reg_alpha, &quot;reg_lambda&quot; = model@allparameters$reg_lambda, &quot;sample_rate&quot; = model@allparameters$sample_rate ) ) } } h2o.removeAll() } dl_grid_params &lt;- as.data.frame(t(data.frame(dl_grid_params))) rownames(dl_grid_params) &lt;- paste(&quot;Neural network grid model&quot;, 1:nrow(dl_grid_params)) gbm_grid_params &lt;- as.data.frame(t(data.frame(gbm_grid_params))) rownames(gbm_grid_params) &lt;- paste(&quot;GBM grid model&quot;, 1:nrow(gbm_grid_params)) xgboost_grid_params &lt;- as.data.frame(t(data.frame(xgboost_grid_params))) rownames(xgboost_grid_params) &lt;- paste(&quot;XGBoost grid model&quot;, 1:nrow(xgboost_grid_params)) write.csv(dl_grid_params, &quot;./other_data/cell_penetration_adversarial/neural_network_grid_params.csv&quot;) write.csv(gbm_grid_params, &quot;./other_data/cell_penetration_adversarial/gbm_grid_params.csv&quot;) write.csv(xgboost_grid_params, &quot;./other_data/cell_penetration_adversarial/xgboost_grid_params.csv&quot;) save(dl_grid, gbm_grid, xgboost_grid, file = &quot;./rdata/model_training_grid_models_cpp_ad.RData&quot;) 5.3.3 Model evaluation model_evaluation &lt;- function(holdout_pred, fold_asign, grid_name, grid_meta = NULL) { res_ &lt;- list() model_num &lt;- c() if (startsWith(grid_name, &quot;Super learner&quot;)) { if (grid_name == &quot;Super learner all models&quot;) { m &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_iter_0&quot;)] } else if (grid_name == &quot;Super learner final&quot;) { sl_models &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_iter&quot;)] m &lt;- sl_models[length(sl_models)] } else if (grid_name == &quot;Super learner neural network&quot;) { m &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_deeplearning&quot;)] } else if (grid_name == &quot;Super learner GBM&quot;) { m &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_gbm&quot;)] } else if (grid_name == &quot;Super learner XGBoost&quot;) { m &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_xgboost&quot;)] } logloss &lt;- c() MCC &lt;- c() F1 &lt;- c() acc &lt;- c() EF &lt;- c() BEDROC &lt;- c() threshold &lt;- 0.5 for (k in 1:10) { y_true &lt;- sapply(holdout_pred[fold_assign == k, &quot;category&quot;], function(x) if (x == &quot;penetrating&quot;) 1 else 0) y_pred &lt;- holdout_pred[fold_assign == k, m] y_pred_cls &lt;- sapply(y_pred, function(x) if (x &gt;= threshold) 1 else 0) logloss &lt;- c(logloss, LogLoss(y_pred = y_pred, y_true = y_true)) MCC &lt;- c(MCC, mcc(preds = y_pred_cls, actuals = y_true)) F1 &lt;- c(F1, F1_Score(y_pred = y_pred_cls, y_true = y_true)) acc &lt;- c(acc, Accuracy(y_pred = y_pred_cls, y_true = y_true)) EF &lt;- c(EF, enrichment_factor(x = y_pred, y = y_true, top = 0.05)) BEDROC &lt;- c(BEDROC, bedroc(x = y_pred, y = y_true, alpha = 20)) } res_ &lt;- list.append(res_, c( mean(logloss, na.rm = TRUE), sem(logloss), mean(MCC, na.rm = TRUE), sem(MCC), mean(F1, na.rm = TRUE), sem(F1), mean(acc, na.rm = TRUE), sem(acc), mean(EF, na.rm = TRUE), sem(EF), mean(BEDROC, na.rm = TRUE), sem(BEDROC), logloss, MCC, F1, acc, EF, BEDROC )) } else { for (j in 1:length(grid_meta)) { g &lt;- grid_meta[[j]] m &lt;- intersect(colnames(holdout_pred), g) if (length(m) == 1) { model_num &lt;- c(model_num, j) logloss &lt;- c() MCC &lt;- c() F1 &lt;- c() acc &lt;- c() EF &lt;- c() BEDROC &lt;- c() threshold &lt;- 0.5 for (k in 1:10) { y_true &lt;- sapply(holdout_pred[fold_assign == k, &quot;category&quot;], function(x) if (x == &quot;penetrating&quot;) 1 else 0) y_pred &lt;- holdout_pred[fold_assign == k, m] y_pred_cls &lt;- sapply(y_pred, function(x) if (x &gt;= threshold) 1 else 0) logloss &lt;- c(logloss, LogLoss(y_pred = y_pred, y_true = y_true)) MCC &lt;- c(MCC, mcc(preds = y_pred_cls, actuals = y_true)) F1 &lt;- c(F1, F1_Score(y_pred = y_pred_cls, y_true = y_true)) acc &lt;- c(acc, Accuracy(y_pred = y_pred_cls, y_true = y_true)) EF &lt;- c(EF, enrichment_factor(x = y_pred, y = y_true, top = 0.05)) BEDROC &lt;- c(BEDROC, bedroc(x = y_pred, y = y_true, alpha = 20)) } res_ &lt;- list.append(res_, c( mean(logloss, na.rm = TRUE), sem(logloss), mean(MCC, na.rm = TRUE), sem(MCC), mean(F1, na.rm = TRUE), sem(F1), mean(acc, na.rm = TRUE), sem(acc), mean(EF, na.rm = TRUE), sem(EF), mean(BEDROC, na.rm = TRUE), sem(BEDROC), logloss, MCC, F1, acc, EF, BEDROC )) } } } res &lt;- as.data.frame(t(data.frame(res_))) colnames(res) &lt;- c( &quot;Log loss mean&quot;, &quot;Log loss s.e.m.&quot;, &quot;MCC mean&quot;, &quot;MCC s.e.m.&quot;, &quot;F_1 mean&quot;, &quot;F_1 s.e.m.&quot;, &quot;Accuracy mean&quot;, &quot;Accuracy s.e.m.&quot;, &quot;EF mean&quot;, &quot;EF s.e.m.&quot;, &quot;BEDROC mean&quot;, &quot;BEDROC s.e.m.&quot;, paste(&quot;Log loss CV&quot;, 1:10), paste(&quot;MCC CV&quot;, 1:10), paste(&quot;F_1 CV&quot;, 1:10), paste(&quot;Accuracy CV&quot;, 1:10), paste(&quot;EF CV&quot;, 1:10), paste(&quot;BEDROC CV&quot;, 1:10) ) if (nrow(res) == 1) { rownames(res) &lt;- grid_name } else { rownames(res) &lt;- paste(grid_name, model_num) } return(res) } 5.3.4 Inner loop model selection load(file = &quot;./rdata/model_training_grid_models_cpp_ad.RData&quot;) for (i in 1:10) { holdout_pred &lt;- read.csv(paste0(&quot;./other_data/cell_penetration_adversarial/outer_&quot;, i, &quot;/cv_holdout_predictions.csv&quot;)) fold_assign &lt;- rep(1:10, ceiling(nrow(holdout_pred) / 10))[1:nrow(holdout_pred)] data_ &lt;- list() # Deep learning grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Neural network grid model&quot;, grid_meta = dl_grid )) # GBM grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;GBM grid model&quot;, grid_meta = gbm_grid )) # GBM default models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;GBM model&quot;, grid_meta = as.list(paste0(&quot;GBM_&quot;, 1:5)) )) # XGBoost grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XGBoost grid model&quot;, grid_meta = xgboost_grid )) # XGBoost default models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XGBoost model&quot;, grid_meta = as.list(paste0(&quot;XGBoost_&quot;, 1:3)) )) # DRF data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;DRF model&quot;, grid_meta = list(&quot;DRF&quot;))) # XRT data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XRT model&quot;, grid_meta = list(&quot;XRT&quot;))) # Super learner all data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner all models&quot;)) # Super learner final data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner final&quot;)) # Super learner deep learning data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner neural network&quot;)) # Super learner GBM data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner GBM&quot;)) # Super learner XGBoost data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner XGBoost&quot;)) data &lt;- do.call(rbind, data_) write.csv(data, paste0(&quot;./other_data/cell_penetration_adversarial/outer_&quot;, i, &quot;/cv_res.csv&quot;)) } 5.3.4.1 CV 1 5.3.4.2 CV 2 5.3.4.3 CV 3 5.3.4.4 CV 4 5.3.4.5 CV 5 5.3.4.6 CV 6 5.3.4.7 CV 7 5.3.4.8 CV 8 5.3.4.9 CV 9 5.3.4.10 CV 10 5.3.5 Final evaluation load(file = &quot;./rdata/var_reduct_cpp_train_test_splits_y_shuffled.RData&quot;) load(file = &quot;./rdata/model_training_grid_models_cpp_ad.RData&quot;) dir &lt;- &quot;/Users/renee/Downloads/cell_penetration_adversarial&quot; selected_models &lt;- c( &quot;GBM grid model 68&quot;, &quot;Super learner XGBoost&quot;, &quot;GBM grid model 75&quot;, &quot;Super learner XGBoost&quot;, &quot;GBM grid model 21&quot;, &quot;Super learner final&quot;, &quot;GBM grid model 15&quot;, &quot;GBM grid model 6&quot;, &quot;GBM grid model 76&quot;, &quot;Neural network grid model 86&quot; ) logloss &lt;- c() MCC &lt;- c() F1 &lt;- c() acc &lt;- c() EF &lt;- c() BEDROC &lt;- c() threshold &lt;- 0.5 for (i in 1:10) { holdout_pred &lt;- read.csv(paste0(&quot;./other_data/cell_penetration_adversarial/outer_&quot;, i, &quot;/cv_holdout_predictions.csv&quot;)) if (startsWith(selected_models[i], &quot;Neural network grid model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) m &lt;- intersect(colnames(holdout_pred), dl_grid[[grid_num]]) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, m)) } else if (startsWith(selected_models[i], &quot;GBM grid model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) m &lt;- intersect(colnames(holdout_pred), gbm_grid[[grid_num]]) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, m)) } else if (startsWith(selected_models[i], &quot;GBM model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/GBM_&quot;, grid_num)) } else if (startsWith(selected_models[i], &quot;XGBoost grid model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) m &lt;- intersect(colnames(holdout_pred), xgboost_grid[[grid_num]]) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, m)) } else if (startsWith(selected_models[i], &quot;XGBoost model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/XGBoost_&quot;, grid_num)) } else if (selected_models[i] == &quot;Super learner all models&quot;) { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/superlearner_iter_0&quot;)) } else if (selected_models[i] == &quot;Super learner final&quot;) { files &lt;- list.files(paste0(dir, &quot;/outer_&quot;, i)) files &lt;- files[startsWith(files, &quot;superlearner_iter&quot;)] model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, files[length(files)])) } else if (selected_models[i] == &quot;Super learner neural network&quot;) { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/superlearner_deeplearning&quot;)) } else if (selected_models[i] == &quot;Super learner GBM&quot;) { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/superlearner_gbm&quot;)) } else if (selected_models[i] == &quot;Super learner XGBoost&quot;) { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/superlearner_xgboost&quot;)) } else { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, unlist(strsplit(selected_models[i], &quot; &quot;))[1])) } tmp &lt;- as.h2o(subset(outer_splits[[i]][[&quot;test&quot;]], select = -category)) y_true &lt;- sapply(outer_splits[[i]][[&quot;test&quot;]]$category, function(x) if (x == &quot;penetrating&quot;) 1 else 0) y_pred &lt;- as.data.frame(as.data.frame(h2o.predict(model, tmp)))[, &quot;penetrating&quot;] y_pred_cls &lt;- sapply(y_pred, function(x) if (x &gt;= threshold) 1 else 0) logloss &lt;- c(logloss, LogLoss(y_pred = y_pred, y_true = y_true)) MCC &lt;- c(MCC, mcc(preds = y_pred_cls, actuals = y_true)) F1 &lt;- c(F1, F1_Score(y_pred = y_pred_cls, y_true = y_true)) acc &lt;- c(acc, Accuracy(y_pred = y_pred_cls, y_true = y_true)) EF &lt;- c(EF, enrichment_factor(x = y_pred, y = y_true, top = 0.05)) BEDROC &lt;- c(BEDROC, bedroc(x = y_pred, y = y_true, alpha = 20)) } save(logloss, MCC, F1, acc, EF, BEDROC, file = &quot;./rdata/final_evaluation_cpp_ad.RData&quot;) load(file = &quot;./rdata/final_evaluation_cpp_ad.RData&quot;) data.frame( Metric = c(&quot;Log loss&quot;, &quot;MCC&quot;, &quot;F&lt;sub&gt;1&lt;/sub&gt;&quot;, &quot;Accuracy&quot;, &quot;EF&quot;, &quot;BEDROC&quot;), `Mean ± s.e.m.` = c( paste0(round(mean(logloss), 3), &quot; ± &quot;, round(sem(logloss), 3)), paste0(round(mean(MCC), 3), &quot; ± &quot;, round(sem(MCC), 3)), paste0(round(mean(F1), 3), &quot; ± &quot;, round(sem(F1), 3)), paste0(round(mean(acc), 3), &quot; ± &quot;, round(sem(acc), 3)), paste0(round(mean(EF), 3), &quot; ± &quot;, round(sem(EF), 3)), paste0(round(mean(BEDROC), 3), &quot; ± &quot;, round(sem(BEDROC), 3)) ), check.names = FALSE ) %&gt;% datatable(escape = FALSE, rownames = FALSE) 5.4 Toxicity (classification) load(file = &quot;./rdata/var_reduct_tx_train_test_splits.RData&quot;) set.seed(32) # Shuffle the response variable of cross-validation training sets for (i in 1:10) { response &lt;- outer_splits[[i]]$train$category response &lt;- sample(response) outer_splits[[i]]$train$category &lt;- response } # Shuffle the response variable of the whole data set response &lt;- tx_data$category response &lt;- sample(response) tx_data$category &lt;- response save(tx_data, outer_splits, file = &quot;./rdata/var_reduct_tx_train_test_splits_y_shuffled.RData&quot; ) 5.4.1 Model training train_tx_models &lt;- function(train_set, exp_dir, prefix, nfolds = 10, grid_seed = 1) { tmp &lt;- as.h2o(train_set, destination_frame = prefix) tmp[&quot;category&quot;] &lt;- as.factor(tmp[&quot;category&quot;]) y &lt;- &quot;category&quot; x &lt;- setdiff(names(tmp), y) res &lt;- as.data.frame(tmp$category) samp_factors &lt;- as.vector(mean(table(train_set$category)) / table(train_set$category)) # ------------------- # base model training # ------------------- cat(&quot;Deep learning grid 1\\n&quot;) deeplearning_1 &lt;- h2o.grid( algorithm = &quot;deeplearning&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = deeplearning_params_1, stopping_rounds = 3, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5, search_criteria = list( strategy = &quot;RandomDiscrete&quot;, max_models = 100, seed = grid_seed ), keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in deeplearning_1@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;GBM grid\\n&quot;) gbm &lt;- h2o.grid( algorithm = &quot;gbm&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = gbm_params, stopping_rounds = 3, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5, search_criteria = list(strategy = &quot;RandomDiscrete&quot;, max_models = 100, seed = grid_seed), keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in gbm@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;GBM 5 default models\\n&quot;) gbm_1 &lt;- h2o.gbm( model_id = &quot;GBM_1&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 6, min_rows = 1, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_2 &lt;- h2o.gbm( model_id = &quot;GBM_2&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 7, min_rows = 10, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_3 &lt;- h2o.gbm( model_id = &quot;GBM_3&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 8, min_rows = 10, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_4 &lt;- h2o.gbm( model_id = &quot;GBM_4&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 10, min_rows = 10, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_5 &lt;- h2o.gbm( model_id = &quot;GBM_5&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 15, min_rows = 100, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) for (model_id in paste0(&quot;GBM_&quot;, 1:5)) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;XGBoost grid\\n&quot;) xgboost &lt;- h2o.grid( algorithm = &quot;xgboost&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = xgboost_params, stopping_rounds = 3, search_criteria = list(strategy = &quot;RandomDiscrete&quot;, max_models = 100, seed = grid_seed), keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in xgboost@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;XGBoost 3 default models\\n&quot;) xgboost_1 &lt;- h2o.xgboost( model_id = &quot;XGBoost_1&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, booster = &quot;gbtree&quot;, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, max_depth = 10, min_rows = 5, ntrees = 10000, reg_alpha = 0, reg_lambda = 1, sample_rate = 0.6, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) xgboost_2 &lt;- h2o.xgboost( model_id = &quot;XGBoost_2&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, booster = &quot;gbtree&quot;, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, max_depth = 20, min_rows = 10, ntrees = 10000, reg_alpha = 0, reg_lambda = 1, sample_rate = 0.6, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) xgboost_3 &lt;- h2o.xgboost( model_id = &quot;XGBoost_3&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, booster = &quot;gbtree&quot;, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, max_depth = 5, min_rows = 3, ntrees = 10000, reg_alpha = 0, reg_lambda = 1, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) for (model_id in paste0(&quot;XGBoost_&quot;, 1:3)) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;GLM\\n&quot;) glm &lt;- h2o.glm( model_id = &quot;GLM&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, alpha = c(0.0, 0.2, 0.4, 0.6, 0.8, 1.0), balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;GLM&quot;), path = exp_dir, force = TRUE) cat(&quot;DRF\\n&quot;) drf &lt;- h2o.randomForest( model_id = &quot;DRF&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, ntrees = 10000, score_tree_interval = 5, stopping_rounds = 3, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;DRF&quot;), path = exp_dir, force = TRUE) cat(&quot;XRT\\n&quot;) xrt &lt;- h2o.randomForest( model_id = &quot;XRT&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, ntrees = 10000, histogram_type = &quot;Random&quot;, score_tree_interval = 5, stopping_rounds = 3, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;XRT&quot;), path = exp_dir, force = TRUE) # ----------------------- # get holdout predictions # ----------------------- base_models &lt;- as.list(c( unlist(deeplearning_1@model_ids), unlist(gbm@model_ids), paste0(&quot;GBM_&quot;, 1:5), unlist(xgboost@model_ids), paste0(&quot;XGBoost_&quot;, 1:3), &quot;GLM&quot;, &quot;DRF&quot;, &quot;XRT&quot; )) for (model_id in base_models) { res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[3], col.names = model_id )) } # ---------------------- # super learner training # ---------------------- sl_iter &lt;- 0 cat(paste0(&quot;Super learner iteration &quot;, sl_iter, &quot; (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = paste0(&quot;superlearner_iter_&quot;, sl_iter), training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list( standardize = TRUE, keep_cross_validation_predictions = TRUE, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5 ) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(paste0(&quot;superlearner_iter_&quot;, sl_iter)), path = exp_dir, force = TRUE) model_id &lt;- paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter) tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[3], col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) # ---------------------------------- # super learner base model reduction # ---------------------------------- while (TRUE) { meta &lt;- h2o.getModel(paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter, &quot;_cv_1&quot;)) names &lt;- meta@model$coefficients_table[, &quot;names&quot;] coeffs &lt;- (meta@model$coefficients_table[, &quot;standardized_coefficients&quot;] &gt; 0) for (j in 2:nfolds) { meta &lt;- h2o.getModel(paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter, &quot;_cv_&quot;, j)) names &lt;- meta@model$coefficients_table[, &quot;names&quot;] coeffs &lt;- coeffs + (meta@model$coefficients_table[, &quot;standardized_coefficients&quot;] &gt; 0) } base_models_ &lt;- as.list(names[coeffs &gt;= ceiling(nfolds / 2) &amp; names != &quot;Intercept&quot;]) if (length(base_models_) == 0) { cat(&quot;No base models passing the threshold\\n\\n&quot;) break } if (sum(base_models %in% base_models_) == length(base_models)) { cat(&quot;No further reduction of base models\\n\\n&quot;) break } sl_iter &lt;- sl_iter + 1 base_models &lt;- base_models_ cat(paste0(&quot;Super learner iteration &quot;, sl_iter, &quot; (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = paste0(&quot;superlearner_iter_&quot;, sl_iter), training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list( standardize = TRUE, keep_cross_validation_predictions = TRUE, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5 ) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(paste0(&quot;superlearner_iter_&quot;, sl_iter)), path = exp_dir, force = TRUE) model_id &lt;- paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter) tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[3], col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) } # ----------------------------------------- # super learner for homogeneous base models # ----------------------------------------- # DeepLearning base_models &lt;- deeplearning_1@model_ids cat(paste0(&quot;Super learner deep learning (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = &quot;superlearner_deeplearning&quot;, training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list( standardize = TRUE, keep_cross_validation_predictions = TRUE, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5 ) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;superlearner_deeplearning&quot;), path = exp_dir, force = TRUE) model_id &lt;- &quot;metalearner_glm_superlearner_deeplearning&quot; tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[3], col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) # GBM base_models &lt;- as.list(c(unlist(gbm@model_ids), paste0(&quot;GBM_&quot;, 1:5))) cat(paste0(&quot;Super learner GBM (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = &quot;superlearner_gbm&quot;, training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list( standardize = TRUE, keep_cross_validation_predictions = TRUE, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5 ) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;superlearner_gbm&quot;), path = exp_dir, force = TRUE) model_id &lt;- &quot;metalearner_glm_superlearner_gbm&quot; tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[3], col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) # XGBoost base_models &lt;- as.list(c(unlist(xgboost@model_ids), paste0(&quot;XGBoost_&quot;, 1:3))) cat(paste0(&quot;Super learner XGBoost (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = &quot;superlearner_xgboost&quot;, training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list( standardize = TRUE, keep_cross_validation_predictions = TRUE, balance_classes = TRUE, class_sampling_factors = samp_factors, max_after_balance_size = 0.5 ) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;superlearner_xgboost&quot;), path = exp_dir, force = TRUE) model_id &lt;- &quot;metalearner_glm_superlearner_xgboost&quot; tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[3], col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) write.csv(res, file = paste0(exp_dir, &quot;/cv_holdout_predictions.csv&quot;), row.names = FALSE) cat(&quot;\\n\\n&quot;) h2o.removeAll() } 5.4.2 Inner cross-validation load(file = &quot;./rdata/var_reduct_tx_train_test_splits_y_shuffled.RData&quot;) for (i in 1:10) { cat(paste0(&quot;Outer training set &quot;, i, &quot;\\n&quot;)) prefix &lt;- paste0(&quot;outer_&quot;, i) exp_dir &lt;- paste0(&quot;/Users/renee/Downloads/toxicity_adversarial/&quot;, prefix) dir.create(exp_dir) train_tx_models(train_set = outer_splits[[i]][[&quot;train&quot;]], exp_dir = exp_dir, prefix = prefix) } # Keep track of grid models dl_grid &lt;- list() gbm_grid &lt;- list() xgboost_grid &lt;- list() dl_grid_params &lt;- list() gbm_grid_params &lt;- list() xgboost_grid_params &lt;- list() for (i in 1:10) { cat(paste0(&quot;Outer training set &quot;, i, &quot;\\n&quot;)) prefix &lt;- paste0(&quot;outer_&quot;, i) dir &lt;- paste0(&quot;/Users/renee/Downloads/toxicity_adversarial/&quot;, prefix) files &lt;- list.files(dir) # Deep learning dl &lt;- files[str_detect(files, &quot;DeepLearning_model&quot;)] for (m in dl) { model &lt;- h2o.loadModel(paste0(dir, &quot;/&quot;, m)) hs &lt;- sha1(paste(c( model@allparameters$epsilon, model@allparameters$hidden, model@allparameters$hidden_dropout_ratios, model@allparameters$input_dropout_ratio, model@allparameters$rho ), collapse = &quot; &quot;)) if (hs %in% names(dl_grid)) { dl_grid[[hs]] &lt;- c(dl_grid[[hs]], m) } else { dl_grid[[hs]] &lt;- c(m) dl_grid_params &lt;- list.append( dl_grid_params, c( &quot;epsilon&quot; = model@allparameters$epsilon, &quot;hidden&quot; = paste0(&quot;[&quot;, paste(model@allparameters$hidden, collapse = &quot;,&quot;), &quot;]&quot;), &quot;hidden_dropout_ratios&quot; = paste0( &quot;[&quot;, paste(model@allparameters$hidden_dropout_ratios, collapse = &quot;,&quot; ), &quot;]&quot; ), &quot;input_dropout_ratio&quot; = model@allparameters$input_dropout_ratio, &quot;rho&quot; = model@allparameters$rho ) ) } } h2o.removeAll() # GBM gbm &lt;- files[str_detect(files, &quot;GBM_model&quot;)] for (m in gbm) { model &lt;- h2o.loadModel(paste0(dir, &quot;/&quot;, m)) hs &lt;- sha1(paste(c( model@allparameters$col_sample_rate, model@allparameters$col_sample_rate_per_tree, model@allparameters$max_depth, model@allparameters$min_rows, model@allparameters$min_split_improvement, model@allparameters$sample_rate ), collapse = &quot; &quot;)) if (hs %in% names(gbm_grid)) { gbm_grid[[hs]] &lt;- c(gbm_grid[[hs]], m) } else { gbm_grid[[hs]] &lt;- c(m) gbm_grid_params &lt;- list.append( gbm_grid_params, c( &quot;col_sample_rate&quot; = model@allparameters$col_sample_rate, &quot;col_sample_rate_per_tree&quot; = model@allparameters$col_sample_rate_per_tree, &quot;max_depth&quot; = model@allparameters$max_depth, &quot;min_rows&quot; = model@allparameters$min_rows, &quot;min_split_improvement&quot; = model@allparameters$min_split_improvement, &quot;sample_rate&quot; = model@allparameters$sample_rate ) ) } } h2o.removeAll() # XGBoost xgboost &lt;- files[str_detect(files, &quot;XGBoost_model&quot;)] for (m in xgboost) { model &lt;- h2o.loadModel(paste0(dir, &quot;/&quot;, m)) hs &lt;- sha1(paste(c( model@allparameters$booster, model@allparameters$col_sample_rate, model@allparameters$col_sample_rate_per_tree, model@allparameters$max_depth, model@allparameters$min_rows, model@allparameters$reg_alpha, model@allparameters$reg_lambda, model@allparameters$sample_rate ), collapse = &quot; &quot;)) if (hs %in% names(xgboost_grid)) { xgboost_grid[[hs]] &lt;- c(xgboost_grid[[hs]], m) } else { xgboost_grid[[hs]] &lt;- c(m) xgboost_grid_params &lt;- list.append( xgboost_grid_params, c( &quot;booster&quot; = model@allparameters$booster, &quot;col_sample_rate&quot; = model@allparameters$col_sample_rate, &quot;col_sample_rate_per_tree&quot; = model@allparameters$col_sample_rate_per_tree, &quot;max_depth&quot; = model@allparameters$max_depth, &quot;min_rows&quot; = model@allparameters$min_rows, &quot;reg_alpha&quot; = model@allparameters$reg_alpha, &quot;reg_lambda&quot; = model@allparameters$reg_lambda, &quot;sample_rate&quot; = model@allparameters$sample_rate ) ) } } h2o.removeAll() } dl_grid_params &lt;- as.data.frame(t(data.frame(dl_grid_params))) rownames(dl_grid_params) &lt;- paste(&quot;Neural network grid model&quot;, 1:nrow(dl_grid_params)) gbm_grid_params &lt;- as.data.frame(t(data.frame(gbm_grid_params))) rownames(gbm_grid_params) &lt;- paste(&quot;GBM grid model&quot;, 1:nrow(gbm_grid_params)) xgboost_grid_params &lt;- as.data.frame(t(data.frame(xgboost_grid_params))) rownames(xgboost_grid_params) &lt;- paste(&quot;XGBoost grid model&quot;, 1:nrow(xgboost_grid_params)) write.csv(dl_grid_params, &quot;./other_data/toxicity_adversarial/neural_network_grid_params.csv&quot;) write.csv(gbm_grid_params, &quot;./other_data/toxicity_adversarial/gbm_grid_params.csv&quot;) write.csv(xgboost_grid_params, &quot;./other_data/toxicity_adversarial/xgboost_grid_params.csv&quot;) save(dl_grid, gbm_grid, xgboost_grid, file = &quot;./rdata/model_training_grid_models_tx_ad.RData&quot;) 5.4.3 Model evaluation model_evaluation &lt;- function(holdout_pred, fold_asign, grid_name, grid_meta = NULL) { res_ &lt;- list() model_num &lt;- c() if (startsWith(grid_name, &quot;Super learner&quot;)) { if (grid_name == &quot;Super learner all models&quot;) { m &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_iter_0&quot;)] } else if (grid_name == &quot;Super learner final&quot;) { sl_models &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_iter&quot;)] m &lt;- sl_models[length(sl_models)] } else if (grid_name == &quot;Super learner neural network&quot;) { m &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_deeplearning&quot;)] } else if (grid_name == &quot;Super learner GBM&quot;) { m &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_gbm&quot;)] } else if (grid_name == &quot;Super learner XGBoost&quot;) { m &lt;- colnames(holdout_pred)[startsWith(colnames(holdout_pred), &quot;metalearner_glm_superlearner_xgboost&quot;)] } logloss &lt;- c() MCC &lt;- c() F1 &lt;- c() acc &lt;- c() EF &lt;- c() BEDROC &lt;- c() threshold &lt;- 0.5 for (k in 1:10) { y_true &lt;- holdout_pred[fold_assign == k, &quot;category&quot;] y_pred &lt;- holdout_pred[fold_assign == k, m] y_pred_cls &lt;- sapply(y_pred, function(x) if (x &gt;= threshold) 1 else 0) logloss &lt;- c(logloss, LogLoss(y_pred = y_pred, y_true = y_true)) MCC &lt;- c(MCC, mcc(preds = y_pred_cls, actuals = y_true)) F1 &lt;- c(F1, F1_Score(y_pred = y_pred_cls, y_true = y_true)) acc &lt;- c(acc, Accuracy(y_pred = y_pred_cls, y_true = y_true)) EF &lt;- c(EF, enrichment_factor(x = y_pred, y = y_true, top = 0.05)) BEDROC &lt;- c(BEDROC, bedroc(x = y_pred, y = y_true, alpha = 20)) } res_ &lt;- list.append(res_, c( mean(logloss, na.rm = TRUE), sem(logloss), mean(MCC, na.rm = TRUE), sem(MCC), mean(F1, na.rm = TRUE), sem(F1), mean(acc, na.rm = TRUE), sem(acc), mean(EF, na.rm = TRUE), sem(EF), mean(BEDROC, na.rm = TRUE), sem(BEDROC), logloss, MCC, F1, acc, EF, BEDROC )) } else { for (j in 1:length(grid_meta)) { g &lt;- grid_meta[[j]] m &lt;- intersect(colnames(holdout_pred), g) if (length(m) == 1) { model_num &lt;- c(model_num, j) logloss &lt;- c() MCC &lt;- c() F1 &lt;- c() acc &lt;- c() EF &lt;- c() BEDROC &lt;- c() threshold &lt;- 0.5 for (k in 1:10) { y_true &lt;- holdout_pred[fold_assign == k, &quot;category&quot;] y_pred &lt;- holdout_pred[fold_assign == k, m] y_pred_cls &lt;- sapply(y_pred, function(x) if (x &gt;= threshold) 1 else 0) logloss &lt;- c(logloss, LogLoss(y_pred = y_pred, y_true = y_true)) MCC &lt;- c(MCC, mcc(preds = y_pred_cls, actuals = y_true)) F1 &lt;- c(F1, F1_Score(y_pred = y_pred_cls, y_true = y_true)) acc &lt;- c(acc, Accuracy(y_pred = y_pred_cls, y_true = y_true)) EF &lt;- c(EF, enrichment_factor(x = y_pred, y = y_true, top = 0.05)) BEDROC &lt;- c(BEDROC, bedroc(x = y_pred, y = y_true, alpha = 20)) } res_ &lt;- list.append(res_, c( mean(logloss, na.rm = TRUE), sem(logloss), mean(MCC, na.rm = TRUE), sem(MCC), mean(F1, na.rm = TRUE), sem(F1), mean(acc, na.rm = TRUE), sem(acc), mean(EF, na.rm = TRUE), sem(EF), mean(BEDROC, na.rm = TRUE), sem(BEDROC), logloss, MCC, F1, acc, EF, BEDROC )) } } } res &lt;- as.data.frame(t(data.frame(res_))) colnames(res) &lt;- c( &quot;Log loss mean&quot;, &quot;Log loss s.e.m.&quot;, &quot;MCC mean&quot;, &quot;MCC s.e.m.&quot;, &quot;F_1 mean&quot;, &quot;F_1 s.e.m.&quot;, &quot;Accuracy mean&quot;, &quot;Accuracy s.e.m.&quot;, &quot;EF mean&quot;, &quot;EF s.e.m.&quot;, &quot;BEDROC mean&quot;, &quot;BEDROC s.e.m.&quot;, paste(&quot;Log loss CV&quot;, 1:10), paste(&quot;MCC CV&quot;, 1:10), paste(&quot;F_1 CV&quot;, 1:10), paste(&quot;Accuracy CV&quot;, 1:10), paste(&quot;EF CV&quot;, 1:10), paste(&quot;BEDROC CV&quot;, 1:10) ) if (nrow(res) == 1) { rownames(res) &lt;- grid_name } else { rownames(res) &lt;- paste(grid_name, model_num) } return(res) } 5.4.4 Inner loop model selection load(file = &quot;./rdata/model_training_grid_models_tx_ad.RData&quot;) for (i in 1:10) { holdout_pred &lt;- read.csv(paste0(&quot;./other_data/toxicity_adversarial/outer_&quot;, i, &quot;/cv_holdout_predictions.csv&quot;)) fold_assign &lt;- rep(1:10, ceiling(nrow(holdout_pred) / 10))[1:nrow(holdout_pred)] data_ &lt;- list() # Deep learning grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Neural network grid model&quot;, grid_meta = dl_grid )) # GBM grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;GBM grid model&quot;, grid_meta = gbm_grid )) # GBM default models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;GBM model&quot;, grid_meta = as.list(paste0(&quot;GBM_&quot;, 1:5)) )) # XGBoost grid models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XGBoost grid model&quot;, grid_meta = xgboost_grid )) # XGBoost default models data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XGBoost model&quot;, grid_meta = as.list(paste0(&quot;XGBoost_&quot;, 1:3)) )) # GLM data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;GLM model&quot;, grid_meta = list(&quot;GLM&quot;))) # DRF data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;DRF model&quot;, grid_meta = list(&quot;DRF&quot;))) # XRT data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;XRT model&quot;, grid_meta = list(&quot;XRT&quot;))) # Super learner all data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner all models&quot;)) # Super learner final data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner final&quot;)) # Super learner deep learning data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner neural network&quot;)) # Super learner GBM data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner GBM&quot;)) # Super learner XGBoost data_ &lt;- list.append(data_, model_evaluation(holdout_pred, fold_assign, grid_name = &quot;Super learner XGBoost&quot;)) data &lt;- do.call(rbind, data_) write.csv(data, paste0(&quot;./other_data/toxicity_adversarial/outer_&quot;, i, &quot;/cv_res.csv&quot;)) } 5.4.4.1 CV 1 5.4.4.2 CV 2 5.4.4.3 CV 3 5.4.4.4 CV 4 5.4.4.5 CV 5 5.4.4.6 CV 6 5.4.4.7 CV 7 5.4.4.8 CV 8 5.4.4.9 CV 9 5.4.4.10 CV 10 5.4.5 Final evaluation load(file = &quot;./rdata/var_reduct_tx_train_test_splits_y_shuffled.RData&quot;) load(file = &quot;./rdata/model_training_grid_models_tx_ad.RData&quot;) dir &lt;- &quot;/Users/renee/Downloads/toxicity_adversarial&quot; selected_models &lt;- c( &quot;GBM grid model 52&quot;, &quot;GBM grid model 95&quot;, &quot;GBM grid model 48&quot;, &quot;GBM grid model 98&quot;, &quot;GBM grid model 55&quot;, &quot;GBM grid model 21&quot;, &quot;GBM grid model 23&quot;, &quot;GBM grid model 54&quot;, &quot;GBM grid model 3&quot;, &quot;GBM grid model 68&quot; ) logloss &lt;- c() MCC &lt;- c() F1 &lt;- c() acc &lt;- c() EF &lt;- c() BEDROC &lt;- c() threshold &lt;- 0.5 for (i in 1:10) { holdout_pred &lt;- read.csv(paste0(&quot;./other_data/toxicity_adversarial/outer_&quot;, i, &quot;/cv_holdout_predictions.csv&quot;)) if (startsWith(selected_models[i], &quot;Neural network grid model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) m &lt;- intersect(colnames(holdout_pred), dl_grid[[grid_num]]) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, m)) } else if (startsWith(selected_models[i], &quot;GBM grid model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) m &lt;- intersect(colnames(holdout_pred), gbm_grid[[grid_num]]) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, m)) } else if (startsWith(selected_models[i], &quot;GBM model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/GBM_&quot;, grid_num)) } else if (startsWith(selected_models[i], &quot;XGBoost grid model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) m &lt;- intersect(colnames(holdout_pred), xgboost_grid[[grid_num]]) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, m)) } else if (startsWith(selected_models[i], &quot;XGBoost model&quot;)) { grid_num &lt;- as.integer(str_extract(selected_models[i], &quot;[0-9]+&quot;)) model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/XGBoost_&quot;, grid_num)) } else if (selected_models[i] == &quot;Super learner all models&quot;) { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/superlearner_iter_0&quot;)) } else if (selected_models[i] == &quot;Super learner final&quot;) { files &lt;- list.files(paste0(dir, &quot;/outer_&quot;, i)) files &lt;- files[startsWith(files, &quot;superlearner_iter&quot;)] model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, files[length(files)])) } else if (selected_models[i] == &quot;Super learner neural network&quot;) { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/superlearner_deeplearning&quot;)) } else if (selected_models[i] == &quot;Super learner GBM&quot;) { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/superlearner_gbm&quot;)) } else if (selected_models[i] == &quot;Super learner XGBoost&quot;) { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/superlearner_xgboost&quot;)) } else { model &lt;- h2o.loadModel(paste0(dir, &quot;/outer_&quot;, i, &quot;/&quot;, unlist(strsplit(selected_models[i], &quot; &quot;))[1])) } tmp &lt;- as.h2o(subset(outer_splits[[i]][[&quot;test&quot;]], select = -category)) y_true &lt;- as.numeric(outer_splits[[i]][[&quot;test&quot;]]$category) - 1 y_pred &lt;- as.data.frame(as.data.frame(h2o.predict(model, tmp)))[, 3] y_pred_cls &lt;- sapply(y_pred, function(x) if (x &gt;= threshold) 1 else 0) logloss &lt;- c(logloss, LogLoss(y_pred = y_pred, y_true = y_true)) MCC &lt;- c(MCC, mcc(preds = y_pred_cls, actuals = y_true)) F1 &lt;- c(F1, F1_Score(y_pred = y_pred_cls, y_true = y_true)) acc &lt;- c(acc, Accuracy(y_pred = y_pred_cls, y_true = y_true)) EF &lt;- c(EF, enrichment_factor(x = y_pred, y = y_true, top = 0.05)) BEDROC &lt;- c(BEDROC, bedroc(x = y_pred, y = y_true, alpha = 20)) } save(logloss, MCC, F1, acc, EF, BEDROC, file = &quot;./rdata/final_evaluation_tx_ad.RData&quot;) load(file = &quot;./rdata/final_evaluation_tx_ad.RData&quot;) data.frame( Metric = c(&quot;Log loss&quot;, &quot;MCC&quot;, &quot;F&lt;sub&gt;1&lt;/sub&gt;&quot;, &quot;Accuracy&quot;, &quot;EF&quot;, &quot;BEDROC&quot;), `Mean ± s.e.m.` = c( paste0(round(mean(logloss), 3), &quot; ± &quot;, round(sem(logloss), 3)), paste0(round(mean(MCC), 3), &quot; ± &quot;, round(sem(MCC), 3)), paste0(round(mean(F1, na.rm = TRUE), 3), &quot; ± &quot;, round(sem(F1), 3)), paste0(round(mean(acc), 3), &quot; ± &quot;, round(sem(acc), 3)), paste0(round(mean(EF), 3), &quot; ± &quot;, round(sem(EF), 3)), paste0(round(mean(BEDROC), 3), &quot; ± &quot;, round(sem(BEDROC), 3)) ), check.names = FALSE ) %&gt;% datatable(escape = FALSE, rownames = FALSE) 5.5 Overall model interpretation import h2o import pandas as pd import numpy as np import shap import matplotlib.pyplot as plt import session_info 5.5.1 Melanin binding (regression) # Save train and test sets in csv format load(file = &quot;./rdata/var_reduct_mb_train_test_splits_y_shuffled.RData&quot;) for (i in 1:10) { prefix &lt;- paste0(&quot;outer_&quot;, i) exp_dir &lt;- paste0(&quot;/Users/renee/Downloads/melanin_binding_adversarial/&quot;, prefix) write.csv(outer_splits[[i]]$train, file = paste0(exp_dir, &quot;/train_set.csv&quot;)) write.csv(outer_splits[[i]]$test, file = paste0(exp_dir, &quot;/test_set.csv&quot;)) } h2o.init(nthreads=-1) dir_path = &#39;/Users/renee/Downloads/melanin_binding_adversarial&#39; model_names = [&#39;superlearner_iter_6&#39;, &#39;superlearner_iter_5&#39;, &#39;superlearner_iter_6&#39;, &#39;superlearner_iter_6&#39;, &#39;superlearner_iter_7&#39;, &#39;superlearner_iter_7&#39;, &#39;superlearner_iter_6&#39;, &#39;superlearner_iter_7&#39;, &#39;superlearner_iter_7&#39;, &#39;superlearner_iter_6&#39;] shap_res = [] for i in range(10): print(&#39;Iter &#39; + str(i + 1) + &#39;:&#39;) train = pd.read_csv(dir_path + &#39;/outer_&#39; + str(i + 1) + &#39;/train_set.csv&#39;, index_col=0) X_train = train.iloc[:, :-1] test = pd.read_csv(dir_path + &#39;/outer_&#39; + str(i + 1) + &#39;/test_set.csv&#39;, index_col=0) X_test = test.iloc[:, :-1] model = h2o.load_model(dir_path + &#39;/outer_&#39; + str(i + 1) + &#39;/&#39; + model_names[i]) def model_predict_(data): data = pd.DataFrame(data, columns=X_train.columns) h2o_data = h2o.H2OFrame(data) res = model.predict(h2o_data) res = res.as_data_frame() return(res) np.random.seed(1) X_train_ = X_train.iloc[np.random.choice(X_train.shape[0], 100, replace=False), :] explainer = shap.KernelExplainer(model_predict_, X_train_, link=&#39;identity&#39;) shap_values = explainer.shap_values(X_test, nsamples=1000) shap_res.append(shap_values) h2o.remove_all() shap_res = pd.DataFrame(np.vstack(shap_res), columns=X_train.columns) shap_res.to_csv(&#39;./other_data/mb_ad_shap_values_cv_data_sets.csv&#39;, index=False) load(file = &quot;./rdata/var_reduct_mb_train_test_splits_y_shuffled.RData&quot;) data &lt;- do.call(rbind, lapply(outer_splits, function(x) x$test[-ncol(mb_data)])) data &lt;- apply(data, 2, function(x) rank(x)) data &lt;- apply(data, 2, function(x) rescale(x, to = c(0, 100))) shap_values &lt;- as.matrix(read.csv(&quot;./other_data/mb_ad_shap_values_cv_data_sets.csv&quot;, check.names = FALSE)) shap_values &lt;- as.data.frame(rescale(shap_values, to = c(0, 100), from = range(mb_data$log_intensity))) var_diff &lt;- apply(shap_values, 2, function(x) max(x) - min(x)) var_diff &lt;- var_diff[order(-var_diff)] df &lt;- data.frame( variable = melt(shap_values)$variable, shap = melt(shap_values)$value, variable_value = melt(data)$value ) top_vars &lt;- names(var_diff)[1:20] df &lt;- df[df$variable %in% top_vars, ] df$variable &lt;- factor(df$variable, levels = rev(top_vars), labels = rev(top_vars)) p1 &lt;- ggplot(df, aes(x = shap, y = variable)) + geom_hline(yintercept = top_vars, linetype = &quot;dotted&quot;, color = &quot;grey80&quot;) + geom_vline(xintercept = 0, color = &quot;grey80&quot;, size = 1) + geom_point(aes(fill = variable_value), color = &quot;grey30&quot;, shape = 21, alpha = 0.3, size = 2, position = &quot;auto&quot;, stroke = 0.1) + scale_fill_gradient2(low = &quot;#1f77b4&quot;, mid = &quot;white&quot;, high = &quot;#d62728&quot;, midpoint = 50, breaks = c(0, 100), labels = c(&quot;Low&quot;, &quot;High&quot;)) + theme_bw() + theme( plot.margin = ggplot2::margin(1.5, 8, 1.5, -3), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank(), panel.border = element_blank(), axis.line = element_line(color = &quot;black&quot;), plot.title = element_text(hjust = 0.5, size = 20), axis.title.x = element_text(hjust = 0.5, colour = &quot;black&quot;, size = 20), axis.title.y = element_text(colour = &quot;black&quot;, size = 20), axis.text.x = element_text(colour = &quot;black&quot;, size = 17), axis.text.y = element_text(colour = &quot;black&quot;, size = 16), legend.title = element_text(size = 20), legend.text = element_text(colour = &quot;black&quot;, size = 16), legend.title.align = 0.5 ) + guides( fill = guide_colourbar(&quot;Variable\\nvalue\\n&quot;, ticks = FALSE, barheight = 10, barwidth = 0.5), color = &quot;none&quot; ) + xlab(&quot;SHAP value\\n(variable contribution to model prediction)&quot;) + ylab(&quot;&quot;) + ggtitle(&quot;Melanin binding&quot;) 5.5.2 Cell-penetration (classification) # Save train and test sets in csv format load(file = &quot;./rdata/var_reduct_cpp_train_test_splits_y_shuffled.RData&quot;) for (i in 1:10) { prefix &lt;- paste0(&quot;outer_&quot;, i) exp_dir &lt;- paste0(&quot;/Users/renee/Downloads/cell_penetration_adversarial/&quot;, prefix) write.csv(outer_splits[[i]]$train, file = paste0(exp_dir, &quot;/train_set.csv&quot;)) write.csv(outer_splits[[i]]$test, file = paste0(exp_dir, &quot;/test_set.csv&quot;)) } h2o.init(nthreads=-1) dir_path = &#39;/Users/renee/Downloads/cell_penetration_adversarial&#39; model_names = [&#39;GBM_model_1671047404901_26693&#39;, &#39;GBM_model_1671047404901_85146&#39;, &#39;GBM_model_1671047404901_143415&#39;, &#39;GBM_model_1671047404901_201211&#39;, &#39;GBM_model_1671047404901_258107&#39;, &#39;GBM_model_1671047404901_315866&#39;, &#39;GBM_model_1671047404901_373347&#39;, &#39;GBM_model_1671047404901_432044&#39;, &#39;GBM_model_1671122685293_29338&#39;, &#39;GBM_model_1671122685293_86356&#39;] shap_res = [] for i in range(10): print(&#39;Iter &#39; + str(i + 1) + &#39;:&#39;) train = pd.read_csv(dir_path + &#39;/outer_&#39; + str(i + 1) + &#39;/train_set.csv&#39;, index_col=0) X_train = train.iloc[:, :-1] test = pd.read_csv(dir_path + &#39;/outer_&#39; + str(i + 1) + &#39;/test_set.csv&#39;, index_col=0) X_test = test.iloc[:, :-1] model = h2o.load_model(dir_path + &#39;/outer_&#39; + str(i + 1) + &#39;/&#39; + model_names[i]) def model_predict_(data): data = pd.DataFrame(data, columns=X_train.columns) h2o_data = h2o.H2OFrame(data) res = model.predict(h2o_data) res = res.as_data_frame().iloc[:,2] return(res) np.random.seed(1) X_train_ = X_train.iloc[np.random.choice(X_train.shape[0], 100, replace=False), :] explainer = shap.KernelExplainer(model_predict_, X_train_, link=&#39;identity&#39;) shap_values = explainer.shap_values(X_test, nsamples=1000) shap_res.append(shap_values) h2o.remove_all() shap_res = pd.DataFrame(np.vstack(shap_res), columns=X_train.columns) shap_res.to_csv(&#39;./other_data/cpp_ad_shap_values_cv_data_sets.csv&#39;, index=False) load(file = &quot;./rdata/var_reduct_cpp_train_test_splits_y_shuffled.RData&quot;) data &lt;- do.call(rbind, lapply(outer_splits, function(x) x$test[-ncol(cpp_data)])) data &lt;- apply(data, 2, function(x) rank(x)) data &lt;- apply(data, 2, function(x) rescale(x, to = c(0, 100))) shap_values &lt;- read.csv(&quot;./other_data/cpp_ad_shap_values_cv_data_sets.csv&quot;, check.names = FALSE) * 100 var_diff &lt;- apply(shap_values, 2, function(x) max(x) - min(x)) var_diff &lt;- var_diff[order(-var_diff)] df &lt;- data.frame( variable = melt(shap_values)$variable, shap = melt(shap_values)$value, variable_value = melt(data)$value ) top_vars &lt;- names(var_diff)[1:11] df &lt;- df[df$variable %in% top_vars, ] df$variable &lt;- factor(df$variable, levels = rev(top_vars), labels = rev(top_vars)) p2 &lt;- ggplot(df, aes(x = shap, y = variable)) + geom_hline(yintercept = top_vars, linetype = &quot;dotted&quot;, color = &quot;grey80&quot;) + geom_vline(xintercept = 0, color = &quot;grey80&quot;, size = 1) + geom_point(aes(fill = variable_value), color = &quot;grey30&quot;, shape = 21, alpha = 0.5, size = 2, position = &quot;auto&quot;, stroke = 0.1) + scale_fill_gradient2(low = &quot;#1f77b4&quot;, mid = &quot;white&quot;, high = &quot;#d62728&quot;, midpoint = 50, breaks = c(0, 100), labels = c(&quot;Low&quot;, &quot;High&quot;)) + theme_bw() + theme( plot.margin = ggplot2::margin(1.5, 8, 1.5, -3), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank(), panel.border = element_blank(), axis.line = element_line(color = &quot;black&quot;), plot.title = element_text(hjust = 0.5, size = 20), axis.title.x = element_text(hjust = 0.5, colour = &quot;black&quot;, size = 20), axis.title.y = element_text(colour = &quot;black&quot;, size = 20), axis.text.x = element_text(colour = &quot;black&quot;, size = 17), axis.text.y = element_text(colour = &quot;black&quot;, size = 16), legend.title = element_text(size = 20), legend.text = element_text(colour = &quot;black&quot;, size = 16), legend.title.align = 0.5 ) + guides( fill = guide_colourbar(&quot;Variable\\nvalue\\n&quot;, ticks = FALSE, barheight = 10, barwidth = 0.5), color = &quot;none&quot; ) + xlab(&quot;SHAP value\\n(variable contribution to model prediction)&quot;) + ylab(&quot;&quot;) + ggtitle(&quot;Cell-penetration&quot;) 5.5.3 Toxicity (classification) # Save train and test sets in csv format load(file = &quot;./rdata/var_reduct_tx_train_test_splits_y_shuffled.RData&quot;) for (i in 1:10) { prefix &lt;- paste0(&quot;outer_&quot;, i) exp_dir &lt;- paste0(&quot;/Users/renee/Downloads/toxicity_adversarial/&quot;, prefix) write.csv(outer_splits[[i]]$train, file = paste0(exp_dir, &quot;/train_set.csv&quot;)) write.csv(outer_splits[[i]]$test, file = paste0(exp_dir, &quot;/test_set.csv&quot;)) } h2o.init(nthreads=-1) dir_path = &#39;/Users/renee/Downloads/toxicity_adversarial&#39; model_names = [&#39;superlearner_iter_5&#39;, &#39;superlearner_iter_6&#39;, &#39;superlearner_iter_5&#39;, &#39;superlearner_iter_7&#39;, &#39;superlearner_iter_4&#39;, &#39;superlearner_iter_6&#39;, &#39;superlearner_iter_6&#39;, &#39;superlearner_iter_5&#39;, &#39;superlearner_iter_4&#39;, &#39;superlearner_iter_5&#39;] shap_res = [] for i in range(10): print(&#39;Iter &#39; + str(i + 1) + &#39;:&#39;) train = pd.read_csv(dir_path + &#39;/outer_&#39; + str(i + 1) + &#39;/train_set.csv&#39;, index_col=0) X_train = train.iloc[:, :-1] test = pd.read_csv(dir_path + &#39;/outer_&#39; + str(i + 1) + &#39;/test_set.csv&#39;, index_col=0) X_test = test.iloc[:, :-1] model = h2o.load_model(dir_path + &#39;/outer_&#39; + str(i + 1) + &#39;/&#39; + model_names[i]) def model_predict_(data): data = pd.DataFrame(data, columns=X_train.columns) h2o_data = h2o.H2OFrame(data) res = model.predict(h2o_data) res = res.as_data_frame().iloc[:,2] # 0:toxic; 1:non-toxic return(res) np.random.seed(1) X_train_ = X_train.iloc[np.random.choice(X_train.shape[0], 100, replace=False), :] explainer = shap.KernelExplainer(model_predict_, X_train_, link=&#39;identity&#39;) shap_values = explainer.shap_values(X_test, nsamples=1000) shap_res.append(shap_values) h2o.remove_all() shap_res = pd.DataFrame(np.vstack(shap_res), columns=X_train.columns) shap_res.to_csv(&#39;./other_data/tx_ad_shap_values_cv_data_sets.csv&#39;, index=False) load(file = &quot;./rdata/var_reduct_tx_train_test_splits_y_shuffled.RData&quot;) data &lt;- do.call(rbind, lapply(outer_splits, function(x) x$test[-ncol(tx_data)])) data &lt;- apply(data, 2, function(x) rank(x)) data &lt;- apply(data, 2, function(x) rescale(x, to = c(0, 100))) shap_values &lt;- read.csv(&quot;./other_data/tx_ad_shap_values_cv_data_sets.csv&quot;, check.names = FALSE) * 100 set.seed(12) ind &lt;- sample(1:nrow(data), 2500) data &lt;- data[ind, ] shap_values &lt;- shap_values[ind, ] var_diff &lt;- apply(shap_values, 2, function(x) max(x) - min(x)) var_diff &lt;- var_diff[order(-var_diff)] df &lt;- data.frame( variable = melt(shap_values)$variable, shap = melt(shap_values)$value, variable_value = melt(data)$value ) top_vars &lt;- names(var_diff)[1:20] df &lt;- df[df$variable %in% top_vars, ] df$variable &lt;- factor(df$variable, levels = rev(top_vars), labels = rev(top_vars)) p3 &lt;- ggplot(df, aes(x = shap, y = variable)) + geom_hline(yintercept = top_vars, linetype = &quot;dotted&quot;, color = &quot;grey80&quot;) + geom_vline(xintercept = 0, color = &quot;grey80&quot;, size = 1) + geom_point(aes(fill = variable_value), color = &quot;grey30&quot;, shape = 21, alpha = 0.3, size = 2, position = &quot;auto&quot;, stroke = 0.1) + scale_fill_gradient2(low = &quot;#1f77b4&quot;, mid = &quot;white&quot;, high = &quot;#d62728&quot;, midpoint = 50, breaks = c(0, 100), labels = c(&quot;Low&quot;, &quot;High&quot;)) + theme_bw() + theme( plot.margin = ggplot2::margin(1.5, 8, 1.5, -3), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank(), panel.border = element_blank(), axis.line = element_line(color = &quot;black&quot;), plot.title = element_text(hjust = 0.5, size = 20), axis.title.x = element_text(hjust = 0.5, colour = &quot;black&quot;, size = 20), axis.title.y = element_text(colour = &quot;black&quot;, size = 20), axis.text.x = element_text(colour = &quot;black&quot;, size = 17), axis.text.y = element_text(colour = &quot;black&quot;, size = 16), legend.title = element_text(size = 20), legend.text = element_text(colour = &quot;black&quot;, size = 16), legend.title.align = 0.5 ) + guides( fill = guide_colourbar(&quot;Variable\\nvalue\\n&quot;, ticks = FALSE, barheight = 10, barwidth = 0.5), color = &quot;none&quot; ) + xlab(&quot;SHAP value\\n(variable contribution to model prediction)&quot;) + ylab(&quot;&quot;) + ggtitle(&quot;Non-toxic&quot;) sessionInfo() ## R version 4.2.2 (2022-10-31) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur ... 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] reticulate_1.25 ggforce_0.3.4 ggplot2_3.3.6 reshape2_1.4.4 ## [5] DT_0.24 digest_0.6.29 stringr_1.4.1 rlist_0.4.6.2 ## [9] enrichvs_0.0.5 scales_1.2.1 mltools_0.3.5 MLmetrics_1.1.1 ## [13] h2o_3.38.0.2 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_1.0.9 here_1.0.1 lattice_0.20-45 png_0.1-7 ## [5] rprojroot_2.0.3 assertthat_0.2.1 utf8_1.2.2 R6_2.5.1 ## [9] plyr_1.8.7 evaluate_0.16 highr_0.9 pillar_1.8.1 ## [13] rlang_1.0.4 rstudioapi_0.14 data.table_1.14.2 jquerylib_0.1.4 ## [17] R.utils_2.12.0 R.oo_1.25.0 Matrix_1.5-1 rmarkdown_2.16 ## [21] styler_1.8.0 htmlwidgets_1.5.4 RCurl_1.98-1.8 polyclip_1.10-0 ## [25] munsell_0.5.0 compiler_4.2.2 xfun_0.32 pkgconfig_2.0.3 ## [29] htmltools_0.5.3 tidyselect_1.1.2 tibble_3.1.8 bookdown_0.28 ## [33] codetools_0.2-18 fansi_1.0.3 dplyr_1.0.9 withr_2.5.0 ## [37] MASS_7.3-58.1 bitops_1.0-7 R.methodsS3_1.8.2 grid_4.2.2 ## [41] jsonlite_1.8.0 gtable_0.3.0 lifecycle_1.0.1 DBI_1.1.3 ## [45] magrittr_2.0.3 cli_3.3.0 stringi_1.7.8 cachem_1.0.6 ## [49] farver_2.1.1 bslib_0.4.0 vctrs_0.4.1 generics_0.1.3 ## [53] tools_4.2.2 R.cache_0.16.0 glue_1.6.2 tweenr_2.0.1 ## [57] purrr_0.3.4 crosstalk_1.2.0 fastmap_1.1.0 yaml_2.3.5 ## [61] colorspace_2.0-3 knitr_1.40 sass_0.4.2 session_info.show() ## ----- ## h2o 3.38.0.2 ## matplotlib 3.6.2 ## numpy 1.23.5 ## pandas 1.5.2 ## session_info 1.0.0 ## shap 0.41.0 ## ----- ## Python 3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 09:04:40) [Clang 14.0.6 ] ## macOS-10.16-x86_64-i386-64bit ## ----- ## Session information updated at 2023-01-20 23:43 "],["id_06_small_data_set_demo.html", "Section 6 Small data set demo 6.1 Setup 6.2 Variable reduction 6.3 Model training 6.4 Run pipeline 6.5 Session info", " Section 6 Small data set demo 6.1 Setup 6.1.1 Load libraries library(ranger) library(rlist) library(h2o) h2o.init(nthreads = -1) 6.1.2 AIC functions #&#39; Mean squared error function #&#39; #&#39; @param y_true a factor vector representing true values. #&#39; @param y_pred a numeric vector or a matrix of predicted values. #&#39; #&#39; @return MSE #&#39; mse &lt;- function(y_true, y_pred) { return(mean((y_true - y_pred)^2)) } #&#39; Regression AIC #&#39; #&#39; @param y_true a factor vector representing true values. #&#39; @param y_pred a numeric vector or a matrix of predicted values. #&#39; @param k number of parameters/variables. #&#39; @param eps a very small value to avoid negative infinitives generated by log(p=0). #&#39; #&#39; @return AIC #&#39; aic_reg &lt;- function(y_true, y_pred, k, eps = 1e-15) { mserr &lt;- mse(y_true, y_pred) if (mserr == 0) mserr &lt;- eps AIC &lt;- length(y_true) * log(mserr) + 2 * k return(AIC) } #&#39; Cross-entropy function #&#39; #&#39; @param y_true a factor vector representing true labels. #&#39; @param y_pred a numeric vector (probabilities of the second class/factor level) #&#39; or a matrix of predicted probabilities. #&#39; @param eps a very small value to avoid negative infinitives generated by log(p=0). #&#39; If the function returns NaN, then increasing eps may solve the issue. Alternatively, #&#39; As NaN is usually generated in the case of p = 1 - eps = 1, resulting in log(1 - p) #&#39; = log(0) from binary cross-entropy, the user can pass prediction values as a matrix #&#39; to calculate categorical cross-entropy instead. #&#39; #&#39; @return H, cross-entropy (mean loss per sample) #&#39; crossEntropy &lt;- function(y_true, y_pred, eps = 1e-15) { stopifnot(is.factor(y_true)) y_pred &lt;- pmax(pmin(y_pred, 1 - eps), eps) n_levels &lt;- nlevels(y_true) H &lt;- NULL # Binary classification if (n_levels == 2 &amp;&amp; is.vector(y_pred)) { y_true &lt;- as.numeric(y_true == levels(y_true)[-1]) H &lt;- -mean(y_true * log2(y_pred) + (1 - y_true) * log2(1 - y_pred)) } # Multi-class classification else if (n_levels == ncol(y_pred)) { y_true &lt;- as.numeric(y_true) y_true_encoded &lt;- t(sapply(y_true, function(x) { tmp &lt;- rep(0, n_levels) tmp[x] &lt;- 1 return(tmp) })) H &lt;- -mean(sapply(1:nrow(y_true_encoded), function(x) sum(y_true_encoded[x, ] * log2(y_pred[x, ])))) } return(H) } #&#39; Classification AIC #&#39; #&#39; @param y_true a factor vector representing true labels. #&#39; @param y_pred a numeric vector or a matrix of predicted probabilities. #&#39; @param k number of parameters/variables. #&#39; @param ... other parameters to be passed to `crossEntropy()` #&#39; #&#39; @return AIC #&#39; aic_clf &lt;- function(y_true, y_pred, k, ...) { H &lt;- crossEntropy(y_true, y_pred, ...) AIC &lt;- 2 * log(2) * length(y_true) * H + 2 * k return(AIC) } 6.1.3 Model parameters # DeepLearning Grid 1 deeplearning_params_1 &lt;- list( activation = &quot;RectifierWithDropout&quot;, epochs = 10000, # early stopping epsilon = c(1e-6, 1e-7, 1e-8, 1e-9), hidden = list(c(50), c(200), c(500)), hidden_dropout_ratios = list(c(0.1), c(0.2), c(0.3), c(0.4), c(0.5)), input_dropout_ratio = c(0, 0.05, 0.1, 0.15, 0.2), rho = c(0.9, 0.95, 0.99) ) # DeepLearning Grid 2 deeplearning_params_2 &lt;- list( activation = &quot;RectifierWithDropout&quot;, epochs = 10000, # early stopping epsilon = c(1e-6, 1e-7, 1e-8, 1e-9), hidden = list(c(50, 50), c(200, 200), c(500, 500)), hidden_dropout_ratios = list( c(0.1, 0.1), c(0.2, 0.2), c(0.3, 0.3), c(0.4, 0.4), c(0.5, 0.5) ), input_dropout_ratio = c(0, 0.05, 0.1, 0.15, 0.2), rho = c(0.9, 0.95, 0.99) ) # DeepLearning Grid 3 deeplearning_params_3 &lt;- list( activation = &quot;RectifierWithDropout&quot;, epochs = 10000, # early stopping epsilon = c(1e-6, 1e-7, 1e-8, 1e-9), hidden = list(c(50, 50, 50), c(200, 200, 200), c(500, 500, 500)), hidden_dropout_ratios = list( c(0.1, 0.1, 0.1), c(0.2, 0.2, 0.2), c(0.3, 0.3, 0.3), c(0.4, 0.4, 0.4), c(0.5, 0.5, 0.5) ), input_dropout_ratio = c(0, 0.05, 0.1, 0.15, 0.2), rho = c(0.9, 0.95, 0.99) ) # GBM gbm_params &lt;- list( col_sample_rate = c(0.4, 0.7, 1.0), col_sample_rate_per_tree = c(0.4, 0.7, 1.0), learn_rate = 0.1, max_depth = c(3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17), min_rows = c(1, 5, 10, 15, 30, 100), min_split_improvement = c(1e-4, 1e-5), ntrees = 10000, # early stopping sample_rate = c(0.5, 0.6, 0.7, 0.8, 0.9, 1.0) ) # XGBoost xgboost_params &lt;- list( booster = c(&quot;gbtree&quot;, &quot;dart&quot;), col_sample_rate = c(0.6, 0.8, 1.0), col_sample_rate_per_tree = c(0.7, 0.8, 0.9, 1.0), max_depth = c(5, 10, 15, 20), min_rows = c(0.01, 0.1, 1.0, 3.0, 5.0, 10.0, 15.0, 20.0), ntrees = 10000, # early stopping reg_alpha = c(0.001, 0.01, 0.1, 1, 10, 100), reg_lambda = c(0.001, 0.01, 0.1, 0.5, 1), sample_rate = c(0.6, 0.8, 1.0) ) 6.2 Variable reduction variable_reduction &lt;- function(predictor_variables, response_variable, seed = 12) { # Calculate initial variable importance set.seed(seed) # Train random forest using ranger ntree &lt;- 100000 rf &lt;- ranger(x = predictor_variables, y = response_variable, num.trees = ntree, importance = &quot;permutation&quot;, verbose = TRUE, scale.permutation.importance = TRUE, seed = 3, keep.inbag = TRUE) var_imp &lt;- ranger::importance(rf) var_imp &lt;- var_imp[order(-var_imp)] var_imp &lt;- var_imp[var_imp &gt;= 0] # Variable reduction iterations ntree &lt;- 1000 var_subset_res &lt;- list() for (i in 1:length(var_imp)) { cat(paste0(&quot;Evaluating &quot;, i, &quot; variable(s) out of &quot;, length(var_imp), &quot; variables\\n&quot;)) predictor_variables_ &lt;- predictor_variables[, colnames(predictor_variables) %in% names(var_imp)[1:i], drop = FALSE] rf_ &lt;- ranger(x = predictor_variables_, y = response_variable, num.trees = ntree, importance = &quot;none&quot;, verbose = TRUE, seed = seed, probability = is.factor(response_variable)) if (!is.factor(response_variable)) { # Regression var_subset_res &lt;- list.append(var_subset_res, list( rsq = rf$r.squared, aic = aic_reg( y_true = response_variable, y_pred = rf_$predictions, k = i ) )) } else { # Classification var_subset_res &lt;- list.append(var_subset_res, list( acc = 1 - rf$prediction.error, aic = aic_clf( y_true = response_variable, y_pred = rf_$predictions, k = i ) )) } } aic_vec &lt;- unlist(lapply(var_subset_res, function(x) x$aic)) aic_cutoff &lt;- which.min(aic_vec) var_imp &lt;- ranger::importance(rf) var_imp &lt;- var_imp[order(-var_imp)][1:aic_cutoff] cat(paste(&quot;Number of variables after variable reduction: &quot;, aic_cutoff, &quot;\\n&quot;)) data &lt;- predictor_variables[, colnames(predictor_variables) %in% names(var_imp), drop = FALSE] data$response &lt;- response_variable return(data) } 6.3 Model training model_training &lt;- function(train_set, prefix = &quot;demo_models&quot;, exp_dir = &quot;./demo_models&quot;, nfolds = 10, grid_seed = 1) { h2o.init(nthreads = -1) h2o.removeAll() dir.create(exp_dir) tmp &lt;- as.h2o(train_set, destination_frame = prefix) classification &lt;- FALSE if (is.factor(train_set$response)) classification &lt;- TRUE samp_factors &lt;- NULL if (classification) { tmp[&quot;response&quot;] &lt;- as.factor(tmp[&quot;response&quot;]) samp_factors &lt;- as.vector(mean(table(train_set$response)) / table(train_set$response)) } y &lt;- &quot;response&quot; x &lt;- setdiff(names(tmp), y) res &lt;- as.data.frame(tmp$response) # ------------------- # base model training # ------------------- cat(&quot;Deep learning grid 1\\n&quot;) deeplearning_1 &lt;- h2o.grid( algorithm = &quot;deeplearning&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = deeplearning_params_1, stopping_rounds = 3, balance_classes = classification, class_sampling_factors = samp_factors, search_criteria = list( strategy = &quot;RandomDiscrete&quot;, max_models = 5, seed = grid_seed ), keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in deeplearning_1@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;Deep learning grid 2\\n&quot;) deeplearning_2 &lt;- h2o.grid( algorithm = &quot;deeplearning&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = deeplearning_params_2, stopping_rounds = 3, balance_classes = classification, class_sampling_factors = samp_factors, search_criteria = list( strategy = &quot;RandomDiscrete&quot;, max_models = 5, seed = grid_seed ), keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in deeplearning_2@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;Deep learning grid 3\\n&quot;) deeplearning_3 &lt;- h2o.grid( algorithm = &quot;deeplearning&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = deeplearning_params_3, stopping_rounds = 3, balance_classes = classification, class_sampling_factors = samp_factors, search_criteria = list( strategy = &quot;RandomDiscrete&quot;, max_models = 5, seed = grid_seed ), keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in deeplearning_3@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;GBM grid\\n&quot;) gbm &lt;- h2o.grid( algorithm = &quot;gbm&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = gbm_params, stopping_rounds = 3, balance_classes = classification, class_sampling_factors = samp_factors, search_criteria = list(strategy = &quot;RandomDiscrete&quot;, max_models = 15, seed = grid_seed), keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in gbm@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;GBM 5 default models\\n&quot;) gbm_1 &lt;- h2o.gbm( model_id = &quot;GBM_1&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, balance_classes = classification, class_sampling_factors = samp_factors, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 6, min_rows = 1, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_2 &lt;- h2o.gbm( model_id = &quot;GBM_2&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, balance_classes = classification, class_sampling_factors = samp_factors, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 7, min_rows = 10, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_3 &lt;- h2o.gbm( model_id = &quot;GBM_3&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, balance_classes = classification, class_sampling_factors = samp_factors, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 8, min_rows = 10, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) gbm_4 &lt;- h2o.gbm( model_id = &quot;GBM_4&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, balance_classes = classification, class_sampling_factors = samp_factors, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, learn_rate = 0.1, max_depth = 10, min_rows = 10, min_split_improvement = 1e-5, ntrees = 10000, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) # gbm_5 = h2o.gbm(model_id=&#39;GBM_5&#39;, x=x, y=y, training_frame=tmp, seed=1, nfolds=nfolds, # keep_cross_validation_predictions=TRUE, stopping_rounds=3, score_tree_interval=5, # balance_classes=classification, class_sampling_factors=samp_factors, # col_sample_rate=0.8, col_sample_rate_per_tree=0.8, learn_rate=0.1, # max_depth=15, min_rows=100, min_split_improvement=1e-5, ntrees=10000, sample_rate=0.8, # keep_cross_validation_models=FALSE, fold_assignment=&#39;Modulo&#39;) for (model_id in paste0(&quot;GBM_&quot;, 1:4)) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;XGBoost grid\\n&quot;) xgboost &lt;- h2o.grid( algorithm = &quot;xgboost&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, hyper_params = xgboost_params, stopping_rounds = 3, search_criteria = list(strategy = &quot;RandomDiscrete&quot;, max_models = 15, seed = grid_seed), keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot;, parallelism = 0 ) for (model_id in xgboost@model_ids) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;XGBoost 3 default models\\n&quot;) xgboost_1 &lt;- h2o.xgboost( model_id = &quot;XGBoost_1&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, booster = &quot;gbtree&quot;, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, max_depth = 10, min_rows = 5, ntrees = 10000, reg_alpha = 0, reg_lambda = 1, sample_rate = 0.6, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) xgboost_2 &lt;- h2o.xgboost( model_id = &quot;XGBoost_2&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, booster = &quot;gbtree&quot;, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, max_depth = 20, min_rows = 10, ntrees = 10000, reg_alpha = 0, reg_lambda = 1, sample_rate = 0.6, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) xgboost_3 &lt;- h2o.xgboost( model_id = &quot;XGBoost_3&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, stopping_rounds = 3, score_tree_interval = 5, booster = &quot;gbtree&quot;, col_sample_rate = 0.8, col_sample_rate_per_tree = 0.8, max_depth = 5, min_rows = 3, ntrees = 10000, reg_alpha = 0, reg_lambda = 1, sample_rate = 0.8, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) for (model_id in paste0(&quot;XGBoost_&quot;, 1:3)) { tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) } cat(&quot;GLM\\n&quot;) glm &lt;- h2o.glm( model_id = &quot;GLM&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, alpha = c(0.0, 0.2, 0.4, 0.6, 0.8, 1.0), balance_classes = classification, class_sampling_factors = samp_factors, max_after_balance_size = 0.5, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;GLM&quot;), path = exp_dir, force = TRUE) cat(&quot;DRF\\n&quot;) drf &lt;- h2o.randomForest( model_id = &quot;DRF&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, ntrees = 10000, score_tree_interval = 5, stopping_rounds = 3, balance_classes = classification, class_sampling_factors = samp_factors, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;DRF&quot;), path = exp_dir, force = TRUE) cat(&quot;XRT\\n&quot;) xrt &lt;- h2o.randomForest( model_id = &quot;XRT&quot;, x = x, y = y, training_frame = tmp, seed = 1, nfolds = nfolds, keep_cross_validation_predictions = TRUE, ntrees = 10000, histogram_type = &quot;Random&quot;, score_tree_interval = 5, stopping_rounds = 3, balance_classes = classification, class_sampling_factors = samp_factors, keep_cross_validation_models = FALSE, fold_assignment = &quot;Modulo&quot; ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;XRT&quot;), path = exp_dir, force = TRUE) # ----------------------- # get holdout predictions # ----------------------- base_models &lt;- as.list(c( unlist(deeplearning_1@model_ids), unlist(deeplearning_2@model_ids), unlist(deeplearning_3@model_ids), unlist(gbm@model_ids), paste0(&quot;GBM_&quot;, 1:4), unlist(xgboost@model_ids), paste0(&quot;XGBoost_&quot;, 1:3), &quot;GLM&quot;, &quot;DRF&quot;, &quot;XRT&quot; )) for (model_id in base_models) { if (!classification) { res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id)), col.names = model_id )) } else { res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[3], col.names = model_id )) } } # ---------------------- # super learner training # ---------------------- sl_iter &lt;- 0 cat(paste0(&quot;Super learner iteration &quot;, sl_iter, &quot; (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = paste0(&quot;superlearner_iter_&quot;, sl_iter), training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list( standardize = TRUE, keep_cross_validation_predictions = TRUE, balance_classes = classification, class_sampling_factors = samp_factors, max_after_balance_size = 0.5 ) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(paste0(&quot;superlearner_iter_&quot;, sl_iter)), path = exp_dir, force = TRUE) model_id &lt;- paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter) tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) if (!classification) { res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id)), col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) } else { res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[3], col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) } # ---------------------------------- # super learner base model reduction # ---------------------------------- while (TRUE) { meta &lt;- h2o.getModel(paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter, &quot;_cv_1&quot;)) names &lt;- meta@model$coefficients_table[, &quot;names&quot;] coeffs &lt;- (meta@model$coefficients_table[, &quot;standardized_coefficients&quot;] &gt; 0) for (j in 2:nfolds) { meta &lt;- h2o.getModel(paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter, &quot;_cv_&quot;, j)) names &lt;- meta@model$coefficients_table[, &quot;names&quot;] coeffs &lt;- coeffs + (meta@model$coefficients_table[, &quot;standardized_coefficients&quot;] &gt; 0) } base_models_ &lt;- as.list(names[coeffs &gt;= ceiling(nfolds / 2) &amp; names != &quot;Intercept&quot;]) if (length(base_models_) == 0) { cat(&quot;No base models passing the threshold\\n\\n&quot;) break } if (sum(base_models %in% base_models_) == length(base_models)) { cat(&quot;No further reduction of base models\\n\\n&quot;) break } sl_iter &lt;- sl_iter + 1 base_models &lt;- base_models_ cat(paste0(&quot;Super learner iteration &quot;, sl_iter, &quot; (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = paste0(&quot;superlearner_iter_&quot;, sl_iter), training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list( standardize = TRUE, keep_cross_validation_predictions = TRUE, balance_classes = classification, class_sampling_factors = samp_factors, max_after_balance_size = 0.5 ) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(paste0(&quot;superlearner_iter_&quot;, sl_iter)), path = exp_dir, force = TRUE) model_id &lt;- paste0(&quot;metalearner_glm_superlearner_iter_&quot;, sl_iter) tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) if (!classification) { res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id)), col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) } else { res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[3], col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) } } # ----------------------------------------- # super learner for homogeneous base models # ----------------------------------------- # DeepLearning base_models &lt;- as.list(c( unlist(deeplearning_1@model_ids), unlist(deeplearning_2@model_ids), unlist(deeplearning_3@model_ids) )) cat(paste0(&quot;Super learner deep learning (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = &quot;superlearner_deeplearning&quot;, training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list( standardize = TRUE, keep_cross_validation_predictions = TRUE, balance_classes = classification, class_sampling_factors = samp_factors, max_after_balance_size = 0.5 ) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;superlearner_deeplearning&quot;), path = exp_dir, force = TRUE) model_id &lt;- &quot;metalearner_glm_superlearner_deeplearning&quot; tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) if (!classification) { res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id)), col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) } else { res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[3], col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) } # GBM base_models &lt;- as.list(c(unlist(gbm@model_ids), paste0(&quot;GBM_&quot;, 1:4))) cat(paste0(&quot;Super learner GBM (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = &quot;superlearner_gbm&quot;, training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list( standardize = TRUE, keep_cross_validation_predictions = TRUE, balance_classes = classification, class_sampling_factors = samp_factors, max_after_balance_size = 0.5 ) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;superlearner_gbm&quot;), path = exp_dir, force = TRUE) model_id &lt;- &quot;metalearner_glm_superlearner_gbm&quot; tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) if (!classification) { res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id)), col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) } else { res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[3], col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) } # XGBoost base_models &lt;- as.list(c(unlist(xgboost@model_ids), paste0(&quot;XGBoost_&quot;, 1:3))) cat(paste0(&quot;Super learner XGBoost (&quot;, length(base_models), &quot; models)\\n&quot;)) sl &lt;- h2o.stackedEnsemble( x = x, y = y, model_id = &quot;superlearner_xgboost&quot;, training_frame = tmp, seed = 1, base_models = base_models, metalearner_algorithm = &quot;glm&quot;, metalearner_nfolds = nfolds, keep_levelone_frame = TRUE, metalearner_params = list( standardize = TRUE, keep_cross_validation_predictions = TRUE, balance_classes = classification, class_sampling_factors = samp_factors, max_after_balance_size = 0.5 ) ) tmp_path &lt;- h2o.saveModel(h2o.getModel(&quot;superlearner_xgboost&quot;), path = exp_dir, force = TRUE) model_id &lt;- &quot;metalearner_glm_superlearner_xgboost&quot; tmp_path &lt;- h2o.saveModel(h2o.getModel(model_id), path = exp_dir, force = TRUE) if (!classification) { res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id)), col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) } else { res &lt;- cbind(res, as.data.frame(h2o.getFrame(paste0(&quot;cv_holdout_prediction_&quot;, model_id))[3], col.names = paste0(model_id, &quot;_&quot;, length(base_models), &quot;_models&quot;) )) } write.csv(res, file = paste0(exp_dir, &quot;/cv_holdout_predictions.csv&quot;), row.names = FALSE) cat(&quot;\\n\\n&quot;) h2o.removeAll() } 6.4 Run pipeline data &lt;- read.csv(&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data&quot;, check.names = FALSE, row.names = 1) # Random subset set.seed(10) data &lt;- data[sample(1:nrow(data), size = 50, replace = FALSE), ] predictor_variables &lt;- data[, colnames(data) != &quot;status&quot;] response_variable &lt;- as.factor(data$status) system.time({ reduced_data &lt;- variable_reduction(predictor_variables = predictor_variables, response_variable = response_variable) model_training(train_set = reduced_data) }) 6.5 Session info sessionInfo() ## R version 4.2.2 (2022-10-31) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur ... 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] rstudioapi_0.14 knitr_1.40 magrittr_2.0.3 R.cache_0.16.0 ## [5] R6_2.5.1 rlang_1.0.4 fastmap_1.1.0 stringr_1.4.1 ## [9] styler_1.8.0 tools_4.2.2 xfun_0.32 R.oo_1.25.0 ## [13] cli_3.3.0 jquerylib_0.1.4 htmltools_0.5.3 yaml_2.3.5 ## [17] digest_0.6.29 bookdown_0.28 purrr_0.3.4 vctrs_0.4.1 ## [21] sass_0.4.2 R.utils_2.12.0 cachem_1.0.6 evaluate_0.16 ## [25] rmarkdown_2.16 stringi_1.7.8 compiler_4.2.2 bslib_0.4.0 ## [29] R.methodsS3_1.8.2 jsonlite_1.8.0 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
