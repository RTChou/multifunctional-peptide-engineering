[["index.html", "REU Research Report Section 1 Introduction 1.1 Header 2", " REU Research Report Author name 2022-06-09 Section 1 Introduction Markdown Basics 1.1 Header 2 1.1.1 Header 3 1.1.1.1 Header 4 1.1.1.1.1 Header 5 "],["data-preprocessing.html", "Section 2 Data preprocessing", " Section 2 Data preprocessing iris &lt;- read.csv(file = &quot;./data/iris.csv&quot;) # Some preprocessing steps ... save(iris, file = &quot;./rdata/data.RData&quot;) See Table 2.1 for more detailed information of the variable summary in the Iris data set. load(file = &quot;./rdata/data.RData&quot;) var_sum &lt;- as.data.frame(skim(iris[, 1:3])) var_sum &lt;- var_sum[c(&quot;skim_variable&quot;, &quot;n_missing&quot;, &quot;numeric.mean&quot;, &quot;numeric.sd&quot;)] knitr::kable(var_sum, digits = 2, caption = &quot;Variable summary.&quot;) %&gt;% kable_styling(font_size = 11) Table 2.1: Variable summary. skim_variable n_missing numeric.mean numeric.sd Sepal.Length 0 5.84 0.83 Sepal.Width 0 3.06 0.44 Petal.Length 0 3.76 1.77 load(file = &quot;./rdata/data.RData&quot;) # Run some models tree &lt;- rpart(Species ~ ., data = iris) save(tree, file = &quot;./rdata/model.RData&quot;) # Generate some nice plots pdf(file = &quot;./figures/tree_plot.pdf&quot;) print(rpart.plot(tree)) dev.off() The rpart model structure is shown in Figure 2.1. knitr::include_graphics(&quot;./figures/tree_plot.pdf&quot;) Figure 2.1: Tree model structure. "],["data-analysis.html", "Section 3 Data analysis 3.1 Variable filtering", " Section 3 Data analysis 3.1 Variable filtering We performed variable filtering for variable importance and group variable importance. The procedure is as follows: Based on the initial variable importance or group variable importance where variables were ranked by the importance values, we first identified the most important set of variables or group variables by finding the largest importance gap between variables using an algorithm implementing an objective function. From the remaining set of variables or group variables, we then selected the second most important set of variables or group variables with the largest gap observed in the remaining set. We repeated Step 2 to find the third most important variable set and so on. We trained multiple random forest models using the sets of variables we identified, where the first random forest model was constructed using the most important set of variables or group variables; the second random forest model was trained using the most important and the second most important variable sets; the third random forest model was built using the most important, the second most important, and the third most important variables sets, and so on. For each model with filtered variables, we evaluated the model performance using Akaike information criterion (AIC) (Yun et al. 2022; Rastgou et al. 2020). The criterion is as follows: \\[\\begin{equation} n \\times log(err) + 2 \\times num\\_vars, \\tag{3.1} \\end{equation}\\] where n is the number of samples, err is classification error or mean squared error (MSE) for regression. The criterion (3.1) accounts for both model error and number of variables. If there is a large number of variables in the training data set, the criterion will give more penalties to the model. We reported random forest models having the lowest AIC values for the variable filtering analysis. References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
